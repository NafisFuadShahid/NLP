{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "a-YrFyW2cn_n",
        "outputId": "975995fc-b85b-4cb7-ea3c-364ae1e4508e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SERIAL</th>\n",
              "      <th>MESSAGE</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the other side of * galicismos * * galicismo *...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>re : equistar deal tickets are you still avail...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\r\\nHello I am your hot lil horny toy.\\r\\n    ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SERIAL                                            MESSAGE  CATEGORY\n",
              "0       0  re : 6 . 1100 , disc : uniformitarianism , re ...         0\n",
              "1       1  the other side of * galicismos * * galicismo *...         0\n",
              "2       2  re : equistar deal tickets are you still avail...         0\n",
              "3       3  \\r\\nHello I am your hot lil horny toy.\\r\\n    ...         1\n",
              "4       4  software at incredibly low prices ( 86 % lower...         1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset (https://www.kaggle.com/chandramoulinaidu/spam-classification-for-basic-nlp)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Phishing_Email.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5Brt9s9rkKYb",
        "outputId": "1b8fb1a7-af4c-4893-fead-eb71c0c887d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SERIAL</th>\n",
              "      <th>MESSAGE</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18629</th>\n",
              "      <td>18646</td>\n",
              "      <td>date a lonely housewife always wanted to date ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18630</th>\n",
              "      <td>18647</td>\n",
              "      <td>request submitted : access request for anita ....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18631</th>\n",
              "      <td>18648</td>\n",
              "      <td>re : important - prc mtg hi dorn &amp; john , as y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18632</th>\n",
              "      <td>18649</td>\n",
              "      <td>press clippings - letter on californian utilit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18633</th>\n",
              "      <td>18650</td>\n",
              "      <td>empty</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       SERIAL                                            MESSAGE  CATEGORY\n",
              "18629   18646  date a lonely housewife always wanted to date ...         1\n",
              "18630   18647  request submitted : access request for anita ....         0\n",
              "18631   18648  re : important - prc mtg hi dorn & john , as y...         0\n",
              "18632   18649  press clippings - letter on californian utilit...         0\n",
              "18633   18650                                              empty         1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BZJ1xgHkOOy",
        "outputId": "19b2cb60-3334-41d5-969c-6fb0fea4846f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CATEGORY\n",
              "0    11322\n",
              "1     7312\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['CATEGORY'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUoCyp8mkXsh",
        "outputId": "7bcfa229-8c68-46d0-f672-325c73c0b020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\fuadn\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\fuadn\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWa64YhklKeT",
        "outputId": "9c407065-d1dd-46d7-d3e2-a0af97be6cfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'GGggGG',\n",
              " 'feet',\n",
              " 'it',\n",
              " 'going',\n",
              " 'HTML',\n",
              " 'bads',\n",
              " 'bads',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "test_message = \"Hey,, GGggGG feet it going? <HTML><bads> bads 'randoms' badly\"\n",
        "\n",
        "test_message_tokenized = tokenizer.tokenize(test_message)\n",
        "test_message_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts46HslRmwlB",
        "outputId": "d48edd5e-a828-4374-bf86-7c9f4157da35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey',\n",
              " 'gggggg',\n",
              " 'feet',\n",
              " 'it',\n",
              " 'going',\n",
              " 'html',\n",
              " 'bads',\n",
              " 'bads',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_message_lowercased = [t.lower() for t in test_message_tokenized]\n",
        "test_message_lowercased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfXmuvUn3dy",
        "outputId": "23a3a78a-c9d9-4c6d-d0ff-fa122304f23d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey',\n",
              " 'gggggg',\n",
              " 'foot',\n",
              " 'it',\n",
              " 'going',\n",
              " 'html',\n",
              " 'bad',\n",
              " 'bad',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "test_message_lemmatized_tokens = [lemmatizer.lemmatize(t) for t in test_message_lowercased]\n",
        "test_message_lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG06k4ags0al",
        "outputId": "1fab4508-e4ad-4be7-b32d-cbb2983c02df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey', 'gggggg', 'foot', 'going', 'html', 'bad', 'bad', 'randoms', 'badly']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "test_message_useful_tokens = [t for t in test_message_lemmatized_tokens if t not in stopwords]\n",
        "test_message_useful_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtNehJ51t6Ii",
        "outputId": "8dac74a6-f061-4fc9-a0ad-22bdf637af02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey', 'gggggg', 'foot', 'going', 'html', 'bad', 'bad', 'randoms', 'badly']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def message_to_token_list(s):\n",
        "  tokens = tokenizer.tokenize(s)\n",
        "  lowercased_tokens = [t.lower() for t in tokens]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]\n",
        "  useful_tokens = [t for t in lemmatized_tokens if t not in stopwords]\n",
        "\n",
        "  return useful_tokens\n",
        "\n",
        "message_to_token_list(test_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_InW_NurvEqq",
        "outputId": "5510f999-7e20-4d6d-8b2b-02970916b041"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       SERIAL                                            MESSAGE  CATEGORY\n",
              " 0       13454  fw : priority customer list this is not going ...         0\n",
              " 1       14141  On Tue, 30 Jul 2002 22:22:24 +0200, \"Manfred G...         0\n",
              " 2       17106  re : [ penetrative ] 86 % - off vicodin . dono...         1\n",
              " 3       13497  btu ' s daily power report - eastern edition a...         0\n",
              " 4       17348  On Tue, 27 Aug 2002, David Neary wrote:> > Act...         0\n",
              " ...       ...                                                ...       ...\n",
              " 14902    8884  -----Original Message----- > From: Aherne Pete...         0\n",
              " 14903    7018  re : credit trading brought to you by bryan se...         0\n",
              " 14904    9022  review of perez - leroux & glass ( ed ) contem...         0\n",
              " 14905    6925  enron actuals for july 7 thru 9 , 2000 july 7 ...         0\n",
              " 14906   13046  re : thursday visit frank , we shall have abou...         0\n",
              " \n",
              " [14907 rows x 3 columns],\n",
              "       SERIAL                                            MESSAGE  CATEGORY\n",
              " 0       3220  Note: \\r\\nThis is NOT SPAM!! This is NOT Unsol...         1\n",
              " 1      15469  haul special offer ! you don ' t need to make ...         1\n",
              " 2      13386  has a funky cut/paste model that is essentiall...         0\n",
              " 3       8239  On 0020 -0700 %{!Thu, Sep 05, 2002 at  3:17:36...         0\n",
              " 4       6388  v\\r\\nAmerica's #1 Government Grant Program!\\r\\...         1\n",
              " ...      ...                                                ...       ...\n",
              " 3722   10969  URL: http://www.newsisfree.com/click/-3,868897...         0\n",
              " 3723   17306  Would You Like to Save up to 80\\r\\n Would You ...         1\n",
              " 3724    5200  esslli workshop : lexical semantics in context...         0\n",
              " 3725   12188  Dear Candidate,We recently came across a posti...         1\n",
              " 3726     236  psycholinguistics & neurolinguistics linguisti...         0\n",
              " \n",
              " [3727 rows x 3 columns])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.sample(frac=1, random_state=1)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "split_index = int(len(df) * 0.8)\n",
        "train_df, test_df = df[:split_index], df[split_index:]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwUuMr_0waAu",
        "outputId": "14ab0468-fdff-455f-e6a0-aebbcb7102a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "140862"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "token_counter = {}\n",
        "\n",
        "# Iterate over each message in the 'MESSAGE' column\n",
        "for message in train_df['MESSAGE']:\n",
        "    if isinstance(message, str):  # Check if message is a string\n",
        "        message_as_token_lst = message_to_token_list(message)  # Convert message to a list of tokens\n",
        "\n",
        "        # Count occurrences of each token\n",
        "        for token in message_as_token_lst:\n",
        "            if token in token_counter:\n",
        "                token_counter[token] += 1\n",
        "            else:\n",
        "                token_counter[token] = 1\n",
        "\n",
        "len(token_counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubi91FlbyfXJ",
        "outputId": "1ed3962a-61fd-4b1f-fd80-b3a7322438b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'fw': 582,\n",
              " 'priority': 314,\n",
              " 'customer': 2086,\n",
              " 'list': 11436,\n",
              " 'going': 2023,\n",
              " 'work': 7854,\n",
              " 'want': 4565,\n",
              " 'control': 1358,\n",
              " 'like': 7945,\n",
              " 'used': 3240,\n",
              " 'enron': 16368,\n",
              " 'please': 11825,\n",
              " 'let': 3215,\n",
              " 'know': 6078,\n",
              " 'bill': 1491,\n",
              " 'original': 2557,\n",
              " 'message': 6095,\n",
              " 'forster': 66,\n",
              " 'david': 1896,\n",
              " 'sent': 4127,\n",
              " 'friday': 1589,\n",
              " 'february': 1084,\n",
              " '01': 3184,\n",
              " '2002': 3521,\n",
              " '2': 15186,\n",
              " '53': 299,\n",
              " 'pm': 2841,\n",
              " 'bradford': 88,\n",
              " 'william': 556,\n",
              " 'brackett': 21,\n",
              " 'debbie': 70,\n",
              " 'r': 4269,\n",
              " 'pat': 206,\n",
              " 'odonnell': 8,\n",
              " 'ubsw': 35,\n",
              " 'com': 15812,\n",
              " 'louis': 175,\n",
              " 'eber': 11,\n",
              " 'glass': 91,\n",
              " 'colette': 31,\n",
              " 'dow': 1108,\n",
              " 'cc': 2819,\n",
              " 'kitchen': 555,\n",
              " 'louise': 934,\n",
              " 'subject': 11443,\n",
              " 'attached': 1198,\n",
              " 'intended': 853,\n",
              " 'pre': 1038,\n",
              " 'approved': 581,\n",
              " 'company': 10046,\n",
              " 'ready': 1219,\n",
              " 'launch': 211,\n",
              " 'day': 7004,\n",
              " 'includes': 1150,\n",
              " 'u': 13125,\n",
              " 'canadian': 376,\n",
              " 'entity': 411,\n",
              " 'gas': 2959,\n",
              " 'power': 3024,\n",
              " 'recently': 892,\n",
              " 'told': 849,\n",
              " 'individual': 1690,\n",
              " 'legal': 1389,\n",
              " 'name': 7450,\n",
              " 'wanted': 733,\n",
              " 'group': 4527,\n",
              " 'needed': 921,\n",
              " 'provide': 2587,\n",
              " 'way': 5064,\n",
              " 'well': 4680,\n",
              " 'note': 2832,\n",
              " '25': 3026,\n",
              " 'previously': 475,\n",
              " 'supplied': 171,\n",
              " 'credit': 3506,\n",
              " 'appears': 522,\n",
              " 'number': 6131,\n",
              " 'non': 2426,\n",
              " 'online': 2301,\n",
              " 'included': 1117,\n",
              " 'recent': 1494,\n",
              " 'provides': 760,\n",
              " 'order': 8380,\n",
              " 'sending': 934,\n",
              " 'another': 2221,\n",
              " 'later': 1375,\n",
              " 'identify': 393,\n",
              " 'type': 2843,\n",
              " 'master': 587,\n",
              " 'agreement': 1431,\n",
              " 'isda': 8,\n",
              " 'physical': 402,\n",
              " 'dave': 470,\n",
              " 'tue': 353,\n",
              " '30': 8256,\n",
              " 'jul': 408,\n",
              " '22': 1573,\n",
              " '24': 2217,\n",
              " '0200': 132,\n",
              " 'manfred': 126,\n",
              " 'grobosch': 1,\n",
              " 'wrote': 1658,\n",
              " 'would': 11127,\n",
              " 'install': 430,\n",
              " 'rpm': 1216,\n",
              " 'tried': 538,\n",
              " 'get': 9165,\n",
              " 'information': 13130,\n",
              " 'visiting': 189,\n",
              " 'www': 10334,\n",
              " 'org': 2324,\n",
              " 'related': 2132,\n",
              " 'link': 3009,\n",
              " 'give': 2772,\n",
              " 'seems': 1297,\n",
              " 'assume': 554,\n",
              " 'already': 1605,\n",
              " 'installed': 302,\n",
              " 'firewall': 107,\n",
              " 'based': 4112,\n",
              " 'linux': 3289,\n",
              " '20': 9607,\n",
              " 'smoothwall': 6,\n",
              " 'private': 927,\n",
              " 'use': 6940,\n",
              " 'package': 1570,\n",
              " 'program': 7417,\n",
              " 'scratch': 66,\n",
              " 'found': 1944,\n",
              " 'site': 4839,\n",
              " 'hopefully': 137,\n",
              " 'knowledge': 1869,\n",
              " 'window': 1783,\n",
              " 'sco': 8,\n",
              " 'aix': 72,\n",
              " 'box': 1911,\n",
              " 'generally': 465,\n",
              " 'anything': 1291,\n",
              " 'redhat': 538,\n",
              " 'send': 5995,\n",
              " 'reason': 1415,\n",
              " 'maybe': 724,\n",
              " 'missed': 165,\n",
              " 'something': 1681,\n",
              " 'running': 712,\n",
              " 'anyway': 378,\n",
              " 'brian': 493,\n",
              " 'fahrlã': 24,\n",
              " 'nder': 24,\n",
              " 'zealot': 27,\n",
              " 'conservative': 185,\n",
              " 'technomad': 24,\n",
              " 'evansville': 37,\n",
              " 'voyage': 42,\n",
              " 'http': 15898,\n",
              " 'countermoon': 24,\n",
              " 'icq': 54,\n",
              " '5119262': 24,\n",
              " 'hear': 701,\n",
              " 'news': 2820,\n",
              " 'isreal': 14,\n",
              " 'contains': 904,\n",
              " 'word': 5728,\n",
              " 'bullet': 67,\n",
              " 'brain': 256,\n",
              " 'arafat': 24,\n",
              " '_______________________________________________': 836,\n",
              " 'mailing': 3538,\n",
              " 'freshrpms': 531,\n",
              " 'net': 5720,\n",
              " 'mailman': 1626,\n",
              " 'listinfo': 2079,\n",
              " 'penetrative': 2,\n",
              " '86': 166,\n",
              " 'vicodin': 62,\n",
              " 'donovan': 21,\n",
              " 'harvest': 51,\n",
              " 'pisa': 82,\n",
              " 'toiler': 1,\n",
              " 'staten': 3,\n",
              " 'worrier': 3,\n",
              " 'server': 1662,\n",
              " 'dreadfully': 1,\n",
              " 'pilate': 2,\n",
              " 'unix': 320,\n",
              " 'musical': 131,\n",
              " 'visigoth': 2,\n",
              " 'eastern': 377,\n",
              " 'adjoins': 7,\n",
              " 'unqualified': 12,\n",
              " 'abhors': 1,\n",
              " 'viper': 5,\n",
              " 'buddy': 46,\n",
              " 'grin': 16,\n",
              " 'outcast': 4,\n",
              " 'transistorizing': 1,\n",
              " 'avalanche': 25,\n",
              " 'debt': 1179,\n",
              " 'supermarket': 12,\n",
              " 'fanning': 1,\n",
              " 'mitigates': 8,\n",
              " 'arson': 2,\n",
              " 'dismisser': 1,\n",
              " 'weird': 90,\n",
              " 'swarm': 7,\n",
              " 'sane': 15,\n",
              " 'codify': 11,\n",
              " 'trim': 27,\n",
              " 'bug': 337,\n",
              " 'levity': 2,\n",
              " 'harlot': 1,\n",
              " 'phone': 3581,\n",
              " '568': 10,\n",
              " '857': 11,\n",
              " '4997': 1,\n",
              " 'mobile': 491,\n",
              " '524': 27,\n",
              " '858': 26,\n",
              " '6793': 1,\n",
              " 'email': 10420,\n",
              " 'alycewiley': 1,\n",
              " '2001': 4460,\n",
              " 'madhuri': 3,\n",
              " 'btu': 40,\n",
              " 'daily': 950,\n",
              " 'report': 7882,\n",
              " 'edition': 727,\n",
              " 'latest': 1118,\n",
              " 'issue': 4411,\n",
              " 'e': 17139,\n",
              " 'mail': 12888,\n",
              " 'info': 1727,\n",
              " '732': 52,\n",
              " '758': 26,\n",
              " '8222': 2,\n",
              " 'fax': 5681,\n",
              " '8286': 2,\n",
              " 'peo': 4,\n",
              " '71100': 1,\n",
              " 'pdf': 173,\n",
              " '27': 1810,\n",
              " 'aug': 514,\n",
              " 'neary': 23,\n",
              " 'actually': 1111,\n",
              " 'following': 4599,\n",
              " 'sensible': 41,\n",
              " 'echo': 133,\n",
              " 'enc': 3,\n",
              " 'sed': 55,\n",
              " '0': 12007,\n",
              " '9a': 20,\n",
              " 'fa': 109,\n",
              " 'f': 2183,\n",
              " 'x': 3306,\n",
              " '1': 20185,\n",
              " 'g': 3444,\n",
              " 'idea': 1703,\n",
              " 'wa': 13530,\n",
              " 'along': 948,\n",
              " 'line': 3834,\n",
              " 'attempting': 100,\n",
              " 'realised': 15,\n",
              " 'straight': 198,\n",
              " 'swap': 162,\n",
              " 'couldnt': 8,\n",
              " 'awk': 22,\n",
              " 'gensub': 2,\n",
              " 'insert': 162,\n",
              " 'end': 2332,\n",
              " 'internet': 4077,\n",
              " 'adapted': 41,\n",
              " 'function': 986,\n",
              " 'decode_url': 1,\n",
              " 'str': 30,\n",
              " 'hextab': 21,\n",
              " 'c': 6794,\n",
              " 'c1': 12,\n",
              " 'c2': 13,\n",
              " 'len': 30,\n",
              " 'code': 1872,\n",
              " 'hex': 12,\n",
              " 'dec': 339,\n",
              " 'lookup': 59,\n",
              " 'table': 787,\n",
              " '8': 4681,\n",
              " '9': 5538,\n",
              " '10': 10658,\n",
              " '3': 12529,\n",
              " 'b': 3812,\n",
              " '11': 6075,\n",
              " '4': 8584,\n",
              " '12': 5375,\n",
              " '5': 9874,\n",
              " '13': 2366,\n",
              " '6': 5400,\n",
              " '14': 2647,\n",
              " '7': 5007,\n",
              " '15': 5883,\n",
              " 'decoded': 4,\n",
              " 'length': 990,\n",
              " 'substr': 3,\n",
              " 'check': 3229,\n",
              " 'usual': 266,\n",
              " 'start': 2567,\n",
              " 'uri': 17,\n",
              " 'encoding': 263,\n",
              " 'char': 97,\n",
              " 'valid': 375,\n",
              " 'toupper': 2,\n",
              " '16': 3036,\n",
              " 'sprintf': 1,\n",
              " 'space': 815,\n",
              " 'apparently': 345,\n",
              " 'else': 1087,\n",
              " 'return': 1338,\n",
              " 'cheer': 200,\n",
              " 'p': 5355,\n",
              " 'late': 1050,\n",
              " 'reply': 1747,\n",
              " 'footer': 38,\n",
              " 'received': 2973,\n",
              " 'error': 1460,\n",
              " 'yadda': 4,\n",
              " 'got': 1509,\n",
              " 'caught': 105,\n",
              " 'spam': 1484,\n",
              " 'filter': 239,\n",
              " 'ended': 152,\n",
              " 'junkmail': 4,\n",
              " 'directory': 1374,\n",
              " 'might': 2336,\n",
              " 'header': 425,\n",
              " 'regard': 1523,\n",
              " 'paul': 1046,\n",
              " 'jakma': 27,\n",
              " 'clubi': 10,\n",
              " 'ie': 2395,\n",
              " 'key': 1169,\n",
              " 'id': 1151,\n",
              " '64a2ff6a': 7,\n",
              " 'warning': 296,\n",
              " 'ever': 1612,\n",
              " 'dishone': 12,\n",
              " 'st': 1118,\n",
              " 'fortune': 381,\n",
              " 'one': 14319,\n",
              " 'nuclear': 81,\n",
              " 'bomb': 54,\n",
              " 'ruin': 31,\n",
              " 'whole': 1019,\n",
              " 'irish': 778,\n",
              " 'user': 3961,\n",
              " 'ilug': 1152,\n",
              " 'un': 1202,\n",
              " 'subscription': 1085,\n",
              " 'maintainer': 575,\n",
              " 'listmaster': 564,\n",
              " 'hello': 692,\n",
              " 'ibuyit': 40,\n",
              " 'stating': 74,\n",
              " 'approver': 45,\n",
              " 'invoice': 195,\n",
              " 'requested': 573,\n",
              " 'thanks': 3445,\n",
              " 'shirley': 424,\n",
              " 'forwarded': 1041,\n",
              " 'crenshaw': 250,\n",
              " 'hou': 4108,\n",
              " 'ect': 8484,\n",
              " '04': 1161,\n",
              " '17': 2489,\n",
              " '07': 946,\n",
              " '44': 1157,\n",
              " 'enronxgate': 297,\n",
              " '05': 1331,\n",
              " 'dl': 71,\n",
              " 'payable': 399,\n",
              " 'ou': 319,\n",
              " 'na': 505,\n",
              " 'cn': 123,\n",
              " 'recipient': 363,\n",
              " 'ibuyitpayables': 1,\n",
              " 'ex': 204,\n",
              " 'john': 3385,\n",
              " 'gill': 18,\n",
              " 'eu': 158,\n",
              " 'erin': 25,\n",
              " 'abdelnour': 1,\n",
              " 'shelley': 39,\n",
              " 'robbins': 9,\n",
              " 'sally': 453,\n",
              " 'mcadams': 1,\n",
              " 'joe': 335,\n",
              " 'cuccia': 4,\n",
              " 'judy': 76,\n",
              " 'knepshield': 2,\n",
              " '_': 216611,\n",
              " 'development': 2956,\n",
              " 'thank': 1886,\n",
              " 'identifying': 172,\n",
              " 'future': 2654,\n",
              " 'project': 2862,\n",
              " 'team': 1219,\n",
              " 'make': 7354,\n",
              " 'sure': 2036,\n",
              " 'receive': 3153,\n",
              " 'tool': 1415,\n",
              " 'support': 2229,\n",
              " 'need': 5925,\n",
              " 'successfully': 184,\n",
              " 'transition': 160,\n",
              " 'new': 11828,\n",
              " 'system': 7447,\n",
              " 'may': 8894,\n",
              " 'lst': 195,\n",
              " 'training': 713,\n",
              " 'houston': 1564,\n",
              " 'coder': 22,\n",
              " 'overview': 354,\n",
              " 'session': 3648,\n",
              " 'held': 1761,\n",
              " 'week': 4608,\n",
              " 'monday': 1364,\n",
              " 'thursday': 1173,\n",
              " 'doubletree': 11,\n",
              " 'hotel': 1199,\n",
              " 'nautile': 1,\n",
              " 'room': 1390,\n",
              " '00': 12601,\n",
              " 'hour': 2788,\n",
              " 'demonstration': 283,\n",
              " 'opportunity': 2369,\n",
              " 'ask': 1023,\n",
              " 'question': 4770,\n",
              " 'registration': 2760,\n",
              " 'necessary': 896,\n",
              " 'hand': 1273,\n",
              " 'classroom': 286,\n",
              " 'begin': 769,\n",
              " 'next': 2836,\n",
              " 'complete': 1593,\n",
              " 'real': 2017,\n",
              " 'life': 2212,\n",
              " 'exercise': 361,\n",
              " 'contact': 4506,\n",
              " 'isc': 13,\n",
              " 'registrar': 32,\n",
              " 'register': 856,\n",
              " 'field': 1977,\n",
              " 'worry': 233,\n",
              " 'forgotten': 74,\n",
              " 'material': 1612,\n",
              " 'available': 4748,\n",
              " 'beginning': 672,\n",
              " 'via': 1964,\n",
              " 'integrated': 284,\n",
              " 'solution': 944,\n",
              " 'center': 1762,\n",
              " 'document': 1269,\n",
              " 'library': 644,\n",
              " 'include': 3267,\n",
              " 'step': 1329,\n",
              " 'instruction': 1337,\n",
              " 'help': 3300,\n",
              " 'encourage': 368,\n",
              " 'people': 7457,\n",
              " 'approve': 85,\n",
              " 'address': 10123,\n",
              " 'whether': 1636,\n",
              " 'identified': 344,\n",
              " 'toc': 46,\n",
              " 'journal': 1569,\n",
              " 'african': 440,\n",
              " 'language': 22317,\n",
              " 'linguistics': 8372,\n",
              " 'jall': 3,\n",
              " '19': 2501,\n",
              " '1998': 5327,\n",
              " 'volume': 2303,\n",
              " 'mouton': 387,\n",
              " 'de': 9368,\n",
              " 'gruyter': 459,\n",
              " 'berlin': 639,\n",
              " 'york': 1954,\n",
              " 'marie': 214,\n",
              " 'k': 2044,\n",
              " 'huffman': 24,\n",
              " 'thomas': 498,\n",
              " 'j': 4234,\n",
              " 'hinnebusch': 3,\n",
              " 'phonetic': 454,\n",
              " 'nature': 712,\n",
              " 'voiceless': 63,\n",
              " 'nasal': 104,\n",
              " 'pokomo': 6,\n",
              " 'implication': 411,\n",
              " 'sound': 1387,\n",
              " 'change': 4247,\n",
              " 'kweku': 3,\n",
              " 'osam': 3,\n",
              " 'complementation': 23,\n",
              " 'akan': 6,\n",
              " 'book': 4726,\n",
              " 'review': 2478,\n",
              " 'philip': 215,\n",
              " 'baker': 111,\n",
              " 'creole': 348,\n",
              " 'pidgin': 219,\n",
              " 'varietes': 9,\n",
              " 'vehiculaires': 3,\n",
              " 'proces': 12,\n",
              " 'et': 1760,\n",
              " 'genese': 6,\n",
              " 'gabriel': 35,\n",
              " 'manessy': 3,\n",
              " 'herman': 55,\n",
              " 'batibo': 3,\n",
              " 'topic': 2428,\n",
              " 'salikoko': 14,\n",
              " 'mufwene': 17,\n",
              " 'lioba': 6,\n",
              " 'moshi': 6,\n",
              " 'ed': 1747,\n",
              " 'bruce': 299,\n",
              " 'connell': 29,\n",
              " 'historical': 1054,\n",
              " 'perspective': 973,\n",
              " 'chamba': 3,\n",
              " 'daka': 3,\n",
              " 'raymond': 147,\n",
              " 'boyd': 35,\n",
              " 'jan': 684,\n",
              " 'daeleman': 3,\n",
              " 'luba': 6,\n",
              " 'sprichwoerter': 3,\n",
              " 'uebersetzte': 3,\n",
              " 'erweiterte': 3,\n",
              " 'und': 753,\n",
              " 'ueberarbeitete': 3,\n",
              " 'ausgabe': 7,\n",
              " 'einer': 82,\n",
              " 'anonymen': 3,\n",
              " 'sammlung': 3,\n",
              " 'au': 953,\n",
              " 'zaire': 20,\n",
              " 'tonrelationen': 3,\n",
              " 'sprich': 3,\n",
              " 'woertern': 3,\n",
              " 'reimformen': 3,\n",
              " 'auf': 95,\n",
              " 'suprasegmentaler': 3,\n",
              " 'ebene': 6,\n",
              " 'bei': 66,\n",
              " 'den': 240,\n",
              " 'baluba': 3,\n",
              " 'beena': 3,\n",
              " 'luluwa': 3,\n",
              " 'han': 247,\n",
              " 'ingolf': 39,\n",
              " 'weier': 3,\n",
              " 'jean': 491,\n",
              " 'l': 4753,\n",
              " 'doneux': 3,\n",
              " 'grammar': 2825,\n",
              " 'kisi': 6,\n",
              " 'southern': 547,\n",
              " 'atlantic': 95,\n",
              " 'tucker': 33,\n",
              " 'child': 1777,\n",
              " 'margo': 9,\n",
              " 'fransen': 3,\n",
              " 'discourse': 2354,\n",
              " 'feature': 2025,\n",
              " 'ten': 552,\n",
              " 'west': 934,\n",
              " 'central': 844,\n",
              " 'africa': 491,\n",
              " 'stephen': 396,\n",
              " 'h': 2179,\n",
              " 'levinson': 49,\n",
              " 'kamanda': 3,\n",
              " 'kola': 5,\n",
              " 'la': 2590,\n",
              " 'langue': 138,\n",
              " 'mondo': 17,\n",
              " 'esquisse': 3,\n",
              " 'grammaticale': 12,\n",
              " 'textes': 20,\n",
              " 'dictionnaire': 36,\n",
              " 'andre': 88,\n",
              " 'vallaeys': 3,\n",
              " 'christa': 25,\n",
              " 'koenig': 56,\n",
              " 'perspektiven': 11,\n",
              " 'afrikanistischer': 3,\n",
              " 'forschung': 9,\n",
              " 'beitraege': 16,\n",
              " 'zur': 143,\n",
              " 'linguistik': 53,\n",
              " 'ethnologie': 3,\n",
              " 'geschichte': 41,\n",
              " 'philosophie': 20,\n",
              " 'literatur': 8,\n",
              " 'afrikanistentag': 3,\n",
              " 'bearth': 3,\n",
              " 'wilhelm': 64,\n",
              " 'moehlig': 3,\n",
              " 'beat': 171,\n",
              " 'sottas': 3,\n",
              " 'edgar': 52,\n",
              " 'suter': 4,\n",
              " 'dieke': 7,\n",
              " 'rietkerk': 3,\n",
              " 'talk': 2133,\n",
              " 'thought': 1284,\n",
              " 'thing': 2760,\n",
              " 'emic': 3,\n",
              " 'road': 594,\n",
              " 'towards': 585,\n",
              " 'conscious': 55,\n",
              " 'kenneth': 263,\n",
              " 'pike': 48,\n",
              " 'vincent': 132,\n",
              " 'rooji': 3,\n",
              " 'codeswitching': 30,\n",
              " 'gambia': 7,\n",
              " 'eine': 116,\n",
              " 'soziolinguistische': 3,\n",
              " 'untersuchung': 10,\n",
              " 'von': 370,\n",
              " 'mandinka': 3,\n",
              " 'wolof': 12,\n",
              " 'englisch': 14,\n",
              " 'kontakt': 6,\n",
              " 'delia': 15,\n",
              " 'haust': 3,\n",
              " 'publication': 1682,\n",
              " 'walter': 293,\n",
              " 'inc': 2986,\n",
              " 'postfach': 143,\n",
              " '34': 700,\n",
              " '21': 2025,\n",
              " '200': 1122,\n",
              " 'saw': 455,\n",
              " 'mill': 198,\n",
              " 'river': 276,\n",
              " '10728': 96,\n",
              " 'hawthorne': 103,\n",
              " 'ny': 558,\n",
              " '10532': 96,\n",
              " 'germany': 1307,\n",
              " 'usa': 3428,\n",
              " '49': 1049,\n",
              " '26005': 96,\n",
              " '351': 173,\n",
              " '914': 141,\n",
              " '747': 107,\n",
              " '1326': 99,\n",
              " 'degruyter': 166,\n",
              " 'also': 8971,\n",
              " 'ordered': 487,\n",
              " 'world': 4097,\n",
              " 'wide': 1138,\n",
              " 'web': 4766,\n",
              " 'positive': 459,\n",
              " 'folk': 313,\n",
              " 'done': 1433,\n",
              " 'perhaps': 836,\n",
              " 'someone': 1277,\n",
              " 'drop': 489,\n",
              " 'pointer': 118,\n",
              " 'discussed': 572,\n",
              " 'particular': 1310,\n",
              " 'menu': 180,\n",
              " 'entry': 493,\n",
              " 'remove': 1974,\n",
              " 'sa': 386,\n",
              " 'markup': 43,\n",
              " 'add': 1444,\n",
              " 'whitelisthelp': 1,\n",
              " 'never': 1874,\n",
              " 'customization': 21,\n",
              " 'exmh': 665,\n",
              " 'version': 2360,\n",
              " '23': 1646,\n",
              " '2000': 3862,\n",
              " 'harlan_______________________________________________': 3,\n",
              " 'listman': 136,\n",
              " '06': 920,\n",
              " 'september': 1868,\n",
              " 'tim': 400,\n",
              " 'peter': 879,\n",
              " 'said': 5086,\n",
              " 'case': 4242,\n",
              " 'insensitive': 14,\n",
              " 'different': 3055,\n",
              " 'mime': 220,\n",
              " 'similarly': 94,\n",
              " 'ignoring': 34,\n",
              " 'experiment': 305,\n",
              " 'decide': 393,\n",
              " 'plausible': 61,\n",
              " 'significant': 588,\n",
              " 'generates': 67,\n",
              " 'unusual': 165,\n",
              " 'clueless': 14,\n",
              " 'spammer': 203,\n",
              " 'misconfigures': 3,\n",
              " 'definitely': 273,\n",
              " 'helpful': 252,\n",
              " 'spamassassin': 1567,\n",
              " 'ha': 13283,\n",
              " 'rule': 1593,\n",
              " 'date': 4011,\n",
              " 'point': 3001,\n",
              " 'greg': 347,\n",
              " 'ward': 89,\n",
              " 'gerg': 6,\n",
              " 'ca': 1755,\n",
              " 'god': 373,\n",
              " 'omnipotent': 3,\n",
              " 'omniscient': 2,\n",
              " 'omnibenevolent': 1,\n",
              " 'say': 3361,\n",
              " 'right': 4029,\n",
              " 'label': 358,\n",
              " '793': 13,\n",
              " 'q': 853,\n",
              " 'mohawk': 15,\n",
              " 'russian': 761,\n",
              " 'banning': 36,\n",
              " 'german': 1588,\n",
              " 'mel': 48,\n",
              " 'cuk': 26,\n",
              " 'jun': 185,\n",
              " '1995': 2402,\n",
              " '0400': 154,\n",
              " 'jpkirchner': 33,\n",
              " 'aol': 779,\n",
              " 'banned': 87,\n",
              " 'iowa': 193,\n",
              " 'man': 1163,\n",
              " 'claim': 1488,\n",
              " 'war': 543,\n",
              " 'made': 3385,\n",
              " 'illegal': 158,\n",
              " 'state': 5077,\n",
              " 'decree': 18,\n",
              " 'governor': 147,\n",
              " 'interesting': 903,\n",
              " 'since': 2916,\n",
              " 'family': 1948,\n",
              " 'story': 1001,\n",
              " 'anti': 347,\n",
              " 'discrimination': 40,\n",
              " 'michigan': 256,\n",
              " 'time': 10500,\n",
              " 'limited': 1892,\n",
              " 'snide': 3,\n",
              " 'remark': 276,\n",
              " 'two': 5692,\n",
              " 'surname': 73,\n",
              " 'relevant': 944,\n",
              " 'part': 3491,\n",
              " 'anyone': 2046,\n",
              " 'vouch': 11,\n",
              " 'veracity': 4,\n",
              " 'james': 931,\n",
              " 'kirchner': 68,\n",
              " 'grew': 100,\n",
              " 'household': 89,\n",
              " 'foreign': 1111,\n",
              " 'early': 1199,\n",
              " 'century': 690,\n",
              " 'public': 1266,\n",
              " 'school': 2192,\n",
              " 'english': 6352,\n",
              " 'course': 2571,\n",
              " 'uncle': 43,\n",
              " 'went': 488,\n",
              " 'local': 1342,\n",
              " 'parochial': 20,\n",
              " 'lutheran': 9,\n",
              " 'church': 203,\n",
              " 'liturgy': 9,\n",
              " 'bible': 141,\n",
              " 'luther': 43,\n",
              " 'plattdeutsch': 6,\n",
              " 'speaking': 519,\n",
              " 'go': 3870,\n",
              " 'learn': 1122,\n",
              " 'issued': 317,\n",
              " 'proclamation': 8,\n",
              " 'place': 2981,\n",
              " 'operator': 232,\n",
              " 'instructed': 164,\n",
              " 'pull': 167,\n",
              " 'plug': 156,\n",
              " 'telephone': 1099,\n",
              " 'conversation': 536,\n",
              " 'party': 1272,\n",
              " 'patron': 14,\n",
              " 'hold': 717,\n",
              " 'receiver': 44,\n",
              " 'mouth': 97,\n",
              " 'piece': 517,\n",
              " 'resulting': 274,\n",
              " 'whistling': 7,\n",
              " 'interfere': 32,\n",
              " 'speech': 2931,\n",
              " 'modern': 921,\n",
              " 'dropped': 222,\n",
              " 'curriculum': 207,\n",
              " 'blow': 96,\n",
              " 'really': 1954,\n",
              " 'recovered': 54,\n",
              " 'newspaper': 238,\n",
              " 'published': 1338,\n",
              " 'arrested': 63,\n",
              " 'street': 1356,\n",
              " 'hardship': 31,\n",
              " 'older': 211,\n",
              " 'immigrant': 95,\n",
              " 'suppressed': 8,\n",
              " 'rural': 95,\n",
              " 'county': 209,\n",
              " 'paper': 10431,\n",
              " 'fact': 2277,\n",
              " 'editor': 1019,\n",
              " 'elected': 46,\n",
              " 'treasurer': 88,\n",
              " 'age': 724,\n",
              " 'somewhat': 299,\n",
              " 'embarrassing': 30,\n",
              " 'stumbled': 20,\n",
              " 'organized': 552,\n",
              " 'amounted': 13,\n",
              " 'secret': 856,\n",
              " 'police': 159,\n",
              " 'agency': 682,\n",
              " 'formed': 164,\n",
              " 'whose': 653,\n",
              " 'purpose': 888,\n",
              " 'investigate': 169,\n",
              " 'act': 1540,\n",
              " 'disloyalty': 6,\n",
              " 'given': 2001,\n",
              " 'levy': 42,\n",
              " 'fine': 435,\n",
              " 'imprison': 11,\n",
              " 'duration': 149,\n",
              " 'without': 2468,\n",
              " 'benefit': 948,\n",
              " 'trial': 301,\n",
              " 'america': 1672,\n",
              " 'active': 492,\n",
              " 'participation': 738,\n",
              " 'relatively': 308,\n",
              " 'short': 1563,\n",
              " 'lived': 89,\n",
              " 'around': 1623,\n",
              " 'long': 2713,\n",
              " 'research': 5609,\n",
              " 'sometime': 139,\n",
              " 'record': 1065,\n",
              " 'still': 2581,\n",
              " 'exist': 379,\n",
              " 'happened': 322,\n",
              " 'several': 1985,\n",
              " 'midwest': 66,\n",
              " 'reached': 316,\n",
              " 'supreme': 65,\n",
              " 'court': 640,\n",
              " 'meyer': 120,\n",
              " 'v': 2782,\n",
              " 'nebraska': 77,\n",
              " 'ruled': 60,\n",
              " 'effectively': 189,\n",
              " 'eliminating': 53,\n",
              " 'law': 1130,\n",
              " 'frank': 424,\n",
              " 'anshen': 9,\n",
              " 'dept': 863,\n",
              " 'stony': 30,\n",
              " 'brook': 71,\n",
              " '11794': 6,\n",
              " '1304': 6,\n",
              " 'sum': 705,\n",
              " 'imperialism': 43,\n",
              " 'wed': 317,\n",
              " 'nov': 426,\n",
              " '1994': 1124,\n",
              " 'linguist': 2408,\n",
              " 'came': 752,\n",
              " '18th': 113,\n",
              " 'tan': 29,\n",
              " 'true': 895,\n",
              " 'england': 573,\n",
              " 'fair': 223,\n",
              " '16th': 84,\n",
              " '17th': 113,\n",
              " 'couple': 593,\n",
              " 'example': 3133,\n",
              " 'surely': 227,\n",
              " 'hard': 1589,\n",
              " 'see': 5228,\n",
              " 'relation': 1019,\n",
              " 'humboldt': 62,\n",
              " 'n': 6516,\n",
              " 'even': 3878,\n",
              " 'nation': 558,\n",
              " 'baudoin': 13,\n",
              " 'courtenay': 10,\n",
              " 'poland': 118,\n",
              " 'ditto': 27,\n",
              " 'proponent': 31,\n",
              " 'critical': 441,\n",
              " 'awareness': 115,\n",
              " 'potentially': 138,\n",
              " 'instrument': 128,\n",
              " 'liberation': 16,\n",
              " 'truism': 8,\n",
              " 'science': 2978,\n",
              " 'good': 3337,\n",
              " 'bad': 688,\n",
              " 'seem': 771,\n",
              " 'richard': 783,\n",
              " 'ingham': 32,\n",
              " 'empty': 594,\n",
              " '191': 69,\n",
              " 'opposite': 261,\n",
              " 'known': 937,\n",
              " 'devotee': 12,\n",
              " 'crossword': 7,\n",
              " 'cleave': 10,\n",
              " 'mean': 1853,\n",
              " 'adhere': 31,\n",
              " 'divide': 56,\n",
              " 'cloven': 7,\n",
              " 'hoof': 8,\n",
              " 'meat': 39,\n",
              " 'cleaver': 3,\n",
              " 'hey': 190,\n",
              " 'playing': 248,\n",
              " 'soccer': 23,\n",
              " 'tonight': 105,\n",
              " 'look': 2834,\n",
              " 'pretty': 497,\n",
              " 'much': 3905,\n",
              " 'chickened': 1,\n",
              " 'today': 3059,\n",
              " 'take': 4765,\n",
              " 'courage': 31,\n",
              " 'plan': 2259,\n",
              " 'believe': 1583,\n",
              " 'crystal': 116,\n",
              " 'hyde': 15,\n",
              " 'north': 1310,\n",
              " '121': 93,\n",
              " 'sw': 62,\n",
              " 'salmon': 46,\n",
              " 'portland': 371,\n",
              " 'oregon': 165,\n",
              " '97204': 3,\n",
              " '503': 114,\n",
              " '464': 101,\n",
              " '8318': 2,\n",
              " '3740': 9,\n",
              " 'save': 1808,\n",
              " 'money': 5789,\n",
              " 'getting': 1042,\n",
              " 'oem': 156,\n",
              " 'software': 4091,\n",
              " 'pc': 1053,\n",
              " 'visit': 1909,\n",
              " 'best': 3594,\n",
              " 'lashay': 1,\n",
              " 'milan': 63,\n",
              " 'waterproof': 11,\n",
              " 'stainless': 5,\n",
              " 'steel': 95,\n",
              " 'sapphire': 11,\n",
              " 'surface': 365,\n",
              " 'lovely': 47,\n",
              " 'bring': 889,\n",
              " 'sheer': 19,\n",
              " 'feeling': 215,\n",
              " 'luxury': 36,\n",
              " 'wearing': 31,\n",
              " 'rollexes': 2,\n",
              " 'stylish': 23,\n",
              " 'smart': 234,\n",
              " 'pure': 140,\n",
              " 'gold': 347,\n",
              " 'dazzling': 8,\n",
              " 'diamond': 94,\n",
              " 'rim': 23,\n",
              " 'twinkling': 1,\n",
              " 'star': 368,\n",
              " 'sky': 125,\n",
              " ...}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXm6yay1yqp4",
        "outputId": "81f839da-734c-4b74-df38-8a8fa47ef160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def keep_token(proccessed_token, threshold):\n",
        "  if proccessed_token not in token_counter:\n",
        "    return False\n",
        "  else:\n",
        "    return token_counter[proccessed_token] > threshold\n",
        "\n",
        "keep_token('random', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMiIpz440Lco",
        "outputId": "67b5081f-595d-417f-c715-dabc1aca34b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0',\n",
              " '00',\n",
              " '000',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '08',\n",
              " '09',\n",
              " '1',\n",
              " '10',\n",
              " '100',\n",
              " '11',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '18',\n",
              " '19',\n",
              " '1994',\n",
              " '1995',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '2',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2001',\n",
              " '2002',\n",
              " '21',\n",
              " '22',\n",
              " '23',\n",
              " '24',\n",
              " '25',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '3',\n",
              " '30',\n",
              " '31',\n",
              " '35',\n",
              " '3d',\n",
              " '4',\n",
              " '40',\n",
              " '44',\n",
              " '45',\n",
              " '49',\n",
              " '5',\n",
              " '50',\n",
              " '500',\n",
              " '6',\n",
              " '60',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '90',\n",
              " '95',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '_',\n",
              " 'able',\n",
              " 'abstract',\n",
              " 'ac',\n",
              " 'accepted',\n",
              " 'access',\n",
              " 'account',\n",
              " 'acquisition',\n",
              " 'act',\n",
              " 'action',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'add',\n",
              " 'additional',\n",
              " 'address',\n",
              " 'ago',\n",
              " 'agreement',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'america',\n",
              " 'american',\n",
              " 'among',\n",
              " 'amount',\n",
              " 'analysis',\n",
              " 'analyst',\n",
              " 'announcement',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'application',\n",
              " 'applied',\n",
              " 'approach',\n",
              " 'april',\n",
              " 'area',\n",
              " 'argument',\n",
              " 'around',\n",
              " 'article',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'aspect',\n",
              " 'asset',\n",
              " 'association',\n",
              " 'attached',\n",
              " 'august',\n",
              " 'author',\n",
              " 'available',\n",
              " 'away',\n",
              " 'b',\n",
              " 'back',\n",
              " 'bank',\n",
              " 'based',\n",
              " 'basis',\n",
              " 'become',\n",
              " 'believe',\n",
              " 'benjamin',\n",
              " 'best',\n",
              " 'better',\n",
              " 'big',\n",
              " 'bill',\n",
              " 'billion',\n",
              " 'board',\n",
              " 'book',\n",
              " 'box',\n",
              " 'break',\n",
              " 'bulk',\n",
              " 'business',\n",
              " 'buy',\n",
              " 'c',\n",
              " 'ca',\n",
              " 'california',\n",
              " 'call',\n",
              " 'called',\n",
              " 'canada',\n",
              " 'cannot',\n",
              " 'capital',\n",
              " 'card',\n",
              " 'case',\n",
              " 'cash',\n",
              " 'category',\n",
              " 'cc',\n",
              " 'cd',\n",
              " 'center',\n",
              " 'certain',\n",
              " 'chair',\n",
              " 'change',\n",
              " 'charge',\n",
              " 'check',\n",
              " 'chief',\n",
              " 'child',\n",
              " 'chinese',\n",
              " 'city',\n",
              " 'claim',\n",
              " 'class',\n",
              " 'clear',\n",
              " 'click',\n",
              " 'co',\n",
              " 'code',\n",
              " 'cognitive',\n",
              " 'college',\n",
              " 'color',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comment',\n",
              " 'commercial',\n",
              " 'committee',\n",
              " 'common',\n",
              " 'communication',\n",
              " 'community',\n",
              " 'company',\n",
              " 'complete',\n",
              " 'computational',\n",
              " 'computer',\n",
              " 'conference',\n",
              " 'construction',\n",
              " 'contact',\n",
              " 'content',\n",
              " 'context',\n",
              " 'continue',\n",
              " 'contract',\n",
              " 'control',\n",
              " 'copy',\n",
              " 'copyright',\n",
              " 'corp',\n",
              " 'corpus',\n",
              " 'cost',\n",
              " 'could',\n",
              " 'country',\n",
              " 'course',\n",
              " 'cover',\n",
              " 'credit',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'customer',\n",
              " 'data',\n",
              " 'database',\n",
              " 'date',\n",
              " 'david',\n",
              " 'day',\n",
              " 'de',\n",
              " 'deadline',\n",
              " 'deal',\n",
              " 'dear',\n",
              " 'debt',\n",
              " 'december',\n",
              " 'department',\n",
              " 'description',\n",
              " 'detail',\n",
              " 'development',\n",
              " 'dialect',\n",
              " 'difference',\n",
              " 'different',\n",
              " 'direct',\n",
              " 'director',\n",
              " 'directory',\n",
              " 'discourse',\n",
              " 'discussion',\n",
              " 'document',\n",
              " 'doe',\n",
              " 'dollar',\n",
              " 'domain',\n",
              " 'done',\n",
              " 'dow',\n",
              " 'dr',\n",
              " 'due',\n",
              " 'dynegy',\n",
              " 'e',\n",
              " 'early',\n",
              " 'easy',\n",
              " 'ect',\n",
              " 'ed',\n",
              " 'editor',\n",
              " 'edu',\n",
              " 'education',\n",
              " 'effect',\n",
              " 'effort',\n",
              " 'either',\n",
              " 'electronic',\n",
              " 'else',\n",
              " 'email',\n",
              " 'en',\n",
              " 'end',\n",
              " 'energy',\n",
              " 'english',\n",
              " 'enough',\n",
              " 'enron',\n",
              " 'error',\n",
              " 'especially',\n",
              " 'et',\n",
              " 'etc',\n",
              " 'europe',\n",
              " 'european',\n",
              " 'evaluation',\n",
              " 'even',\n",
              " 'event',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'evidence',\n",
              " 'example',\n",
              " 'exchange',\n",
              " 'executive',\n",
              " 'experience',\n",
              " 'f',\n",
              " 'fact',\n",
              " 'family',\n",
              " 'far',\n",
              " 'fax',\n",
              " 'feature',\n",
              " 'february',\n",
              " 'fee',\n",
              " 'feel',\n",
              " 'field',\n",
              " 'file',\n",
              " 'final',\n",
              " 'financial',\n",
              " 'find',\n",
              " 'first',\n",
              " 'five',\n",
              " 'focus',\n",
              " 'follow',\n",
              " 'following',\n",
              " 'font',\n",
              " 'foreign',\n",
              " 'form',\n",
              " 'format',\n",
              " 'forward',\n",
              " 'forwarded',\n",
              " 'found',\n",
              " 'four',\n",
              " 'france',\n",
              " 'free',\n",
              " 'french',\n",
              " 'friday',\n",
              " 'friend',\n",
              " 'full',\n",
              " 'fund',\n",
              " 'future',\n",
              " 'g',\n",
              " 'gas',\n",
              " 'general',\n",
              " 'german',\n",
              " 'germany',\n",
              " 'get',\n",
              " 'getting',\n",
              " 'give',\n",
              " 'given',\n",
              " 'global',\n",
              " 'go',\n",
              " 'going',\n",
              " 'good',\n",
              " 'got',\n",
              " 'government',\n",
              " 'grammar',\n",
              " 'grant',\n",
              " 'great',\n",
              " 'group',\n",
              " 'h',\n",
              " 'ha',\n",
              " 'hand',\n",
              " 'hard',\n",
              " 'head',\n",
              " 'held',\n",
              " 'help',\n",
              " 'high',\n",
              " 'historical',\n",
              " 'history',\n",
              " 'home',\n",
              " 'hope',\n",
              " 'hotel',\n",
              " 'hou',\n",
              " 'hour',\n",
              " 'houston',\n",
              " 'however',\n",
              " 'html',\n",
              " 'http',\n",
              " 'human',\n",
              " 'hundred',\n",
              " 'id',\n",
              " 'idea',\n",
              " 'ie',\n",
              " 'ilug',\n",
              " 'immediately',\n",
              " 'important',\n",
              " 'inc',\n",
              " 'include',\n",
              " 'included',\n",
              " 'includes',\n",
              " 'including',\n",
              " 'income',\n",
              " 'increase',\n",
              " 'index',\n",
              " 'individual',\n",
              " 'industry',\n",
              " 'info',\n",
              " 'information',\n",
              " 'institute',\n",
              " 'instruction',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'international',\n",
              " 'internet',\n",
              " 'investment',\n",
              " 'investor',\n",
              " 'invited',\n",
              " 'issue',\n",
              " 'item',\n",
              " 'j',\n",
              " 'january',\n",
              " 'japan',\n",
              " 'japanese',\n",
              " 'job',\n",
              " 'john',\n",
              " 'jones',\n",
              " 'journal',\n",
              " 'july',\n",
              " 'june',\n",
              " 'k',\n",
              " 'kaminski',\n",
              " 'keep',\n",
              " 'key',\n",
              " 'kind',\n",
              " 'know',\n",
              " 'knowledge',\n",
              " 'l',\n",
              " 'la',\n",
              " 'language',\n",
              " 'large',\n",
              " 'last',\n",
              " 'late',\n",
              " 'later',\n",
              " 'latest',\n",
              " 'law',\n",
              " 'le',\n",
              " 'learn',\n",
              " 'learning',\n",
              " 'least',\n",
              " 'legal',\n",
              " 'less',\n",
              " 'let',\n",
              " 'letter',\n",
              " 'level',\n",
              " 'lexical',\n",
              " 'life',\n",
              " 'like',\n",
              " 'limited',\n",
              " 'line',\n",
              " 'linguist',\n",
              " 'linguistic',\n",
              " 'linguistics',\n",
              " 'link',\n",
              " 'linux',\n",
              " 'list',\n",
              " 'listinfo',\n",
              " 'little',\n",
              " 'live',\n",
              " 'local',\n",
              " 'logic',\n",
              " 'london',\n",
              " 'long',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'loss',\n",
              " 'lot',\n",
              " 'low',\n",
              " 'lunch',\n",
              " 'machine',\n",
              " 'made',\n",
              " 'mail',\n",
              " 'mailing',\n",
              " 'mailman',\n",
              " 'main',\n",
              " 'major',\n",
              " 'make',\n",
              " 'making',\n",
              " 'man',\n",
              " 'management',\n",
              " 'many',\n",
              " 'march',\n",
              " 'mark',\n",
              " 'market',\n",
              " 'marketing',\n",
              " 'material',\n",
              " 'matter',\n",
              " 'may',\n",
              " 'mean',\n",
              " 'meaning',\n",
              " 'medium',\n",
              " 'meeting',\n",
              " 'member',\n",
              " 'message',\n",
              " 'method',\n",
              " 'michael',\n",
              " 'microsoft',\n",
              " 'might',\n",
              " 'million',\n",
              " 'mind',\n",
              " 'minute',\n",
              " 'model',\n",
              " 'monday',\n",
              " 'money',\n",
              " 'month',\n",
              " 'morphology',\n",
              " 'move',\n",
              " 'movement',\n",
              " 'mr',\n",
              " 'much',\n",
              " 'multi',\n",
              " 'must',\n",
              " 'n',\n",
              " 'name',\n",
              " 'national',\n",
              " 'native',\n",
              " 'natural',\n",
              " 'need',\n",
              " 'net',\n",
              " 'network',\n",
              " 'never',\n",
              " 'new',\n",
              " 'news',\n",
              " 'next',\n",
              " 'nl',\n",
              " 'non',\n",
              " 'north',\n",
              " 'note',\n",
              " 'nothing',\n",
              " 'november',\n",
              " 'number',\n",
              " 'object',\n",
              " 'october',\n",
              " 'offer',\n",
              " 'office',\n",
              " 'old',\n",
              " 'one',\n",
              " 'online',\n",
              " 'open',\n",
              " 'operation',\n",
              " 'opportunity',\n",
              " 'option',\n",
              " 'order',\n",
              " 'org',\n",
              " 'organization',\n",
              " 'original',\n",
              " 'others',\n",
              " 'p',\n",
              " 'package',\n",
              " 'page',\n",
              " 'paid',\n",
              " 'paper',\n",
              " 'part',\n",
              " 'participant',\n",
              " 'particular',\n",
              " 'party',\n",
              " 'past',\n",
              " 'paul',\n",
              " 'pay',\n",
              " 'payment',\n",
              " 'pc',\n",
              " 'people',\n",
              " 'per',\n",
              " 'percent',\n",
              " 'performance',\n",
              " 'person',\n",
              " 'personal',\n",
              " 'phone',\n",
              " 'phonology',\n",
              " 'place',\n",
              " 'plan',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'point',\n",
              " 'policy',\n",
              " 'position',\n",
              " 'possible',\n",
              " 'post',\n",
              " 'potential',\n",
              " 'power',\n",
              " 'pp',\n",
              " 'pragmatic',\n",
              " 'pre',\n",
              " 'present',\n",
              " 'presentation',\n",
              " 'president',\n",
              " 'press',\n",
              " 'price',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'proceeding',\n",
              " 'process',\n",
              " 'processing',\n",
              " 'product',\n",
              " 'production',\n",
              " 'professional',\n",
              " 'professor',\n",
              " 'profit',\n",
              " 'program',\n",
              " 'programme',\n",
              " 'project',\n",
              " 'proposal',\n",
              " 'provide',\n",
              " 'provided',\n",
              " 'public',\n",
              " 'publication',\n",
              " 'published',\n",
              " 'purchase',\n",
              " 'put',\n",
              " 'quality',\n",
              " 'query',\n",
              " 'question',\n",
              " 'r',\n",
              " 'rate',\n",
              " 'rather',\n",
              " 'razor',\n",
              " 'read',\n",
              " 'reading',\n",
              " 'ready',\n",
              " 'real',\n",
              " 'really',\n",
              " 'reason',\n",
              " 'receive',\n",
              " 'received',\n",
              " 'recent',\n",
              " 'record',\n",
              " 'reference',\n",
              " 'regard',\n",
              " 'regarding',\n",
              " 'registration',\n",
              " 'related',\n",
              " 'relation',\n",
              " 'release',\n",
              " 'remember',\n",
              " 'remove',\n",
              " 'removed',\n",
              " 'reply',\n",
              " 'report',\n",
              " 'representation',\n",
              " 'request',\n",
              " 'required',\n",
              " 'research',\n",
              " 'researcher',\n",
              " 'resource',\n",
              " 'response',\n",
              " 'result',\n",
              " 'return',\n",
              " 'review',\n",
              " 'right',\n",
              " 'risk',\n",
              " 'robert',\n",
              " 'role',\n",
              " 'room',\n",
              " 'rpm',\n",
              " 'rule',\n",
              " 'run',\n",
              " 'said',\n",
              " 'sale',\n",
              " 'save',\n",
              " 'say',\n",
              " 'schedule',\n",
              " 'school',\n",
              " 'science',\n",
              " 'search',\n",
              " 'second',\n",
              " 'section',\n",
              " 'security',\n",
              " 'see',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'sell',\n",
              " 'semantic',\n",
              " 'semantics',\n",
              " 'send',\n",
              " 'sent',\n",
              " 'sentence',\n",
              " 'september',\n",
              " 'series',\n",
              " 'server',\n",
              " 'service',\n",
              " 'session',\n",
              " 'set',\n",
              " 'several',\n",
              " 'share',\n",
              " 'short',\n",
              " 'show',\n",
              " 'sign',\n",
              " 'simple',\n",
              " 'simply',\n",
              " 'since',\n",
              " 'single',\n",
              " 'site',\n",
              " 'size',\n",
              " 'small',\n",
              " 'social',\n",
              " 'society',\n",
              " 'software',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'soon',\n",
              " 'sound',\n",
              " 'source',\n",
              " 'sourceforge',\n",
              " 'south',\n",
              " 'spam',\n",
              " 'spamassassin',\n",
              " 'spanish',\n",
              " 'speaker',\n",
              " 'special',\n",
              " 'specific',\n",
              " 'speech',\n",
              " 'spoken',\n",
              " 'st',\n",
              " 'standard',\n",
              " 'start',\n",
              " 'state',\n",
              " 'statement',\n",
              " 'status',\n",
              " 'step',\n",
              " 'still',\n",
              " 'stock',\n",
              " 'stop',\n",
              " 'story',\n",
              " 'street',\n",
              " 'structure',\n",
              " 'student',\n",
              " 'study',\n",
              " 'style',\n",
              " 'subject',\n",
              " 'submission',\n",
              " 'submit',\n",
              " 'subscription',\n",
              " 'success',\n",
              " 'summary',\n",
              " 'summer',\n",
              " 'support',\n",
              " 'sure',\n",
              " 'syntactic',\n",
              " 'syntax',\n",
              " 'system',\n",
              " 'take',\n",
              " 'talk',\n",
              " 'teaching',\n",
              " 'team',\n",
              " 'technical',\n",
              " 'technology',\n",
              " 'tel',\n",
              " 'telephone',\n",
              " 'tell',\n",
              " 'term',\n",
              " 'texas',\n",
              " 'text',\n",
              " 'th',\n",
              " 'thank',\n",
              " 'thanks',\n",
              " 'theme',\n",
              " 'theoretical',\n",
              " 'theory',\n",
              " 'thing',\n",
              " 'think',\n",
              " 'third',\n",
              " 'though',\n",
              " 'thought',\n",
              " 'thousand',\n",
              " 'three',\n",
              " 'thursday',\n",
              " 'time',\n",
              " 'title',\n",
              " 'today',\n",
              " 'together',\n",
              " 'tool',\n",
              " 'top',\n",
              " 'topic',\n",
              " 'total',\n",
              " 'trade',\n",
              " 'trading',\n",
              " 'transaction',\n",
              " 'translation',\n",
              " 'try',\n",
              " 'tuesday',\n",
              " 'tutorial',\n",
              " 'two',\n",
              " 'type',\n",
              " 'u',\n",
              " 'uk',\n",
              " 'un',\n",
              " 'unit',\n",
              " 'united',\n",
              " 'university',\n",
              " 'update',\n",
              " 'url',\n",
              " 'usa',\n",
              " 'use',\n",
              " 'used',\n",
              " 'user',\n",
              " 'using',\n",
              " 'v',\n",
              " 'value',\n",
              " 'various',\n",
              " 'verb',\n",
              " 'version',\n",
              " 'via',\n",
              " 'video',\n",
              " 'view',\n",
              " 'vince',\n",
              " 'visit',\n",
              " 'volume',\n",
              " 'vowel',\n",
              " 'w',\n",
              " 'wa',\n",
              " 'want',\n",
              " 'way',\n",
              " 'web',\n",
              " 'website',\n",
              " 'wednesday',\n",
              " 'week',\n",
              " 'welcome',\n",
              " 'well',\n",
              " 'whether',\n",
              " 'whole',\n",
              " 'wide',\n",
              " 'window',\n",
              " 'wish',\n",
              " 'within',\n",
              " 'without',\n",
              " 'woman',\n",
              " 'word',\n",
              " 'work',\n",
              " 'working',\n",
              " 'workshop',\n",
              " 'world',\n",
              " 'would',\n",
              " 'write',\n",
              " 'writing',\n",
              " 'written',\n",
              " 'wrote',\n",
              " 'www',\n",
              " 'x',\n",
              " 'yahoo',\n",
              " 'year',\n",
              " 'yes',\n",
              " 'yet',\n",
              " 'york',\n",
              " '½ï',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ï'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = set()\n",
        "\n",
        "for token in token_counter:\n",
        "  if keep_token(token, 1000):\n",
        "    features.add(token)\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLz9f1X73cwq",
        "outputId": "28b15d0d-5843-437f-b89c-bdf3abb540df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['away',\n",
              " 'bulk',\n",
              " 'learn',\n",
              " 'volume',\n",
              " 'week',\n",
              " 'august',\n",
              " 'sentence',\n",
              " '10',\n",
              " 'internet',\n",
              " 'direct',\n",
              " 'package',\n",
              " 'certain',\n",
              " '13',\n",
              " 'native',\n",
              " 'enough',\n",
              " 'subscription',\n",
              " 'paul',\n",
              " 'sourceforge',\n",
              " 'applied',\n",
              " '4',\n",
              " 'never',\n",
              " 'news',\n",
              " 'total',\n",
              " 'day',\n",
              " 'multi',\n",
              " '9',\n",
              " 'dollar',\n",
              " 'michael',\n",
              " 'society',\n",
              " 'proceeding',\n",
              " '00',\n",
              " 'remove',\n",
              " 'month',\n",
              " 'save',\n",
              " 'rpm',\n",
              " 'matter',\n",
              " 'number',\n",
              " 'many',\n",
              " 'pm',\n",
              " 'mailing',\n",
              " 'discourse',\n",
              " 'discussion',\n",
              " 'session',\n",
              " 'london',\n",
              " 'capital',\n",
              " 'people',\n",
              " 'status',\n",
              " 'section',\n",
              " 'transaction',\n",
              " 'speech',\n",
              " 'life',\n",
              " 'tell',\n",
              " 'said',\n",
              " 'go',\n",
              " 'fee',\n",
              " 'february',\n",
              " '2002',\n",
              " 'among',\n",
              " 'st',\n",
              " 'proposal',\n",
              " 'including',\n",
              " 'object',\n",
              " 'social',\n",
              " 'course',\n",
              " 'due',\n",
              " 'regarding',\n",
              " 'context',\n",
              " 'org',\n",
              " 'inc',\n",
              " 'book',\n",
              " 'based',\n",
              " 'want',\n",
              " 'web',\n",
              " 'market',\n",
              " 'david',\n",
              " 'account',\n",
              " 'keep',\n",
              " 'database',\n",
              " 'believe',\n",
              " 'email',\n",
              " 'ï',\n",
              " 'immediately',\n",
              " 'reference',\n",
              " 'microsoft',\n",
              " 'specific',\n",
              " 'click',\n",
              " 'grant',\n",
              " 'whole',\n",
              " 'loss',\n",
              " 'department',\n",
              " 'claim',\n",
              " '22',\n",
              " 'three',\n",
              " 'john',\n",
              " 'â',\n",
              " 'college',\n",
              " 'everyone',\n",
              " 'construction',\n",
              " 'december',\n",
              " 'performance',\n",
              " 'woman',\n",
              " 'school',\n",
              " 'high',\n",
              " 'government',\n",
              " '40',\n",
              " 'four',\n",
              " 'series',\n",
              " 'see',\n",
              " 'point',\n",
              " 'percent',\n",
              " 'text',\n",
              " 'france',\n",
              " 'style',\n",
              " 'security',\n",
              " 'trading',\n",
              " '99',\n",
              " 'going',\n",
              " 'long',\n",
              " 'submission',\n",
              " 'phonology',\n",
              " 'minute',\n",
              " 'pc',\n",
              " 'software',\n",
              " 'deadline',\n",
              " 'get',\n",
              " 'check',\n",
              " 'k',\n",
              " 'called',\n",
              " 'writing',\n",
              " 'american',\n",
              " 'includes',\n",
              " 'issue',\n",
              " 'provided',\n",
              " 'spamassassin',\n",
              " 'type',\n",
              " 'january',\n",
              " 'although',\n",
              " 'directory',\n",
              " 'group',\n",
              " 'statement',\n",
              " 'time',\n",
              " '35',\n",
              " 'hotel',\n",
              " 'one',\n",
              " 'review',\n",
              " 'limited',\n",
              " 'look',\n",
              " 'speaker',\n",
              " 'result',\n",
              " '21',\n",
              " 'pay',\n",
              " 'possible',\n",
              " 'little',\n",
              " 'case',\n",
              " 'effect',\n",
              " 'required',\n",
              " 'listinfo',\n",
              " 'individual',\n",
              " 'today',\n",
              " 'step',\n",
              " 'year',\n",
              " 'index',\n",
              " 'ready',\n",
              " 'room',\n",
              " 'organization',\n",
              " 'submit',\n",
              " 'aspect',\n",
              " 'street',\n",
              " 'asset',\n",
              " 'would',\n",
              " 'via',\n",
              " 'york',\n",
              " 'phone',\n",
              " 'cc',\n",
              " 'un',\n",
              " 'resource',\n",
              " 'contract',\n",
              " 'electronic',\n",
              " 'development',\n",
              " 'bill',\n",
              " 'plus',\n",
              " 'service',\n",
              " 'ed',\n",
              " '3',\n",
              " 'plan',\n",
              " 'forwarded',\n",
              " 'full',\n",
              " 'right',\n",
              " 'included',\n",
              " 'medium',\n",
              " 'investor',\n",
              " 'schedule',\n",
              " 'got',\n",
              " 'others',\n",
              " 'national',\n",
              " 'child',\n",
              " 'best',\n",
              " '98',\n",
              " 'give',\n",
              " 'ever',\n",
              " 'mind',\n",
              " 'university',\n",
              " 'color',\n",
              " 'production',\n",
              " 'probably',\n",
              " '28',\n",
              " 'man',\n",
              " 'box',\n",
              " 'dow',\n",
              " '25',\n",
              " 'also',\n",
              " 'director',\n",
              " 'operation',\n",
              " 'pragmatic',\n",
              " 'thursday',\n",
              " 'september',\n",
              " 'hard',\n",
              " 'home',\n",
              " 'x',\n",
              " 'search',\n",
              " 'try',\n",
              " 'future',\n",
              " 'several',\n",
              " 'third',\n",
              " 'office',\n",
              " 'attached',\n",
              " 'benjamin',\n",
              " 'late',\n",
              " 'tel',\n",
              " 'document',\n",
              " 'computer',\n",
              " '3d',\n",
              " 'participant',\n",
              " 'move',\n",
              " 'follow',\n",
              " 'letter',\n",
              " 'published',\n",
              " 'pre',\n",
              " 'publication',\n",
              " 'relation',\n",
              " 'marketing',\n",
              " 'think',\n",
              " 'font',\n",
              " 'forward',\n",
              " 'simple',\n",
              " 'professor',\n",
              " 'regard',\n",
              " 'etc',\n",
              " 'company',\n",
              " 'last',\n",
              " 'know',\n",
              " 'theoretical',\n",
              " 'post',\n",
              " 'united',\n",
              " 'include',\n",
              " 'area',\n",
              " 'announcement',\n",
              " 'video',\n",
              " 'registration',\n",
              " 'must',\n",
              " 'author',\n",
              " 'approach',\n",
              " 'additional',\n",
              " 'increase',\n",
              " '1',\n",
              " 'query',\n",
              " 'structure',\n",
              " 'become',\n",
              " '16',\n",
              " 'less',\n",
              " 'robert',\n",
              " 'process',\n",
              " 'country',\n",
              " 'cover',\n",
              " 'method',\n",
              " 'syntactic',\n",
              " 'least',\n",
              " 'recent',\n",
              " 'friday',\n",
              " 'seems',\n",
              " 'website',\n",
              " 'id',\n",
              " 'short',\n",
              " 'tool',\n",
              " 'association',\n",
              " '0',\n",
              " 'quality',\n",
              " 'report',\n",
              " '29',\n",
              " 'however',\n",
              " 'ask',\n",
              " 'conference',\n",
              " 'may',\n",
              " 'semantic',\n",
              " 'difference',\n",
              " 'usa',\n",
              " 'everything',\n",
              " 'local',\n",
              " 'bank',\n",
              " 'provide',\n",
              " 'story',\n",
              " 'url',\n",
              " 'written',\n",
              " 'dialect',\n",
              " 'comment',\n",
              " 'second',\n",
              " 'energy',\n",
              " 'chinese',\n",
              " 'need',\n",
              " '26',\n",
              " 'machine',\n",
              " 'customer',\n",
              " 'monday',\n",
              " 'linux',\n",
              " 'payment',\n",
              " 'kind',\n",
              " 'update',\n",
              " 'board',\n",
              " 'debt',\n",
              " 'later',\n",
              " 'original',\n",
              " 'done',\n",
              " 'simply',\n",
              " 'fund',\n",
              " 'processing',\n",
              " 'chief',\n",
              " '2',\n",
              " 'non',\n",
              " 'editor',\n",
              " 'release',\n",
              " 'argument',\n",
              " 'remember',\n",
              " 'spanish',\n",
              " 'sale',\n",
              " 'description',\n",
              " 'take',\n",
              " 'pp',\n",
              " 'cannot',\n",
              " 'experience',\n",
              " 'old',\n",
              " 'copyright',\n",
              " 'verb',\n",
              " '60',\n",
              " 'unit',\n",
              " 'well',\n",
              " 'meaning',\n",
              " 'dr',\n",
              " 'state',\n",
              " 'general',\n",
              " 'find',\n",
              " 'california',\n",
              " 'credit',\n",
              " 'could',\n",
              " 'article',\n",
              " '2001',\n",
              " 'two',\n",
              " 'basis',\n",
              " 'agreement',\n",
              " 'say',\n",
              " 'teaching',\n",
              " '18',\n",
              " 'soon',\n",
              " 'version',\n",
              " 'user',\n",
              " 'germany',\n",
              " 'act',\n",
              " 'north',\n",
              " 'date',\n",
              " 'knowledge',\n",
              " 'cash',\n",
              " 'access',\n",
              " 'show',\n",
              " 'online',\n",
              " 'set',\n",
              " 'another',\n",
              " 'looking',\n",
              " 'content',\n",
              " 'new',\n",
              " 'information',\n",
              " 'latest',\n",
              " 'effort',\n",
              " 'texas',\n",
              " 'abstract',\n",
              " '1994',\n",
              " 'great',\n",
              " 'line',\n",
              " 'whether',\n",
              " 'rate',\n",
              " 'source',\n",
              " 'corp',\n",
              " 'particular',\n",
              " '19',\n",
              " 'july',\n",
              " '95',\n",
              " 'received',\n",
              " 'reading',\n",
              " 'special',\n",
              " 'since',\n",
              " 'receive',\n",
              " 'tutorial',\n",
              " 'stop',\n",
              " 'sound',\n",
              " '49',\n",
              " 'together',\n",
              " 'focus',\n",
              " 'student',\n",
              " 'action',\n",
              " 'family',\n",
              " 'card',\n",
              " 'return',\n",
              " 'every',\n",
              " 'business',\n",
              " 'copy',\n",
              " 'head',\n",
              " 'following',\n",
              " '1995',\n",
              " 'gas',\n",
              " 'per',\n",
              " 'human',\n",
              " 'french',\n",
              " 'doe',\n",
              " 'www',\n",
              " 'still',\n",
              " 'management',\n",
              " 'ac',\n",
              " 'ilug',\n",
              " 'linguist',\n",
              " 'end',\n",
              " 'linguistic',\n",
              " 'policy',\n",
              " 'h',\n",
              " 'et',\n",
              " 'question',\n",
              " 'amount',\n",
              " 'item',\n",
              " 'though',\n",
              " 'power',\n",
              " 'workshop',\n",
              " 'control',\n",
              " 'de',\n",
              " '000',\n",
              " '½ï',\n",
              " 'info',\n",
              " 'ã',\n",
              " 'thing',\n",
              " '2000',\n",
              " 'working',\n",
              " 'buy',\n",
              " 'g',\n",
              " '5',\n",
              " 'available',\n",
              " 'european',\n",
              " 'much',\n",
              " 'history',\n",
              " 'easy',\n",
              " 'value',\n",
              " 'ago',\n",
              " 'journal',\n",
              " 'yahoo',\n",
              " 'name',\n",
              " 'response',\n",
              " 'clear',\n",
              " '01',\n",
              " 'form',\n",
              " 'either',\n",
              " '200',\n",
              " 'getting',\n",
              " 'nothing',\n",
              " '23',\n",
              " 'hand',\n",
              " 'answer',\n",
              " 'window',\n",
              " 'team',\n",
              " 'fact',\n",
              " '14',\n",
              " 'top',\n",
              " 'fax',\n",
              " 'v',\n",
              " '7',\n",
              " 'science',\n",
              " 'kaminski',\n",
              " 'better',\n",
              " 'legal',\n",
              " 'evidence',\n",
              " 'interest',\n",
              " 'german',\n",
              " 'sent',\n",
              " 'even',\n",
              " 'march',\n",
              " 'reply',\n",
              " 'call',\n",
              " 'theme',\n",
              " 'thanks',\n",
              " 'u',\n",
              " 'party',\n",
              " 'investment',\n",
              " 'jones',\n",
              " '02',\n",
              " 'server',\n",
              " 'network',\n",
              " 'study',\n",
              " 'uk',\n",
              " 'razor',\n",
              " 'edu',\n",
              " 'yet',\n",
              " 'c',\n",
              " 'start',\n",
              " 'without',\n",
              " 'dear',\n",
              " 'system',\n",
              " '6',\n",
              " 'anyone',\n",
              " 'term',\n",
              " 'change',\n",
              " 'tuesday',\n",
              " 'good',\n",
              " 'low',\n",
              " 'representation',\n",
              " 'part',\n",
              " 'morphology',\n",
              " 'idea',\n",
              " '1998',\n",
              " 'word',\n",
              " 'houston',\n",
              " '09',\n",
              " 'data',\n",
              " 'press',\n",
              " 'category',\n",
              " 'e',\n",
              " 'add',\n",
              " '500',\n",
              " 'let',\n",
              " 'world',\n",
              " 'program',\n",
              " 'currently',\n",
              " 'mark',\n",
              " 'center',\n",
              " 'project',\n",
              " '04',\n",
              " 'paid',\n",
              " 'cognitive',\n",
              " 'ha',\n",
              " 'com',\n",
              " 'actually',\n",
              " '08',\n",
              " '1997',\n",
              " 'free',\n",
              " 'spam',\n",
              " 'common',\n",
              " 'r',\n",
              " 'executive',\n",
              " 'html',\n",
              " 'role',\n",
              " 'used',\n",
              " 'wish',\n",
              " 'read',\n",
              " 'wa',\n",
              " 'request',\n",
              " 'job',\n",
              " 'live',\n",
              " 'removed',\n",
              " 'always',\n",
              " 'technical',\n",
              " 'europe',\n",
              " 'anything',\n",
              " 'first',\n",
              " 'address',\n",
              " 'material',\n",
              " 'using',\n",
              " 'talk',\n",
              " 'instruction',\n",
              " 'le',\n",
              " '20',\n",
              " 'billion',\n",
              " 'telephone',\n",
              " 'foreign',\n",
              " 'http',\n",
              " 'big',\n",
              " 'order',\n",
              " 'detail',\n",
              " 'break',\n",
              " 'cost',\n",
              " 'complete',\n",
              " 'please',\n",
              " 'ie',\n",
              " 'feel',\n",
              " 'mail',\n",
              " 'october',\n",
              " 'syntax',\n",
              " 'yes',\n",
              " 'la',\n",
              " 'example',\n",
              " 'evaluation',\n",
              " 'come',\n",
              " 'link',\n",
              " 'might',\n",
              " 'j',\n",
              " 'final',\n",
              " 'invited',\n",
              " 'summer',\n",
              " 'back',\n",
              " 'cd',\n",
              " 'level',\n",
              " 'movement',\n",
              " 'someone',\n",
              " 'title',\n",
              " 'net',\n",
              " '05',\n",
              " 'five',\n",
              " 'public',\n",
              " 'support',\n",
              " 'mailman',\n",
              " 'major',\n",
              " 'put',\n",
              " 'wednesday',\n",
              " 'hour',\n",
              " 'english',\n",
              " 'key',\n",
              " '27',\n",
              " 'present',\n",
              " 'personal',\n",
              " 'note',\n",
              " 'education',\n",
              " 'accepted',\n",
              " 'continue',\n",
              " '50',\n",
              " 'making',\n",
              " 'way',\n",
              " 'language',\n",
              " 'found',\n",
              " 'standard',\n",
              " 'l',\n",
              " 'past',\n",
              " '97',\n",
              " 'income',\n",
              " 'community',\n",
              " '1999',\n",
              " 'format',\n",
              " 'use',\n",
              " 'single',\n",
              " 'wrote',\n",
              " 'vince',\n",
              " 'code',\n",
              " 'presentation',\n",
              " 'seen',\n",
              " 'application',\n",
              " 'opportunity',\n",
              " 'n',\n",
              " 'file',\n",
              " 'learning',\n",
              " 'welcome',\n",
              " 'interested',\n",
              " 'mr',\n",
              " 'f',\n",
              " 'semantics',\n",
              " '15',\n",
              " 'corpus',\n",
              " 'mean',\n",
              " 'thousand',\n",
              " 'acquisition',\n",
              " 'logic',\n",
              " 'south',\n",
              " 'large',\n",
              " 'analysis',\n",
              " '31',\n",
              " 'historical',\n",
              " 'early',\n",
              " 'related',\n",
              " 'spoken',\n",
              " 'professional',\n",
              " 'analyst',\n",
              " 'open',\n",
              " 'potential',\n",
              " 'lexical',\n",
              " 'help',\n",
              " 'deal',\n",
              " 'japanese',\n",
              " '30',\n",
              " 'reason',\n",
              " 'real',\n",
              " 'run',\n",
              " 'small',\n",
              " 'already',\n",
              " 'ect',\n",
              " 'rule',\n",
              " 'error',\n",
              " 'th',\n",
              " 'especially',\n",
              " '90',\n",
              " 'contact',\n",
              " 'size',\n",
              " 'product',\n",
              " 'natural',\n",
              " 'sign',\n",
              " 'message',\n",
              " 'held',\n",
              " 'able',\n",
              " 'nl',\n",
              " 'w',\n",
              " 'current',\n",
              " 'far',\n",
              " 'technology',\n",
              " 'else',\n",
              " 'domain',\n",
              " '100',\n",
              " 'canada',\n",
              " 'person',\n",
              " 'subject',\n",
              " 'hou',\n",
              " 'communication',\n",
              " '45',\n",
              " 'institute',\n",
              " 'b',\n",
              " 'main',\n",
              " 'p',\n",
              " 'asked',\n",
              " 'option',\n",
              " 'view',\n",
              " 'position',\n",
              " 'research',\n",
              " 'charge',\n",
              " 'meeting',\n",
              " 'class',\n",
              " 'hundred',\n",
              " 'researcher',\n",
              " 'november',\n",
              " '17',\n",
              " 'different',\n",
              " 'million',\n",
              " 'vowel',\n",
              " '8',\n",
              " 'president',\n",
              " 'japan',\n",
              " 'like',\n",
              " 'city',\n",
              " 'wide',\n",
              " 'write',\n",
              " 'site',\n",
              " 'international',\n",
              " '03',\n",
              " 'america',\n",
              " 'profit',\n",
              " '12',\n",
              " 'en',\n",
              " 'visit',\n",
              " 'enron',\n",
              " '11',\n",
              " 'place',\n",
              " 'success',\n",
              " 'thought',\n",
              " 'linguistics',\n",
              " 'work',\n",
              " 'risk',\n",
              " 'price',\n",
              " 'committee',\n",
              " 'share',\n",
              " 'industry',\n",
              " 'ad',\n",
              " 'list',\n",
              " 'really',\n",
              " '44',\n",
              " 'grammar',\n",
              " 'april',\n",
              " 'next',\n",
              " 'computational',\n",
              " 'problem',\n",
              " 'money',\n",
              " 'translation',\n",
              " 'member',\n",
              " 'lunch',\n",
              " 'purchase',\n",
              " 'theory',\n",
              " 'something',\n",
              " '24',\n",
              " 'within',\n",
              " 'made',\n",
              " 'model',\n",
              " 'dynegy',\n",
              " '_',\n",
              " 'field',\n",
              " 'thank',\n",
              " 'record',\n",
              " 'various',\n",
              " 'summary',\n",
              " 'chair',\n",
              " 'rather',\n",
              " 'topic',\n",
              " 'sell',\n",
              " 'global',\n",
              " 'programme',\n",
              " 'sure',\n",
              " 'feature',\n",
              " 'important',\n",
              " 'event',\n",
              " 'friend',\n",
              " 'june',\n",
              " 'page',\n",
              " 'exchange',\n",
              " 'financial',\n",
              " 'commercial',\n",
              " 'paper',\n",
              " 'around',\n",
              " 'send',\n",
              " 'law',\n",
              " 'hope',\n",
              " 'ca',\n",
              " 'lot',\n",
              " 'offer',\n",
              " 'stock',\n",
              " 'make',\n",
              " 'given',\n",
              " 'trade',\n",
              " 'co']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = list(features)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8357HG1Q3itk",
        "outputId": "188a7383-4bdb-49e5-9f40-ef14e28b6258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'away': 0,\n",
              " 'bulk': 1,\n",
              " 'learn': 2,\n",
              " 'volume': 3,\n",
              " 'week': 4,\n",
              " 'august': 5,\n",
              " 'sentence': 6,\n",
              " '10': 7,\n",
              " 'internet': 8,\n",
              " 'direct': 9,\n",
              " 'package': 10,\n",
              " 'certain': 11,\n",
              " '13': 12,\n",
              " 'native': 13,\n",
              " 'enough': 14,\n",
              " 'subscription': 15,\n",
              " 'paul': 16,\n",
              " 'sourceforge': 17,\n",
              " 'applied': 18,\n",
              " '4': 19,\n",
              " 'never': 20,\n",
              " 'news': 21,\n",
              " 'total': 22,\n",
              " 'day': 23,\n",
              " 'multi': 24,\n",
              " '9': 25,\n",
              " 'dollar': 26,\n",
              " 'michael': 27,\n",
              " 'society': 28,\n",
              " 'proceeding': 29,\n",
              " '00': 30,\n",
              " 'remove': 31,\n",
              " 'month': 32,\n",
              " 'save': 33,\n",
              " 'rpm': 34,\n",
              " 'matter': 35,\n",
              " 'number': 36,\n",
              " 'many': 37,\n",
              " 'pm': 38,\n",
              " 'mailing': 39,\n",
              " 'discourse': 40,\n",
              " 'discussion': 41,\n",
              " 'session': 42,\n",
              " 'london': 43,\n",
              " 'capital': 44,\n",
              " 'people': 45,\n",
              " 'status': 46,\n",
              " 'section': 47,\n",
              " 'transaction': 48,\n",
              " 'speech': 49,\n",
              " 'life': 50,\n",
              " 'tell': 51,\n",
              " 'said': 52,\n",
              " 'go': 53,\n",
              " 'fee': 54,\n",
              " 'february': 55,\n",
              " '2002': 56,\n",
              " 'among': 57,\n",
              " 'st': 58,\n",
              " 'proposal': 59,\n",
              " 'including': 60,\n",
              " 'object': 61,\n",
              " 'social': 62,\n",
              " 'course': 63,\n",
              " 'due': 64,\n",
              " 'regarding': 65,\n",
              " 'context': 66,\n",
              " 'org': 67,\n",
              " 'inc': 68,\n",
              " 'book': 69,\n",
              " 'based': 70,\n",
              " 'want': 71,\n",
              " 'web': 72,\n",
              " 'market': 73,\n",
              " 'david': 74,\n",
              " 'account': 75,\n",
              " 'keep': 76,\n",
              " 'database': 77,\n",
              " 'believe': 78,\n",
              " 'email': 79,\n",
              " 'ï': 80,\n",
              " 'immediately': 81,\n",
              " 'reference': 82,\n",
              " 'microsoft': 83,\n",
              " 'specific': 84,\n",
              " 'click': 85,\n",
              " 'grant': 86,\n",
              " 'whole': 87,\n",
              " 'loss': 88,\n",
              " 'department': 89,\n",
              " 'claim': 90,\n",
              " '22': 91,\n",
              " 'three': 92,\n",
              " 'john': 93,\n",
              " 'â': 94,\n",
              " 'college': 95,\n",
              " 'everyone': 96,\n",
              " 'construction': 97,\n",
              " 'december': 98,\n",
              " 'performance': 99,\n",
              " 'woman': 100,\n",
              " 'school': 101,\n",
              " 'high': 102,\n",
              " 'government': 103,\n",
              " '40': 104,\n",
              " 'four': 105,\n",
              " 'series': 106,\n",
              " 'see': 107,\n",
              " 'point': 108,\n",
              " 'percent': 109,\n",
              " 'text': 110,\n",
              " 'france': 111,\n",
              " 'style': 112,\n",
              " 'security': 113,\n",
              " 'trading': 114,\n",
              " '99': 115,\n",
              " 'going': 116,\n",
              " 'long': 117,\n",
              " 'submission': 118,\n",
              " 'phonology': 119,\n",
              " 'minute': 120,\n",
              " 'pc': 121,\n",
              " 'software': 122,\n",
              " 'deadline': 123,\n",
              " 'get': 124,\n",
              " 'check': 125,\n",
              " 'k': 126,\n",
              " 'called': 127,\n",
              " 'writing': 128,\n",
              " 'american': 129,\n",
              " 'includes': 130,\n",
              " 'issue': 131,\n",
              " 'provided': 132,\n",
              " 'spamassassin': 133,\n",
              " 'type': 134,\n",
              " 'january': 135,\n",
              " 'although': 136,\n",
              " 'directory': 137,\n",
              " 'group': 138,\n",
              " 'statement': 139,\n",
              " 'time': 140,\n",
              " '35': 141,\n",
              " 'hotel': 142,\n",
              " 'one': 143,\n",
              " 'review': 144,\n",
              " 'limited': 145,\n",
              " 'look': 146,\n",
              " 'speaker': 147,\n",
              " 'result': 148,\n",
              " '21': 149,\n",
              " 'pay': 150,\n",
              " 'possible': 151,\n",
              " 'little': 152,\n",
              " 'case': 153,\n",
              " 'effect': 154,\n",
              " 'required': 155,\n",
              " 'listinfo': 156,\n",
              " 'individual': 157,\n",
              " 'today': 158,\n",
              " 'step': 159,\n",
              " 'year': 160,\n",
              " 'index': 161,\n",
              " 'ready': 162,\n",
              " 'room': 163,\n",
              " 'organization': 164,\n",
              " 'submit': 165,\n",
              " 'aspect': 166,\n",
              " 'street': 167,\n",
              " 'asset': 168,\n",
              " 'would': 169,\n",
              " 'via': 170,\n",
              " 'york': 171,\n",
              " 'phone': 172,\n",
              " 'cc': 173,\n",
              " 'un': 174,\n",
              " 'resource': 175,\n",
              " 'contract': 176,\n",
              " 'electronic': 177,\n",
              " 'development': 178,\n",
              " 'bill': 179,\n",
              " 'plus': 180,\n",
              " 'service': 181,\n",
              " 'ed': 182,\n",
              " '3': 183,\n",
              " 'plan': 184,\n",
              " 'forwarded': 185,\n",
              " 'full': 186,\n",
              " 'right': 187,\n",
              " 'included': 188,\n",
              " 'medium': 189,\n",
              " 'investor': 190,\n",
              " 'schedule': 191,\n",
              " 'got': 192,\n",
              " 'others': 193,\n",
              " 'national': 194,\n",
              " 'child': 195,\n",
              " 'best': 196,\n",
              " '98': 197,\n",
              " 'give': 198,\n",
              " 'ever': 199,\n",
              " 'mind': 200,\n",
              " 'university': 201,\n",
              " 'color': 202,\n",
              " 'production': 203,\n",
              " 'probably': 204,\n",
              " '28': 205,\n",
              " 'man': 206,\n",
              " 'box': 207,\n",
              " 'dow': 208,\n",
              " '25': 209,\n",
              " 'also': 210,\n",
              " 'director': 211,\n",
              " 'operation': 212,\n",
              " 'pragmatic': 213,\n",
              " 'thursday': 214,\n",
              " 'september': 215,\n",
              " 'hard': 216,\n",
              " 'home': 217,\n",
              " 'x': 218,\n",
              " 'search': 219,\n",
              " 'try': 220,\n",
              " 'future': 221,\n",
              " 'several': 222,\n",
              " 'third': 223,\n",
              " 'office': 224,\n",
              " 'attached': 225,\n",
              " 'benjamin': 226,\n",
              " 'late': 227,\n",
              " 'tel': 228,\n",
              " 'document': 229,\n",
              " 'computer': 230,\n",
              " '3d': 231,\n",
              " 'participant': 232,\n",
              " 'move': 233,\n",
              " 'follow': 234,\n",
              " 'letter': 235,\n",
              " 'published': 236,\n",
              " 'pre': 237,\n",
              " 'publication': 238,\n",
              " 'relation': 239,\n",
              " 'marketing': 240,\n",
              " 'think': 241,\n",
              " 'font': 242,\n",
              " 'forward': 243,\n",
              " 'simple': 244,\n",
              " 'professor': 245,\n",
              " 'regard': 246,\n",
              " 'etc': 247,\n",
              " 'company': 248,\n",
              " 'last': 249,\n",
              " 'know': 250,\n",
              " 'theoretical': 251,\n",
              " 'post': 252,\n",
              " 'united': 253,\n",
              " 'include': 254,\n",
              " 'area': 255,\n",
              " 'announcement': 256,\n",
              " 'video': 257,\n",
              " 'registration': 258,\n",
              " 'must': 259,\n",
              " 'author': 260,\n",
              " 'approach': 261,\n",
              " 'additional': 262,\n",
              " 'increase': 263,\n",
              " '1': 264,\n",
              " 'query': 265,\n",
              " 'structure': 266,\n",
              " 'become': 267,\n",
              " '16': 268,\n",
              " 'less': 269,\n",
              " 'robert': 270,\n",
              " 'process': 271,\n",
              " 'country': 272,\n",
              " 'cover': 273,\n",
              " 'method': 274,\n",
              " 'syntactic': 275,\n",
              " 'least': 276,\n",
              " 'recent': 277,\n",
              " 'friday': 278,\n",
              " 'seems': 279,\n",
              " 'website': 280,\n",
              " 'id': 281,\n",
              " 'short': 282,\n",
              " 'tool': 283,\n",
              " 'association': 284,\n",
              " '0': 285,\n",
              " 'quality': 286,\n",
              " 'report': 287,\n",
              " '29': 288,\n",
              " 'however': 289,\n",
              " 'ask': 290,\n",
              " 'conference': 291,\n",
              " 'may': 292,\n",
              " 'semantic': 293,\n",
              " 'difference': 294,\n",
              " 'usa': 295,\n",
              " 'everything': 296,\n",
              " 'local': 297,\n",
              " 'bank': 298,\n",
              " 'provide': 299,\n",
              " 'story': 300,\n",
              " 'url': 301,\n",
              " 'written': 302,\n",
              " 'dialect': 303,\n",
              " 'comment': 304,\n",
              " 'second': 305,\n",
              " 'energy': 306,\n",
              " 'chinese': 307,\n",
              " 'need': 308,\n",
              " '26': 309,\n",
              " 'machine': 310,\n",
              " 'customer': 311,\n",
              " 'monday': 312,\n",
              " 'linux': 313,\n",
              " 'payment': 314,\n",
              " 'kind': 315,\n",
              " 'update': 316,\n",
              " 'board': 317,\n",
              " 'debt': 318,\n",
              " 'later': 319,\n",
              " 'original': 320,\n",
              " 'done': 321,\n",
              " 'simply': 322,\n",
              " 'fund': 323,\n",
              " 'processing': 324,\n",
              " 'chief': 325,\n",
              " '2': 326,\n",
              " 'non': 327,\n",
              " 'editor': 328,\n",
              " 'release': 329,\n",
              " 'argument': 330,\n",
              " 'remember': 331,\n",
              " 'spanish': 332,\n",
              " 'sale': 333,\n",
              " 'description': 334,\n",
              " 'take': 335,\n",
              " 'pp': 336,\n",
              " 'cannot': 337,\n",
              " 'experience': 338,\n",
              " 'old': 339,\n",
              " 'copyright': 340,\n",
              " 'verb': 341,\n",
              " '60': 342,\n",
              " 'unit': 343,\n",
              " 'well': 344,\n",
              " 'meaning': 345,\n",
              " 'dr': 346,\n",
              " 'state': 347,\n",
              " 'general': 348,\n",
              " 'find': 349,\n",
              " 'california': 350,\n",
              " 'credit': 351,\n",
              " 'could': 352,\n",
              " 'article': 353,\n",
              " '2001': 354,\n",
              " 'two': 355,\n",
              " 'basis': 356,\n",
              " 'agreement': 357,\n",
              " 'say': 358,\n",
              " 'teaching': 359,\n",
              " '18': 360,\n",
              " 'soon': 361,\n",
              " 'version': 362,\n",
              " 'user': 363,\n",
              " 'germany': 364,\n",
              " 'act': 365,\n",
              " 'north': 366,\n",
              " 'date': 367,\n",
              " 'knowledge': 368,\n",
              " 'cash': 369,\n",
              " 'access': 370,\n",
              " 'show': 371,\n",
              " 'online': 372,\n",
              " 'set': 373,\n",
              " 'another': 374,\n",
              " 'looking': 375,\n",
              " 'content': 376,\n",
              " 'new': 377,\n",
              " 'information': 378,\n",
              " 'latest': 379,\n",
              " 'effort': 380,\n",
              " 'texas': 381,\n",
              " 'abstract': 382,\n",
              " '1994': 383,\n",
              " 'great': 384,\n",
              " 'line': 385,\n",
              " 'whether': 386,\n",
              " 'rate': 387,\n",
              " 'source': 388,\n",
              " 'corp': 389,\n",
              " 'particular': 390,\n",
              " '19': 391,\n",
              " 'july': 392,\n",
              " '95': 393,\n",
              " 'received': 394,\n",
              " 'reading': 395,\n",
              " 'special': 396,\n",
              " 'since': 397,\n",
              " 'receive': 398,\n",
              " 'tutorial': 399,\n",
              " 'stop': 400,\n",
              " 'sound': 401,\n",
              " '49': 402,\n",
              " 'together': 403,\n",
              " 'focus': 404,\n",
              " 'student': 405,\n",
              " 'action': 406,\n",
              " 'family': 407,\n",
              " 'card': 408,\n",
              " 'return': 409,\n",
              " 'every': 410,\n",
              " 'business': 411,\n",
              " 'copy': 412,\n",
              " 'head': 413,\n",
              " 'following': 414,\n",
              " '1995': 415,\n",
              " 'gas': 416,\n",
              " 'per': 417,\n",
              " 'human': 418,\n",
              " 'french': 419,\n",
              " 'doe': 420,\n",
              " 'www': 421,\n",
              " 'still': 422,\n",
              " 'management': 423,\n",
              " 'ac': 424,\n",
              " 'ilug': 425,\n",
              " 'linguist': 426,\n",
              " 'end': 427,\n",
              " 'linguistic': 428,\n",
              " 'policy': 429,\n",
              " 'h': 430,\n",
              " 'et': 431,\n",
              " 'question': 432,\n",
              " 'amount': 433,\n",
              " 'item': 434,\n",
              " 'though': 435,\n",
              " 'power': 436,\n",
              " 'workshop': 437,\n",
              " 'control': 438,\n",
              " 'de': 439,\n",
              " '000': 440,\n",
              " '½ï': 441,\n",
              " 'info': 442,\n",
              " 'ã': 443,\n",
              " 'thing': 444,\n",
              " '2000': 445,\n",
              " 'working': 446,\n",
              " 'buy': 447,\n",
              " 'g': 448,\n",
              " '5': 449,\n",
              " 'available': 450,\n",
              " 'european': 451,\n",
              " 'much': 452,\n",
              " 'history': 453,\n",
              " 'easy': 454,\n",
              " 'value': 455,\n",
              " 'ago': 456,\n",
              " 'journal': 457,\n",
              " 'yahoo': 458,\n",
              " 'name': 459,\n",
              " 'response': 460,\n",
              " 'clear': 461,\n",
              " '01': 462,\n",
              " 'form': 463,\n",
              " 'either': 464,\n",
              " '200': 465,\n",
              " 'getting': 466,\n",
              " 'nothing': 467,\n",
              " '23': 468,\n",
              " 'hand': 469,\n",
              " 'answer': 470,\n",
              " 'window': 471,\n",
              " 'team': 472,\n",
              " 'fact': 473,\n",
              " '14': 474,\n",
              " 'top': 475,\n",
              " 'fax': 476,\n",
              " 'v': 477,\n",
              " '7': 478,\n",
              " 'science': 479,\n",
              " 'kaminski': 480,\n",
              " 'better': 481,\n",
              " 'legal': 482,\n",
              " 'evidence': 483,\n",
              " 'interest': 484,\n",
              " 'german': 485,\n",
              " 'sent': 486,\n",
              " 'even': 487,\n",
              " 'march': 488,\n",
              " 'reply': 489,\n",
              " 'call': 490,\n",
              " 'theme': 491,\n",
              " 'thanks': 492,\n",
              " 'u': 493,\n",
              " 'party': 494,\n",
              " 'investment': 495,\n",
              " 'jones': 496,\n",
              " '02': 497,\n",
              " 'server': 498,\n",
              " 'network': 499,\n",
              " 'study': 500,\n",
              " 'uk': 501,\n",
              " 'razor': 502,\n",
              " 'edu': 503,\n",
              " 'yet': 504,\n",
              " 'c': 505,\n",
              " 'start': 506,\n",
              " 'without': 507,\n",
              " 'dear': 508,\n",
              " 'system': 509,\n",
              " '6': 510,\n",
              " 'anyone': 511,\n",
              " 'term': 512,\n",
              " 'change': 513,\n",
              " 'tuesday': 514,\n",
              " 'good': 515,\n",
              " 'low': 516,\n",
              " 'representation': 517,\n",
              " 'part': 518,\n",
              " 'morphology': 519,\n",
              " 'idea': 520,\n",
              " '1998': 521,\n",
              " 'word': 522,\n",
              " 'houston': 523,\n",
              " '09': 524,\n",
              " 'data': 525,\n",
              " 'press': 526,\n",
              " 'category': 527,\n",
              " 'e': 528,\n",
              " 'add': 529,\n",
              " '500': 530,\n",
              " 'let': 531,\n",
              " 'world': 532,\n",
              " 'program': 533,\n",
              " 'currently': 534,\n",
              " 'mark': 535,\n",
              " 'center': 536,\n",
              " 'project': 537,\n",
              " '04': 538,\n",
              " 'paid': 539,\n",
              " 'cognitive': 540,\n",
              " 'ha': 541,\n",
              " 'com': 542,\n",
              " 'actually': 543,\n",
              " '08': 544,\n",
              " '1997': 545,\n",
              " 'free': 546,\n",
              " 'spam': 547,\n",
              " 'common': 548,\n",
              " 'r': 549,\n",
              " 'executive': 550,\n",
              " 'html': 551,\n",
              " 'role': 552,\n",
              " 'used': 553,\n",
              " 'wish': 554,\n",
              " 'read': 555,\n",
              " 'wa': 556,\n",
              " 'request': 557,\n",
              " 'job': 558,\n",
              " 'live': 559,\n",
              " 'removed': 560,\n",
              " 'always': 561,\n",
              " 'technical': 562,\n",
              " 'europe': 563,\n",
              " 'anything': 564,\n",
              " 'first': 565,\n",
              " 'address': 566,\n",
              " 'material': 567,\n",
              " 'using': 568,\n",
              " 'talk': 569,\n",
              " 'instruction': 570,\n",
              " 'le': 571,\n",
              " '20': 572,\n",
              " 'billion': 573,\n",
              " 'telephone': 574,\n",
              " 'foreign': 575,\n",
              " 'http': 576,\n",
              " 'big': 577,\n",
              " 'order': 578,\n",
              " 'detail': 579,\n",
              " 'break': 580,\n",
              " 'cost': 581,\n",
              " 'complete': 582,\n",
              " 'please': 583,\n",
              " 'ie': 584,\n",
              " 'feel': 585,\n",
              " 'mail': 586,\n",
              " 'october': 587,\n",
              " 'syntax': 588,\n",
              " 'yes': 589,\n",
              " 'la': 590,\n",
              " 'example': 591,\n",
              " 'evaluation': 592,\n",
              " 'come': 593,\n",
              " 'link': 594,\n",
              " 'might': 595,\n",
              " 'j': 596,\n",
              " 'final': 597,\n",
              " 'invited': 598,\n",
              " 'summer': 599,\n",
              " 'back': 600,\n",
              " 'cd': 601,\n",
              " 'level': 602,\n",
              " 'movement': 603,\n",
              " 'someone': 604,\n",
              " 'title': 605,\n",
              " 'net': 606,\n",
              " '05': 607,\n",
              " 'five': 608,\n",
              " 'public': 609,\n",
              " 'support': 610,\n",
              " 'mailman': 611,\n",
              " 'major': 612,\n",
              " 'put': 613,\n",
              " 'wednesday': 614,\n",
              " 'hour': 615,\n",
              " 'english': 616,\n",
              " 'key': 617,\n",
              " '27': 618,\n",
              " 'present': 619,\n",
              " 'personal': 620,\n",
              " 'note': 621,\n",
              " 'education': 622,\n",
              " 'accepted': 623,\n",
              " 'continue': 624,\n",
              " '50': 625,\n",
              " 'making': 626,\n",
              " 'way': 627,\n",
              " 'language': 628,\n",
              " 'found': 629,\n",
              " 'standard': 630,\n",
              " 'l': 631,\n",
              " 'past': 632,\n",
              " '97': 633,\n",
              " 'income': 634,\n",
              " 'community': 635,\n",
              " '1999': 636,\n",
              " 'format': 637,\n",
              " 'use': 638,\n",
              " 'single': 639,\n",
              " 'wrote': 640,\n",
              " 'vince': 641,\n",
              " 'code': 642,\n",
              " 'presentation': 643,\n",
              " 'seen': 644,\n",
              " 'application': 645,\n",
              " 'opportunity': 646,\n",
              " 'n': 647,\n",
              " 'file': 648,\n",
              " 'learning': 649,\n",
              " 'welcome': 650,\n",
              " 'interested': 651,\n",
              " 'mr': 652,\n",
              " 'f': 653,\n",
              " 'semantics': 654,\n",
              " '15': 655,\n",
              " 'corpus': 656,\n",
              " 'mean': 657,\n",
              " 'thousand': 658,\n",
              " 'acquisition': 659,\n",
              " 'logic': 660,\n",
              " 'south': 661,\n",
              " 'large': 662,\n",
              " 'analysis': 663,\n",
              " '31': 664,\n",
              " 'historical': 665,\n",
              " 'early': 666,\n",
              " 'related': 667,\n",
              " 'spoken': 668,\n",
              " 'professional': 669,\n",
              " 'analyst': 670,\n",
              " 'open': 671,\n",
              " 'potential': 672,\n",
              " 'lexical': 673,\n",
              " 'help': 674,\n",
              " 'deal': 675,\n",
              " 'japanese': 676,\n",
              " '30': 677,\n",
              " 'reason': 678,\n",
              " 'real': 679,\n",
              " 'run': 680,\n",
              " 'small': 681,\n",
              " 'already': 682,\n",
              " 'ect': 683,\n",
              " 'rule': 684,\n",
              " 'error': 685,\n",
              " 'th': 686,\n",
              " 'especially': 687,\n",
              " '90': 688,\n",
              " 'contact': 689,\n",
              " 'size': 690,\n",
              " 'product': 691,\n",
              " 'natural': 692,\n",
              " 'sign': 693,\n",
              " 'message': 694,\n",
              " 'held': 695,\n",
              " 'able': 696,\n",
              " 'nl': 697,\n",
              " 'w': 698,\n",
              " 'current': 699,\n",
              " 'far': 700,\n",
              " 'technology': 701,\n",
              " 'else': 702,\n",
              " 'domain': 703,\n",
              " '100': 704,\n",
              " 'canada': 705,\n",
              " 'person': 706,\n",
              " 'subject': 707,\n",
              " 'hou': 708,\n",
              " 'communication': 709,\n",
              " '45': 710,\n",
              " 'institute': 711,\n",
              " 'b': 712,\n",
              " 'main': 713,\n",
              " 'p': 714,\n",
              " 'asked': 715,\n",
              " 'option': 716,\n",
              " 'view': 717,\n",
              " 'position': 718,\n",
              " 'research': 719,\n",
              " 'charge': 720,\n",
              " 'meeting': 721,\n",
              " 'class': 722,\n",
              " 'hundred': 723,\n",
              " 'researcher': 724,\n",
              " 'november': 725,\n",
              " '17': 726,\n",
              " 'different': 727,\n",
              " 'million': 728,\n",
              " 'vowel': 729,\n",
              " '8': 730,\n",
              " 'president': 731,\n",
              " 'japan': 732,\n",
              " 'like': 733,\n",
              " 'city': 734,\n",
              " 'wide': 735,\n",
              " 'write': 736,\n",
              " 'site': 737,\n",
              " 'international': 738,\n",
              " '03': 739,\n",
              " 'america': 740,\n",
              " 'profit': 741,\n",
              " '12': 742,\n",
              " 'en': 743,\n",
              " 'visit': 744,\n",
              " 'enron': 745,\n",
              " '11': 746,\n",
              " 'place': 747,\n",
              " 'success': 748,\n",
              " 'thought': 749,\n",
              " 'linguistics': 750,\n",
              " 'work': 751,\n",
              " 'risk': 752,\n",
              " 'price': 753,\n",
              " 'committee': 754,\n",
              " 'share': 755,\n",
              " 'industry': 756,\n",
              " 'ad': 757,\n",
              " 'list': 758,\n",
              " 'really': 759,\n",
              " '44': 760,\n",
              " 'grammar': 761,\n",
              " 'april': 762,\n",
              " 'next': 763,\n",
              " 'computational': 764,\n",
              " 'problem': 765,\n",
              " 'money': 766,\n",
              " 'translation': 767,\n",
              " 'member': 768,\n",
              " 'lunch': 769,\n",
              " 'purchase': 770,\n",
              " 'theory': 771,\n",
              " 'something': 772,\n",
              " '24': 773,\n",
              " 'within': 774,\n",
              " 'made': 775,\n",
              " 'model': 776,\n",
              " 'dynegy': 777,\n",
              " '_': 778,\n",
              " 'field': 779,\n",
              " 'thank': 780,\n",
              " 'record': 781,\n",
              " 'various': 782,\n",
              " 'summary': 783,\n",
              " 'chair': 784,\n",
              " 'rather': 785,\n",
              " 'topic': 786,\n",
              " 'sell': 787,\n",
              " 'global': 788,\n",
              " 'programme': 789,\n",
              " 'sure': 790,\n",
              " 'feature': 791,\n",
              " 'important': 792,\n",
              " 'event': 793,\n",
              " 'friend': 794,\n",
              " 'june': 795,\n",
              " 'page': 796,\n",
              " 'exchange': 797,\n",
              " 'financial': 798,\n",
              " 'commercial': 799,\n",
              " 'paper': 800,\n",
              " 'around': 801,\n",
              " 'send': 802,\n",
              " 'law': 803,\n",
              " 'hope': 804,\n",
              " 'ca': 805,\n",
              " 'lot': 806,\n",
              " 'offer': 807,\n",
              " 'stock': 808,\n",
              " 'make': 809,\n",
              " 'given': 810,\n",
              " 'trade': 811,\n",
              " 'co': 812}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
        "token_to_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE6xAzIN7DY5",
        "outputId": "d8ec86ba-ea77-4e9b-b0c0-53781febc50f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['3d', 'b', 'br', 'com', 'bad', 'font', 'font', 'com', 'randoms']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message_to_token_list('3d b <br> .com bad font font com randoms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OuXO-Cjf4vNo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"Bag of Words\" (counts vector)\n",
        "\n",
        "# ->  http  tr  size  3d  font  br  com  td   p   b\n",
        "# ->    0    1    2    3   4    5    6    7   8   9\n",
        "# ->   [0,   0,   0,   1,  2,   1,   2,   0,  0,  1]\n",
        "\n",
        "[0.,  0.,  0.,   1., 2.,  1., 2.,  0., 0., 1.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ3kBFPW7vpm",
        "outputId": "3280787f-4a7f-4ed2-8836-5d3152393809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def message_to_count_vector(message):\n",
        "  count_vector = np.zeros(len(features))\n",
        "\n",
        "  processed_list_of_tokens = message_to_token_list(message)\n",
        "\n",
        "  for token in processed_list_of_tokens:\n",
        "    if token not in features:\n",
        "      continue\n",
        "    index = token_to_index_mapping[token]\n",
        "    count_vector[index] += 1\n",
        "\n",
        "  return count_vector\n",
        "\n",
        "message_to_count_vector('3d b <br> .com bad font font com randoms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvnPzPqN_kWU",
        "outputId": "2af5915b-12dd-4b9f-b40f-0eacbc4ef2ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message_to_count_vector(train_df['MESSAGE'].iloc[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We7My4so_y4P",
        "outputId": "ecdfd9ac-38a6-40be-c22e-baf528a44839"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SERIAL                                                  13497\n",
              "MESSAGE     btu ' s daily power report - eastern edition a...\n",
              "CATEGORY                                                    0\n",
              "Name: 3, dtype: object"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.iloc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w3V5tgVbB_9K"
      },
      "outputs": [],
      "source": [
        "def df_to_X_y(dff):\n",
        "  y = dff['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "  message_col = dff['MESSAGE']\n",
        "  count_vectors = []\n",
        "\n",
        "  for message in message_col:\n",
        "    count_vector = message_to_count_vector(message)\n",
        "    count_vectors.append(count_vector)\n",
        "\n",
        "  X = np.array(count_vectors).astype(int)\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1AB_Ehd_18I",
        "outputId": "ac1da35b-3721-441f-a386-efd12e084422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14907, 813), (14907,), (3727, 813), (3727,))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, y_train = df_to_X_y(train_df)\n",
        "\n",
        "X_test, y_test = df_to_X_y(test_df)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "243hURJZC7pt",
        "outputId": "843808cf-2f87-43bc-8d18-f41d8f6abc6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.00110132, 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler().fit(X_train)\n",
        "\n",
        "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# **TF-IDF Feature Extraction**\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def custom_tokenizer(message):\n",
        "    return message_to_token_list(message)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['MESSAGE'])\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['MESSAGE'])\n",
        "\n",
        "y_train_tfidf = train_df['CATEGORY'].to_numpy().astype(int)\n",
        "y_test_tfidf = test_df['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "\n",
        "# View the dense matrix (first 5 rows, for example)\n",
        "print(X_train_tfidf_dense[:1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Vector Feature Matrix Shapes:\n",
            "Training Set: (14907, 100)\n",
            "Test Set: (3727, 100)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Prepare tokenized sentences from the training data\n",
        "train_sentences = [message_to_token_list(msg) for msg in train_df['MESSAGE'] if isinstance(msg, str)]\n",
        "test_sentences = [message_to_token_list(msg) for msg in test_df['MESSAGE'] if isinstance(msg, str)]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4, seed=1)\n",
        "\n",
        "# Function to convert a message to a Word2Vec vector (averaged over all tokens)\n",
        "def message_to_wv_vector(message, model, vector_size):\n",
        "    tokens = message_to_token_list(message)\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    if len(vectors) == 0:  # If no tokens are found in the model, return a zero vector\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Extract Word2Vec features for training and test sets\n",
        "vector_size = word2vec_model.vector_size\n",
        "wv_X_train = np.array([message_to_wv_vector(msg, word2vec_model, vector_size) for msg in train_df['MESSAGE']])\n",
        "wv_X_test = np.array([message_to_wv_vector(msg, word2vec_model, vector_size) for msg in test_df['MESSAGE']])\n",
        "\n",
        "# Labels remain the same\n",
        "wv_y_train = train_df['CATEGORY'].to_numpy().astype(int)\n",
        "wv_y_test = test_df['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "# Verify the shape of the feature matrices\n",
        "print(\"Word Vector Feature Matrix Shapes:\")\n",
        "print(\"Training Set:\", wv_X_train.shape)\n",
        "print(\"Test Set:\", wv_X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid Train Feature Shape: (14907, 140962)\n",
            "Hybrid Test Feature Shape: (3727, 140962)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Combine TF-IDF and Word Vectors into a hybrid feature\n",
        "hybrid_X_train = hstack([X_train_tfidf, wv_X_train])\n",
        "hybrid_X_test = hstack([X_test_tfidf, wv_X_test])\n",
        "\n",
        "# Ensure the data is in dense format if needed for certain classifiers\n",
        "hybrid_X_train = hybrid_X_train.toarray()\n",
        "hybrid_X_test = hybrid_X_test.toarray()\n",
        "\n",
        "print(f\"Hybrid Train Feature Shape: {hybrid_X_train.shape}\")\n",
        "print(f\"Hybrid Test Feature Shape: {hybrid_X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptZ2wUTxD1MF",
        "outputId": "b960271b-8777-4950-f2ec-0e6109446812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.99      0.77      2290\n",
            "           1       0.86      0.05      0.10      1437\n",
            "\n",
            "    accuracy                           0.63      3727\n",
            "   macro avg       0.74      0.52      0.43      3727\n",
            "weighted avg       0.72      0.63      0.51      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lr = LogisticRegression().fit(X_train, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZK0jMUSEOCi",
        "outputId": "578ad022-7481-43d7-a74e-a38a3afc89cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96      2290\n",
            "           1       0.93      0.95      0.94      1437\n",
            "\n",
            "    accuracy                           0.95      3727\n",
            "   macro avg       0.95      0.95      0.95      3727\n",
            "weighted avg       0.95      0.95      0.95      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare logistic regression to random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier().fit(X_train, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with Bag of Words Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      2290\n",
            "           1       0.92      0.97      0.94      1437\n",
            "\n",
            "    accuracy                           0.95      3727\n",
            "   macro avg       0.95      0.96      0.95      3727\n",
            "weighted avg       0.96      0.95      0.95      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_bow = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "xgb_bow.fit(X_train, y_train)\n",
        "\n",
        "xgb_predictions_bow = xgb_bow.predict(X_test)\n",
        "\n",
        "print(\"XGBoost with Bag of Words Classification Report:\\n\")\n",
        "print(classification_report(y_test, xgb_predictions_bow))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TF_IDF EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3vhIALXMEkb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n",
            "Logistic Regression Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      2290\n",
            "           1       0.94      0.98      0.96      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.96      0.97      0.97      3727\n",
            "weighted avg       0.97      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# **Logistic Regression Training and Evaluation**\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Training Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Predictions and evaluation\n",
        "lr_predictions = lr_model.predict(X_test_tfidf)\n",
        "print(\"Logistic Regression Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, lr_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest...\n",
            "Random Forest Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97      2290\n",
            "           1       0.95      0.96      0.96      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.96      0.97      0.97      3727\n",
            "weighted avg       0.97      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# **Random Forest Training and Evaluation**\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Training Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Predictions and evaluation\n",
        "rf_predictions = rf_model.predict(X_test_tfidf)\n",
        "print(\"Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, rf_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:19:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97      2290\n",
            "           1       0.93      0.98      0.96      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.96      0.97      0.96      3727\n",
            "weighted avg       0.97      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the XGBoost model\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "xgb_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "xgb_predictions = xgb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"XGBoost Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, xgb_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      2290\n",
            "           1       0.92      0.95      0.94      1437\n",
            "\n",
            "    accuracy                           0.95      3727\n",
            "   macro avg       0.94      0.95      0.95      3727\n",
            "weighted avg       0.95      0.95      0.95      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "wv_lr_model = LogisticRegression().fit(wv_X_train, wv_y_train)\n",
        "wv_predictions = wv_lr_model.predict(wv_X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "print(\"Logistic Regression with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, wv_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest on Word Vectors...\n",
            "Random Forest with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97      2290\n",
            "           1       0.94      0.95      0.95      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.95      0.96      0.96      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Random Forest Classifier on Word Vectors\n",
        "print(\"Training Random Forest on Word Vectors...\")\n",
        "rf_wv_model = RandomForestClassifier(random_state=1)\n",
        "rf_wv_model.fit(wv_X_train, wv_y_train)\n",
        "\n",
        "rf_wv_predictions = rf_wv_model.predict(wv_X_test)\n",
        "\n",
        "print(\"Random Forest with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, rf_wv_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost on Word Vectors...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:20:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      2290\n",
            "           1       0.94      0.97      0.95      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.96      0.96      0.96      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# XGBoost Classifier on Word Vectors\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Training XGBoost on Word Vectors...\")\n",
        "xgb_wv_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=1)\n",
        "xgb_wv_model.fit(wv_X_train, wv_y_train)\n",
        "\n",
        "xgb_wv_predictions = xgb_wv_model.predict(wv_X_test)\n",
        "\n",
        "print(\"XGBoost with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, xgb_wv_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classification Report (Hybrid Features):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      2290\n",
            "           1       0.94      0.97      0.95      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.96      0.96      0.96      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Logistic Regression on Hybrid Features\n",
        "lr_hybrid_model = LogisticRegression()\n",
        "lr_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "lr_hybrid_predictions = lr_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"Logistic Regression Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, lr_hybrid_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classification Report (Hybrid Features):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97      2290\n",
            "           1       0.95      0.96      0.96      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.97      0.97      0.97      3727\n",
            "weighted avg       0.97      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest on Hybrid Features\n",
        "rf_hybrid_model = RandomForestClassifier()\n",
        "rf_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_hybrid_predictions = rf_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"Random Forest Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, rf_hybrid_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:03:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Classification Report (Hybrid Features):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      2290\n",
            "           1       0.95      0.98      0.97      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.97      0.98      0.97      3727\n",
            "weighted avg       0.98      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost on Hybrid Features\n",
        "xgb_hybrid_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "xgb_hybrid_predictions = xgb_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"XGBoost Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, xgb_hybrid_predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

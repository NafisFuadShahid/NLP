{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "a-YrFyW2cn_n",
        "outputId": "975995fc-b85b-4cb7-ea3c-364ae1e4508e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SERIAL</th>\n",
              "      <th>MESSAGE</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the other side of * galicismos * * galicismo *...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>re : equistar deal tickets are you still avail...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SERIAL                                            MESSAGE  CATEGORY\n",
              "0       0  re : 6 . 1100 , disc : uniformitarianism , re ...         0\n",
              "1       1  the other side of * galicismos * * galicismo *...         0\n",
              "2       2  re : equistar deal tickets are you still avail...         0\n",
              "3       3  \\nHello I am your hot lil horny toy.\\n    I am...         1\n",
              "4       4  software at incredibly low prices ( 86 % lower...         1"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset (https://www.kaggle.com/chandramoulinaidu/spam-classification-for-basic-nlp)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Phishing_Email.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5Brt9s9rkKYb",
        "outputId": "1b8fb1a7-af4c-4893-fead-eb71c0c887d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SERIAL</th>\n",
              "      <th>MESSAGE</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18629</th>\n",
              "      <td>18646</td>\n",
              "      <td>date a lonely housewife always wanted to date ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18630</th>\n",
              "      <td>18647</td>\n",
              "      <td>request submitted : access request for anita ....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18631</th>\n",
              "      <td>18648</td>\n",
              "      <td>re : important - prc mtg hi dorn &amp; john , as y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18632</th>\n",
              "      <td>18649</td>\n",
              "      <td>press clippings - letter on californian utilit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18633</th>\n",
              "      <td>18650</td>\n",
              "      <td>empty</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       SERIAL                                            MESSAGE  CATEGORY\n",
              "18629   18646  date a lonely housewife always wanted to date ...         1\n",
              "18630   18647  request submitted : access request for anita ....         0\n",
              "18631   18648  re : important - prc mtg hi dorn & john , as y...         0\n",
              "18632   18649  press clippings - letter on californian utilit...         0\n",
              "18633   18650                                              empty         1"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BZJ1xgHkOOy",
        "outputId": "19b2cb60-3334-41d5-969c-6fb0fea4846f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CATEGORY\n",
              "0    11322\n",
              "1     7312\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['CATEGORY'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUoCyp8mkXsh",
        "outputId": "7bcfa229-8c68-46d0-f672-325c73c0b020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\fuadn\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\fuadn\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWa64YhklKeT",
        "outputId": "9c407065-d1dd-46d7-d3e2-a0af97be6cfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'GGggGG',\n",
              " 'feet',\n",
              " 'it',\n",
              " 'going',\n",
              " 'HTML',\n",
              " 'bads',\n",
              " 'bads',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "test_message = \"Hey,, GGggGG feet it going? <HTML><bads> bads 'randoms' badly\"\n",
        "\n",
        "test_message_tokenized = tokenizer.tokenize(test_message)\n",
        "test_message_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts46HslRmwlB",
        "outputId": "d48edd5e-a828-4374-bf86-7c9f4157da35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey',\n",
              " 'gggggg',\n",
              " 'feet',\n",
              " 'it',\n",
              " 'going',\n",
              " 'html',\n",
              " 'bads',\n",
              " 'bads',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_message_lowercased = [t.lower() for t in test_message_tokenized]\n",
        "test_message_lowercased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfXmuvUn3dy",
        "outputId": "23a3a78a-c9d9-4c6d-d0ff-fa122304f23d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey',\n",
              " 'gggggg',\n",
              " 'foot',\n",
              " 'it',\n",
              " 'going',\n",
              " 'html',\n",
              " 'bad',\n",
              " 'bad',\n",
              " 'randoms',\n",
              " 'badly']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "test_message_lemmatized_tokens = [lemmatizer.lemmatize(t) for t in test_message_lowercased]\n",
        "test_message_lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG06k4ags0al",
        "outputId": "1fab4508-e4ad-4be7-b32d-cbb2983c02df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey', 'gggggg', 'foot', 'going', 'html', 'bad', 'bad', 'randoms', 'badly']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "test_message_useful_tokens = [t for t in test_message_lemmatized_tokens if t not in stopwords]\n",
        "test_message_useful_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtNehJ51t6Ii",
        "outputId": "8dac74a6-f061-4fc9-a0ad-22bdf637af02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hey', 'gggggg', 'foot', 'going', 'html', 'bad', 'bad', 'randoms', 'badly']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def message_to_token_list(s):\n",
        "  tokens = tokenizer.tokenize(s)\n",
        "  lowercased_tokens = [t.lower() for t in tokens]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]\n",
        "  useful_tokens = [t for t in lemmatized_tokens if t not in stopwords]\n",
        "\n",
        "  return useful_tokens\n",
        "\n",
        "message_to_token_list(test_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_InW_NurvEqq",
        "outputId": "5510f999-7e20-4d6d-8b2b-02970916b041"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       SERIAL                                            MESSAGE  CATEGORY\n",
              " 0       13454  fw : priority customer list this is not going ...         0\n",
              " 1       14141  On Tue, 30 Jul 2002 22:22:24 +0200, \"Manfred G...         0\n",
              " 2       17106  re : [ penetrative ] 86 % - off vicodin . dono...         1\n",
              " 3       13497  btu ' s daily power report - eastern edition a...         0\n",
              " 4       17348  On Tue, 27 Aug 2002, David Neary wrote:> > Act...         0\n",
              " ...       ...                                                ...       ...\n",
              " 14902    8884  -----Original Message----- > From: Aherne Pete...         0\n",
              " 14903    7018  re : credit trading brought to you by bryan se...         0\n",
              " 14904    9022  review of perez - leroux & glass ( ed ) contem...         0\n",
              " 14905    6925  enron actuals for july 7 thru 9 , 2000 july 7 ...         0\n",
              " 14906   13046  re : thursday visit frank , we shall have abou...         0\n",
              " \n",
              " [14907 rows x 3 columns],\n",
              "       SERIAL                                            MESSAGE  CATEGORY\n",
              " 0       3220  Note: \\nThis is NOT SPAM!! This is NOT Unsolic...         1\n",
              " 1      15469  haul special offer ! you don ' t need to make ...         1\n",
              " 2      13386  has a funky cut/paste model that is essentiall...         0\n",
              " 3       8239  On 0020 -0700 %{!Thu, Sep 05, 2002 at  3:17:36...         0\n",
              " 4       6388  v\\nAmerica's #1 Government Grant Program!\\nThe...         1\n",
              " ...      ...                                                ...       ...\n",
              " 3722   10969  URL: http://www.newsisfree.com/click/-3,868897...         0\n",
              " 3723   17306  Would You Like to Save up to 80\\n Would You Li...         1\n",
              " 3724    5200  esslli workshop : lexical semantics in context...         0\n",
              " 3725   12188  Dear Candidate,We recently came across a posti...         1\n",
              " 3726     236  psycholinguistics & neurolinguistics linguisti...         0\n",
              " \n",
              " [3727 rows x 3 columns])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.sample(frac=1, random_state=1)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "split_index = int(len(df) * 0.8)\n",
        "train_df, test_df = df[:split_index], df[split_index:]\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwUuMr_0waAu",
        "outputId": "14ab0468-fdff-455f-e6a0-aebbcb7102a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "140862"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "token_counter = {}\n",
        "\n",
        "# Iterate over each message in the 'MESSAGE' column\n",
        "for message in train_df['MESSAGE']:\n",
        "    if isinstance(message, str):  # Check if message is a string\n",
        "        message_as_token_lst = message_to_token_list(message)  # Convert message to a list of tokens\n",
        "\n",
        "        # Count occurrences of each token\n",
        "        for token in message_as_token_lst:\n",
        "            if token in token_counter:\n",
        "                token_counter[token] += 1\n",
        "            else:\n",
        "                token_counter[token] = 1\n",
        "\n",
        "len(token_counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubi91FlbyfXJ",
        "outputId": "1ed3962a-61fd-4b1f-fd80-b3a7322438b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'fw': 582,\n",
              " 'priority': 314,\n",
              " 'customer': 2086,\n",
              " 'list': 11436,\n",
              " 'going': 2023,\n",
              " 'work': 7854,\n",
              " 'want': 4565,\n",
              " 'control': 1358,\n",
              " 'like': 7945,\n",
              " 'used': 3240,\n",
              " 'enron': 16368,\n",
              " 'please': 11825,\n",
              " 'let': 3215,\n",
              " 'know': 6078,\n",
              " 'bill': 1491,\n",
              " 'original': 2557,\n",
              " 'message': 6095,\n",
              " 'forster': 66,\n",
              " 'david': 1896,\n",
              " 'sent': 4127,\n",
              " 'friday': 1589,\n",
              " 'february': 1084,\n",
              " '01': 3184,\n",
              " '2002': 3521,\n",
              " '2': 15186,\n",
              " '53': 299,\n",
              " 'pm': 2841,\n",
              " 'bradford': 88,\n",
              " 'william': 556,\n",
              " 'brackett': 21,\n",
              " 'debbie': 70,\n",
              " 'r': 4269,\n",
              " 'pat': 206,\n",
              " 'odonnell': 8,\n",
              " 'ubsw': 35,\n",
              " 'com': 15812,\n",
              " 'louis': 175,\n",
              " 'eber': 11,\n",
              " 'glass': 91,\n",
              " 'colette': 31,\n",
              " 'dow': 1108,\n",
              " 'cc': 2819,\n",
              " 'kitchen': 555,\n",
              " 'louise': 934,\n",
              " 'subject': 11443,\n",
              " 'attached': 1198,\n",
              " 'intended': 853,\n",
              " 'pre': 1038,\n",
              " 'approved': 581,\n",
              " 'company': 10046,\n",
              " 'ready': 1219,\n",
              " 'launch': 211,\n",
              " 'day': 7004,\n",
              " 'includes': 1150,\n",
              " 'u': 13125,\n",
              " 'canadian': 376,\n",
              " 'entity': 411,\n",
              " 'gas': 2959,\n",
              " 'power': 3024,\n",
              " 'recently': 892,\n",
              " 'told': 849,\n",
              " 'individual': 1690,\n",
              " 'legal': 1389,\n",
              " 'name': 7450,\n",
              " 'wanted': 733,\n",
              " 'group': 4527,\n",
              " 'needed': 921,\n",
              " 'provide': 2587,\n",
              " 'way': 5064,\n",
              " 'well': 4680,\n",
              " 'note': 2832,\n",
              " '25': 3026,\n",
              " 'previously': 475,\n",
              " 'supplied': 171,\n",
              " 'credit': 3506,\n",
              " 'appears': 522,\n",
              " 'number': 6131,\n",
              " 'non': 2426,\n",
              " 'online': 2301,\n",
              " 'included': 1117,\n",
              " 'recent': 1494,\n",
              " 'provides': 760,\n",
              " 'order': 8380,\n",
              " 'sending': 934,\n",
              " 'another': 2221,\n",
              " 'later': 1375,\n",
              " 'identify': 393,\n",
              " 'type': 2843,\n",
              " 'master': 587,\n",
              " 'agreement': 1431,\n",
              " 'isda': 8,\n",
              " 'physical': 402,\n",
              " 'dave': 470,\n",
              " 'tue': 353,\n",
              " '30': 8256,\n",
              " 'jul': 408,\n",
              " '22': 1573,\n",
              " '24': 2217,\n",
              " '0200': 132,\n",
              " 'manfred': 126,\n",
              " 'grobosch': 1,\n",
              " 'wrote': 1658,\n",
              " 'would': 11127,\n",
              " 'install': 430,\n",
              " 'rpm': 1216,\n",
              " 'tried': 538,\n",
              " 'get': 9165,\n",
              " 'information': 13130,\n",
              " 'visiting': 189,\n",
              " 'www': 10334,\n",
              " 'org': 2324,\n",
              " 'related': 2132,\n",
              " 'link': 3009,\n",
              " 'give': 2772,\n",
              " 'seems': 1297,\n",
              " 'assume': 554,\n",
              " 'already': 1605,\n",
              " 'installed': 302,\n",
              " 'firewall': 107,\n",
              " 'based': 4112,\n",
              " 'linux': 3289,\n",
              " '20': 9607,\n",
              " 'smoothwall': 6,\n",
              " 'private': 927,\n",
              " 'use': 6940,\n",
              " 'package': 1570,\n",
              " 'program': 7417,\n",
              " 'scratch': 66,\n",
              " 'found': 1944,\n",
              " 'site': 4839,\n",
              " 'hopefully': 137,\n",
              " 'knowledge': 1869,\n",
              " 'window': 1783,\n",
              " 'sco': 8,\n",
              " 'aix': 72,\n",
              " 'box': 1911,\n",
              " 'generally': 465,\n",
              " 'anything': 1291,\n",
              " 'redhat': 538,\n",
              " 'send': 5995,\n",
              " 'reason': 1415,\n",
              " 'maybe': 724,\n",
              " 'missed': 165,\n",
              " 'something': 1681,\n",
              " 'running': 712,\n",
              " 'anyway': 378,\n",
              " 'brian': 493,\n",
              " 'fahrlã': 24,\n",
              " 'nder': 24,\n",
              " 'zealot': 27,\n",
              " 'conservative': 185,\n",
              " 'technomad': 24,\n",
              " 'evansville': 37,\n",
              " 'voyage': 42,\n",
              " 'http': 15898,\n",
              " 'countermoon': 24,\n",
              " 'icq': 54,\n",
              " '5119262': 24,\n",
              " 'hear': 701,\n",
              " 'news': 2820,\n",
              " 'isreal': 14,\n",
              " 'contains': 904,\n",
              " 'word': 5728,\n",
              " 'bullet': 67,\n",
              " 'brain': 256,\n",
              " 'arafat': 24,\n",
              " '_______________________________________________': 836,\n",
              " 'mailing': 3538,\n",
              " 'freshrpms': 531,\n",
              " 'net': 5720,\n",
              " 'mailman': 1626,\n",
              " 'listinfo': 2079,\n",
              " 'penetrative': 2,\n",
              " '86': 166,\n",
              " 'vicodin': 62,\n",
              " 'donovan': 21,\n",
              " 'harvest': 51,\n",
              " 'pisa': 82,\n",
              " 'toiler': 1,\n",
              " 'staten': 3,\n",
              " 'worrier': 3,\n",
              " 'server': 1662,\n",
              " 'dreadfully': 1,\n",
              " 'pilate': 2,\n",
              " 'unix': 320,\n",
              " 'musical': 131,\n",
              " 'visigoth': 2,\n",
              " 'eastern': 377,\n",
              " 'adjoins': 7,\n",
              " 'unqualified': 12,\n",
              " 'abhors': 1,\n",
              " 'viper': 5,\n",
              " 'buddy': 46,\n",
              " 'grin': 16,\n",
              " 'outcast': 4,\n",
              " 'transistorizing': 1,\n",
              " 'avalanche': 25,\n",
              " 'debt': 1179,\n",
              " 'supermarket': 12,\n",
              " 'fanning': 1,\n",
              " 'mitigates': 8,\n",
              " 'arson': 2,\n",
              " 'dismisser': 1,\n",
              " 'weird': 90,\n",
              " 'swarm': 7,\n",
              " 'sane': 15,\n",
              " 'codify': 11,\n",
              " 'trim': 27,\n",
              " 'bug': 337,\n",
              " 'levity': 2,\n",
              " 'harlot': 1,\n",
              " 'phone': 3581,\n",
              " '568': 10,\n",
              " '857': 11,\n",
              " '4997': 1,\n",
              " 'mobile': 491,\n",
              " '524': 27,\n",
              " '858': 26,\n",
              " '6793': 1,\n",
              " 'email': 10420,\n",
              " 'alycewiley': 1,\n",
              " '2001': 4460,\n",
              " 'madhuri': 3,\n",
              " 'btu': 40,\n",
              " 'daily': 950,\n",
              " 'report': 7882,\n",
              " 'edition': 727,\n",
              " 'latest': 1118,\n",
              " 'issue': 4411,\n",
              " 'e': 17139,\n",
              " 'mail': 12888,\n",
              " 'info': 1727,\n",
              " '732': 52,\n",
              " '758': 26,\n",
              " '8222': 2,\n",
              " 'fax': 5681,\n",
              " '8286': 2,\n",
              " 'peo': 4,\n",
              " '71100': 1,\n",
              " 'pdf': 173,\n",
              " '27': 1810,\n",
              " 'aug': 514,\n",
              " 'neary': 23,\n",
              " 'actually': 1111,\n",
              " 'following': 4599,\n",
              " 'sensible': 41,\n",
              " 'echo': 133,\n",
              " 'enc': 3,\n",
              " 'sed': 55,\n",
              " '0': 12007,\n",
              " '9a': 20,\n",
              " 'fa': 109,\n",
              " 'f': 2183,\n",
              " 'x': 3306,\n",
              " '1': 20185,\n",
              " 'g': 3444,\n",
              " 'idea': 1703,\n",
              " 'wa': 13530,\n",
              " 'along': 948,\n",
              " 'line': 3834,\n",
              " 'attempting': 100,\n",
              " 'realised': 15,\n",
              " 'straight': 198,\n",
              " 'swap': 162,\n",
              " 'couldnt': 8,\n",
              " 'awk': 22,\n",
              " 'gensub': 2,\n",
              " 'insert': 162,\n",
              " 'end': 2332,\n",
              " 'internet': 4077,\n",
              " 'adapted': 41,\n",
              " 'function': 986,\n",
              " 'decode_url': 1,\n",
              " 'str': 30,\n",
              " 'hextab': 21,\n",
              " 'c': 6794,\n",
              " 'c1': 12,\n",
              " 'c2': 13,\n",
              " 'len': 30,\n",
              " 'code': 1872,\n",
              " 'hex': 12,\n",
              " 'dec': 339,\n",
              " 'lookup': 59,\n",
              " 'table': 787,\n",
              " '8': 4681,\n",
              " '9': 5538,\n",
              " '10': 10658,\n",
              " '3': 12529,\n",
              " 'b': 3812,\n",
              " '11': 6075,\n",
              " '4': 8584,\n",
              " '12': 5375,\n",
              " '5': 9874,\n",
              " '13': 2366,\n",
              " '6': 5400,\n",
              " '14': 2647,\n",
              " '7': 5007,\n",
              " '15': 5883,\n",
              " 'decoded': 4,\n",
              " 'length': 990,\n",
              " 'substr': 3,\n",
              " 'check': 3229,\n",
              " 'usual': 266,\n",
              " 'start': 2567,\n",
              " 'uri': 17,\n",
              " 'encoding': 263,\n",
              " 'char': 97,\n",
              " 'valid': 375,\n",
              " 'toupper': 2,\n",
              " '16': 3036,\n",
              " 'sprintf': 1,\n",
              " 'space': 815,\n",
              " 'apparently': 345,\n",
              " 'else': 1087,\n",
              " 'return': 1338,\n",
              " 'cheer': 200,\n",
              " 'p': 5355,\n",
              " 'late': 1050,\n",
              " 'reply': 1747,\n",
              " 'footer': 38,\n",
              " 'received': 2973,\n",
              " 'error': 1460,\n",
              " 'yadda': 4,\n",
              " 'got': 1509,\n",
              " 'caught': 105,\n",
              " 'spam': 1484,\n",
              " 'filter': 239,\n",
              " 'ended': 152,\n",
              " 'junkmail': 4,\n",
              " 'directory': 1374,\n",
              " 'might': 2336,\n",
              " 'header': 425,\n",
              " 'regard': 1523,\n",
              " 'paul': 1046,\n",
              " 'jakma': 27,\n",
              " 'clubi': 10,\n",
              " 'ie': 2395,\n",
              " 'key': 1169,\n",
              " 'id': 1151,\n",
              " '64a2ff6a': 7,\n",
              " 'warning': 296,\n",
              " 'ever': 1612,\n",
              " 'dishone': 12,\n",
              " 'st': 1118,\n",
              " 'fortune': 381,\n",
              " 'one': 14319,\n",
              " 'nuclear': 81,\n",
              " 'bomb': 54,\n",
              " 'ruin': 31,\n",
              " 'whole': 1019,\n",
              " 'irish': 778,\n",
              " 'user': 3961,\n",
              " 'ilug': 1152,\n",
              " 'un': 1202,\n",
              " 'subscription': 1085,\n",
              " 'maintainer': 575,\n",
              " 'listmaster': 564,\n",
              " 'hello': 692,\n",
              " 'ibuyit': 40,\n",
              " 'stating': 74,\n",
              " 'approver': 45,\n",
              " 'invoice': 195,\n",
              " 'requested': 573,\n",
              " 'thanks': 3445,\n",
              " 'shirley': 424,\n",
              " 'forwarded': 1041,\n",
              " 'crenshaw': 250,\n",
              " 'hou': 4108,\n",
              " 'ect': 8484,\n",
              " '04': 1161,\n",
              " '17': 2489,\n",
              " '07': 946,\n",
              " '44': 1157,\n",
              " 'enronxgate': 297,\n",
              " '05': 1331,\n",
              " 'dl': 71,\n",
              " 'payable': 399,\n",
              " 'ou': 319,\n",
              " 'na': 505,\n",
              " 'cn': 123,\n",
              " 'recipient': 363,\n",
              " 'ibuyitpayables': 1,\n",
              " 'ex': 204,\n",
              " 'john': 3385,\n",
              " 'gill': 18,\n",
              " 'eu': 158,\n",
              " 'erin': 25,\n",
              " 'abdelnour': 1,\n",
              " 'shelley': 39,\n",
              " 'robbins': 9,\n",
              " 'sally': 453,\n",
              " 'mcadams': 1,\n",
              " 'joe': 335,\n",
              " 'cuccia': 4,\n",
              " 'judy': 76,\n",
              " 'knepshield': 2,\n",
              " '_': 216611,\n",
              " 'development': 2956,\n",
              " 'thank': 1886,\n",
              " 'identifying': 172,\n",
              " 'future': 2654,\n",
              " 'project': 2862,\n",
              " 'team': 1219,\n",
              " 'make': 7354,\n",
              " 'sure': 2036,\n",
              " 'receive': 3153,\n",
              " 'tool': 1415,\n",
              " 'support': 2229,\n",
              " 'need': 5925,\n",
              " 'successfully': 184,\n",
              " 'transition': 160,\n",
              " 'new': 11828,\n",
              " 'system': 7447,\n",
              " 'may': 8894,\n",
              " 'lst': 195,\n",
              " 'training': 713,\n",
              " 'houston': 1564,\n",
              " 'coder': 22,\n",
              " 'overview': 354,\n",
              " 'session': 3648,\n",
              " 'held': 1761,\n",
              " 'week': 4608,\n",
              " 'monday': 1364,\n",
              " 'thursday': 1173,\n",
              " 'doubletree': 11,\n",
              " 'hotel': 1199,\n",
              " 'nautile': 1,\n",
              " 'room': 1390,\n",
              " '00': 12601,\n",
              " 'hour': 2788,\n",
              " 'demonstration': 283,\n",
              " 'opportunity': 2369,\n",
              " 'ask': 1023,\n",
              " 'question': 4770,\n",
              " 'registration': 2760,\n",
              " 'necessary': 896,\n",
              " 'hand': 1273,\n",
              " 'classroom': 286,\n",
              " 'begin': 769,\n",
              " 'next': 2836,\n",
              " 'complete': 1593,\n",
              " 'real': 2017,\n",
              " 'life': 2212,\n",
              " 'exercise': 361,\n",
              " 'contact': 4506,\n",
              " 'isc': 13,\n",
              " 'registrar': 32,\n",
              " 'register': 856,\n",
              " 'field': 1977,\n",
              " 'worry': 233,\n",
              " 'forgotten': 74,\n",
              " 'material': 1612,\n",
              " 'available': 4748,\n",
              " 'beginning': 672,\n",
              " 'via': 1964,\n",
              " 'integrated': 284,\n",
              " 'solution': 944,\n",
              " 'center': 1762,\n",
              " 'document': 1269,\n",
              " 'library': 644,\n",
              " 'include': 3267,\n",
              " 'step': 1329,\n",
              " 'instruction': 1337,\n",
              " 'help': 3300,\n",
              " 'encourage': 368,\n",
              " 'people': 7457,\n",
              " 'approve': 85,\n",
              " 'address': 10123,\n",
              " 'whether': 1636,\n",
              " 'identified': 344,\n",
              " 'toc': 46,\n",
              " 'journal': 1569,\n",
              " 'african': 440,\n",
              " 'language': 22317,\n",
              " 'linguistics': 8372,\n",
              " 'jall': 3,\n",
              " '19': 2501,\n",
              " '1998': 5327,\n",
              " 'volume': 2303,\n",
              " 'mouton': 387,\n",
              " 'de': 9368,\n",
              " 'gruyter': 459,\n",
              " 'berlin': 639,\n",
              " 'york': 1954,\n",
              " 'marie': 214,\n",
              " 'k': 2044,\n",
              " 'huffman': 24,\n",
              " 'thomas': 498,\n",
              " 'j': 4234,\n",
              " 'hinnebusch': 3,\n",
              " 'phonetic': 454,\n",
              " 'nature': 712,\n",
              " 'voiceless': 63,\n",
              " 'nasal': 104,\n",
              " 'pokomo': 6,\n",
              " 'implication': 411,\n",
              " 'sound': 1387,\n",
              " 'change': 4247,\n",
              " 'kweku': 3,\n",
              " 'osam': 3,\n",
              " 'complementation': 23,\n",
              " 'akan': 6,\n",
              " 'book': 4726,\n",
              " 'review': 2478,\n",
              " 'philip': 215,\n",
              " 'baker': 111,\n",
              " 'creole': 348,\n",
              " 'pidgin': 219,\n",
              " 'varietes': 9,\n",
              " 'vehiculaires': 3,\n",
              " 'proces': 12,\n",
              " 'et': 1760,\n",
              " 'genese': 6,\n",
              " 'gabriel': 35,\n",
              " 'manessy': 3,\n",
              " 'herman': 55,\n",
              " 'batibo': 3,\n",
              " 'topic': 2428,\n",
              " 'salikoko': 14,\n",
              " 'mufwene': 17,\n",
              " 'lioba': 6,\n",
              " 'moshi': 6,\n",
              " 'ed': 1747,\n",
              " 'bruce': 299,\n",
              " 'connell': 29,\n",
              " 'historical': 1054,\n",
              " 'perspective': 973,\n",
              " 'chamba': 3,\n",
              " 'daka': 3,\n",
              " 'raymond': 147,\n",
              " 'boyd': 35,\n",
              " 'jan': 684,\n",
              " 'daeleman': 3,\n",
              " 'luba': 6,\n",
              " 'sprichwoerter': 3,\n",
              " 'uebersetzte': 3,\n",
              " 'erweiterte': 3,\n",
              " 'und': 753,\n",
              " 'ueberarbeitete': 3,\n",
              " 'ausgabe': 7,\n",
              " 'einer': 82,\n",
              " 'anonymen': 3,\n",
              " 'sammlung': 3,\n",
              " 'au': 953,\n",
              " 'zaire': 20,\n",
              " 'tonrelationen': 3,\n",
              " 'sprich': 3,\n",
              " 'woertern': 3,\n",
              " 'reimformen': 3,\n",
              " 'auf': 95,\n",
              " 'suprasegmentaler': 3,\n",
              " 'ebene': 6,\n",
              " 'bei': 66,\n",
              " 'den': 240,\n",
              " 'baluba': 3,\n",
              " 'beena': 3,\n",
              " 'luluwa': 3,\n",
              " 'han': 247,\n",
              " 'ingolf': 39,\n",
              " 'weier': 3,\n",
              " 'jean': 491,\n",
              " 'l': 4753,\n",
              " 'doneux': 3,\n",
              " 'grammar': 2825,\n",
              " 'kisi': 6,\n",
              " 'southern': 547,\n",
              " 'atlantic': 95,\n",
              " 'tucker': 33,\n",
              " 'child': 1777,\n",
              " 'margo': 9,\n",
              " 'fransen': 3,\n",
              " 'discourse': 2354,\n",
              " 'feature': 2025,\n",
              " 'ten': 552,\n",
              " 'west': 934,\n",
              " 'central': 844,\n",
              " 'africa': 491,\n",
              " 'stephen': 396,\n",
              " 'h': 2179,\n",
              " 'levinson': 49,\n",
              " 'kamanda': 3,\n",
              " 'kola': 5,\n",
              " 'la': 2590,\n",
              " 'langue': 138,\n",
              " 'mondo': 17,\n",
              " 'esquisse': 3,\n",
              " 'grammaticale': 12,\n",
              " 'textes': 20,\n",
              " 'dictionnaire': 36,\n",
              " 'andre': 88,\n",
              " 'vallaeys': 3,\n",
              " 'christa': 25,\n",
              " 'koenig': 56,\n",
              " 'perspektiven': 11,\n",
              " 'afrikanistischer': 3,\n",
              " 'forschung': 9,\n",
              " 'beitraege': 16,\n",
              " 'zur': 143,\n",
              " 'linguistik': 53,\n",
              " 'ethnologie': 3,\n",
              " 'geschichte': 41,\n",
              " 'philosophie': 20,\n",
              " 'literatur': 8,\n",
              " 'afrikanistentag': 3,\n",
              " 'bearth': 3,\n",
              " 'wilhelm': 64,\n",
              " 'moehlig': 3,\n",
              " 'beat': 171,\n",
              " 'sottas': 3,\n",
              " 'edgar': 52,\n",
              " 'suter': 4,\n",
              " 'dieke': 7,\n",
              " 'rietkerk': 3,\n",
              " 'talk': 2133,\n",
              " 'thought': 1284,\n",
              " 'thing': 2760,\n",
              " 'emic': 3,\n",
              " 'road': 594,\n",
              " 'towards': 585,\n",
              " 'conscious': 55,\n",
              " 'kenneth': 263,\n",
              " 'pike': 48,\n",
              " 'vincent': 132,\n",
              " 'rooji': 3,\n",
              " 'codeswitching': 30,\n",
              " 'gambia': 7,\n",
              " 'eine': 116,\n",
              " 'soziolinguistische': 3,\n",
              " 'untersuchung': 10,\n",
              " 'von': 370,\n",
              " 'mandinka': 3,\n",
              " 'wolof': 12,\n",
              " 'englisch': 14,\n",
              " 'kontakt': 6,\n",
              " 'delia': 15,\n",
              " 'haust': 3,\n",
              " 'publication': 1682,\n",
              " 'walter': 293,\n",
              " 'inc': 2986,\n",
              " 'postfach': 143,\n",
              " '34': 700,\n",
              " '21': 2025,\n",
              " '200': 1122,\n",
              " 'saw': 455,\n",
              " 'mill': 198,\n",
              " 'river': 276,\n",
              " '10728': 96,\n",
              " 'hawthorne': 103,\n",
              " 'ny': 558,\n",
              " '10532': 96,\n",
              " 'germany': 1307,\n",
              " 'usa': 3428,\n",
              " '49': 1049,\n",
              " '26005': 96,\n",
              " '351': 173,\n",
              " '914': 141,\n",
              " '747': 107,\n",
              " '1326': 99,\n",
              " 'degruyter': 166,\n",
              " 'also': 8971,\n",
              " 'ordered': 487,\n",
              " 'world': 4097,\n",
              " 'wide': 1138,\n",
              " 'web': 4766,\n",
              " 'positive': 459,\n",
              " 'folk': 313,\n",
              " 'done': 1433,\n",
              " 'perhaps': 836,\n",
              " 'someone': 1277,\n",
              " 'drop': 489,\n",
              " 'pointer': 118,\n",
              " 'discussed': 572,\n",
              " 'particular': 1310,\n",
              " 'menu': 180,\n",
              " 'entry': 493,\n",
              " 'remove': 1974,\n",
              " 'sa': 386,\n",
              " 'markup': 43,\n",
              " 'add': 1444,\n",
              " 'whitelisthelp': 1,\n",
              " 'never': 1874,\n",
              " 'customization': 21,\n",
              " 'exmh': 665,\n",
              " 'version': 2360,\n",
              " '23': 1646,\n",
              " '2000': 3862,\n",
              " 'harlan_______________________________________________': 3,\n",
              " 'listman': 136,\n",
              " '06': 920,\n",
              " 'september': 1868,\n",
              " 'tim': 400,\n",
              " 'peter': 879,\n",
              " 'said': 5086,\n",
              " 'case': 4242,\n",
              " 'insensitive': 14,\n",
              " 'different': 3055,\n",
              " 'mime': 220,\n",
              " 'similarly': 94,\n",
              " 'ignoring': 34,\n",
              " 'experiment': 305,\n",
              " 'decide': 393,\n",
              " 'plausible': 61,\n",
              " 'significant': 588,\n",
              " 'generates': 67,\n",
              " 'unusual': 165,\n",
              " 'clueless': 14,\n",
              " 'spammer': 203,\n",
              " 'misconfigures': 3,\n",
              " 'definitely': 273,\n",
              " 'helpful': 252,\n",
              " 'spamassassin': 1567,\n",
              " 'ha': 13283,\n",
              " 'rule': 1593,\n",
              " 'date': 4011,\n",
              " 'point': 3001,\n",
              " 'greg': 347,\n",
              " 'ward': 89,\n",
              " 'gerg': 6,\n",
              " 'ca': 1755,\n",
              " 'god': 373,\n",
              " 'omnipotent': 3,\n",
              " 'omniscient': 2,\n",
              " 'omnibenevolent': 1,\n",
              " 'say': 3361,\n",
              " 'right': 4029,\n",
              " 'label': 358,\n",
              " '793': 13,\n",
              " 'q': 853,\n",
              " 'mohawk': 15,\n",
              " 'russian': 761,\n",
              " 'banning': 36,\n",
              " 'german': 1588,\n",
              " 'mel': 48,\n",
              " 'cuk': 26,\n",
              " 'jun': 185,\n",
              " '1995': 2402,\n",
              " '0400': 154,\n",
              " 'jpkirchner': 33,\n",
              " 'aol': 779,\n",
              " 'banned': 87,\n",
              " 'iowa': 193,\n",
              " 'man': 1163,\n",
              " 'claim': 1488,\n",
              " 'war': 543,\n",
              " 'made': 3385,\n",
              " 'illegal': 158,\n",
              " 'state': 5077,\n",
              " 'decree': 18,\n",
              " 'governor': 147,\n",
              " 'interesting': 903,\n",
              " 'since': 2916,\n",
              " 'family': 1948,\n",
              " 'story': 1001,\n",
              " 'anti': 347,\n",
              " 'discrimination': 40,\n",
              " 'michigan': 256,\n",
              " 'time': 10500,\n",
              " 'limited': 1892,\n",
              " 'snide': 3,\n",
              " 'remark': 276,\n",
              " 'two': 5692,\n",
              " 'surname': 73,\n",
              " 'relevant': 944,\n",
              " 'part': 3491,\n",
              " 'anyone': 2046,\n",
              " 'vouch': 11,\n",
              " 'veracity': 4,\n",
              " 'james': 931,\n",
              " 'kirchner': 68,\n",
              " 'grew': 100,\n",
              " 'household': 89,\n",
              " 'foreign': 1111,\n",
              " 'early': 1199,\n",
              " 'century': 690,\n",
              " 'public': 1266,\n",
              " 'school': 2192,\n",
              " 'english': 6352,\n",
              " 'course': 2571,\n",
              " 'uncle': 43,\n",
              " 'went': 488,\n",
              " 'local': 1342,\n",
              " 'parochial': 20,\n",
              " 'lutheran': 9,\n",
              " 'church': 203,\n",
              " 'liturgy': 9,\n",
              " 'bible': 141,\n",
              " 'luther': 43,\n",
              " 'plattdeutsch': 6,\n",
              " 'speaking': 519,\n",
              " 'go': 3870,\n",
              " 'learn': 1122,\n",
              " 'issued': 317,\n",
              " 'proclamation': 8,\n",
              " 'place': 2981,\n",
              " 'operator': 232,\n",
              " 'instructed': 164,\n",
              " 'pull': 167,\n",
              " 'plug': 156,\n",
              " 'telephone': 1099,\n",
              " 'conversation': 536,\n",
              " 'party': 1272,\n",
              " 'patron': 14,\n",
              " 'hold': 717,\n",
              " 'receiver': 44,\n",
              " 'mouth': 97,\n",
              " 'piece': 517,\n",
              " 'resulting': 274,\n",
              " 'whistling': 7,\n",
              " 'interfere': 32,\n",
              " 'speech': 2931,\n",
              " 'modern': 921,\n",
              " 'dropped': 222,\n",
              " 'curriculum': 207,\n",
              " 'blow': 96,\n",
              " 'really': 1954,\n",
              " 'recovered': 54,\n",
              " 'newspaper': 238,\n",
              " 'published': 1338,\n",
              " 'arrested': 63,\n",
              " 'street': 1356,\n",
              " 'hardship': 31,\n",
              " 'older': 211,\n",
              " 'immigrant': 95,\n",
              " 'suppressed': 8,\n",
              " 'rural': 95,\n",
              " 'county': 209,\n",
              " 'paper': 10431,\n",
              " 'fact': 2277,\n",
              " 'editor': 1019,\n",
              " 'elected': 46,\n",
              " 'treasurer': 88,\n",
              " 'age': 724,\n",
              " 'somewhat': 299,\n",
              " 'embarrassing': 30,\n",
              " 'stumbled': 20,\n",
              " 'organized': 552,\n",
              " 'amounted': 13,\n",
              " 'secret': 856,\n",
              " 'police': 159,\n",
              " 'agency': 682,\n",
              " 'formed': 164,\n",
              " 'whose': 653,\n",
              " 'purpose': 888,\n",
              " 'investigate': 169,\n",
              " 'act': 1540,\n",
              " 'disloyalty': 6,\n",
              " 'given': 2001,\n",
              " 'levy': 42,\n",
              " 'fine': 435,\n",
              " 'imprison': 11,\n",
              " 'duration': 149,\n",
              " 'without': 2468,\n",
              " 'benefit': 948,\n",
              " 'trial': 301,\n",
              " 'america': 1672,\n",
              " 'active': 492,\n",
              " 'participation': 738,\n",
              " 'relatively': 308,\n",
              " 'short': 1563,\n",
              " 'lived': 89,\n",
              " 'around': 1623,\n",
              " 'long': 2713,\n",
              " 'research': 5609,\n",
              " 'sometime': 139,\n",
              " 'record': 1065,\n",
              " 'still': 2581,\n",
              " 'exist': 379,\n",
              " 'happened': 322,\n",
              " 'several': 1985,\n",
              " 'midwest': 66,\n",
              " 'reached': 316,\n",
              " 'supreme': 65,\n",
              " 'court': 640,\n",
              " 'meyer': 120,\n",
              " 'v': 2782,\n",
              " 'nebraska': 77,\n",
              " 'ruled': 60,\n",
              " 'effectively': 189,\n",
              " 'eliminating': 53,\n",
              " 'law': 1130,\n",
              " 'frank': 424,\n",
              " 'anshen': 9,\n",
              " 'dept': 863,\n",
              " 'stony': 30,\n",
              " 'brook': 71,\n",
              " '11794': 6,\n",
              " '1304': 6,\n",
              " 'sum': 705,\n",
              " 'imperialism': 43,\n",
              " 'wed': 317,\n",
              " 'nov': 426,\n",
              " '1994': 1124,\n",
              " 'linguist': 2408,\n",
              " 'came': 752,\n",
              " '18th': 113,\n",
              " 'tan': 29,\n",
              " 'true': 895,\n",
              " 'england': 573,\n",
              " 'fair': 223,\n",
              " '16th': 84,\n",
              " '17th': 113,\n",
              " 'couple': 593,\n",
              " 'example': 3133,\n",
              " 'surely': 227,\n",
              " 'hard': 1589,\n",
              " 'see': 5228,\n",
              " 'relation': 1019,\n",
              " 'humboldt': 62,\n",
              " 'n': 6516,\n",
              " 'even': 3878,\n",
              " 'nation': 558,\n",
              " 'baudoin': 13,\n",
              " 'courtenay': 10,\n",
              " 'poland': 118,\n",
              " 'ditto': 27,\n",
              " 'proponent': 31,\n",
              " 'critical': 441,\n",
              " 'awareness': 115,\n",
              " 'potentially': 138,\n",
              " 'instrument': 128,\n",
              " 'liberation': 16,\n",
              " 'truism': 8,\n",
              " 'science': 2978,\n",
              " 'good': 3337,\n",
              " 'bad': 688,\n",
              " 'seem': 771,\n",
              " 'richard': 783,\n",
              " 'ingham': 32,\n",
              " 'empty': 594,\n",
              " '191': 69,\n",
              " 'opposite': 261,\n",
              " 'known': 937,\n",
              " 'devotee': 12,\n",
              " 'crossword': 7,\n",
              " 'cleave': 10,\n",
              " 'mean': 1853,\n",
              " 'adhere': 31,\n",
              " 'divide': 56,\n",
              " 'cloven': 7,\n",
              " 'hoof': 8,\n",
              " 'meat': 39,\n",
              " 'cleaver': 3,\n",
              " 'hey': 190,\n",
              " 'playing': 248,\n",
              " 'soccer': 23,\n",
              " 'tonight': 105,\n",
              " 'look': 2834,\n",
              " 'pretty': 497,\n",
              " 'much': 3905,\n",
              " 'chickened': 1,\n",
              " 'today': 3059,\n",
              " 'take': 4765,\n",
              " 'courage': 31,\n",
              " 'plan': 2259,\n",
              " 'believe': 1583,\n",
              " 'crystal': 116,\n",
              " 'hyde': 15,\n",
              " 'north': 1310,\n",
              " '121': 93,\n",
              " 'sw': 62,\n",
              " 'salmon': 46,\n",
              " 'portland': 371,\n",
              " 'oregon': 165,\n",
              " '97204': 3,\n",
              " '503': 114,\n",
              " '464': 101,\n",
              " '8318': 2,\n",
              " '3740': 9,\n",
              " 'save': 1808,\n",
              " 'money': 5789,\n",
              " 'getting': 1042,\n",
              " 'oem': 156,\n",
              " 'software': 4091,\n",
              " 'pc': 1053,\n",
              " 'visit': 1909,\n",
              " 'best': 3594,\n",
              " 'lashay': 1,\n",
              " 'milan': 63,\n",
              " 'waterproof': 11,\n",
              " 'stainless': 5,\n",
              " 'steel': 95,\n",
              " 'sapphire': 11,\n",
              " 'surface': 365,\n",
              " 'lovely': 47,\n",
              " 'bring': 889,\n",
              " 'sheer': 19,\n",
              " 'feeling': 215,\n",
              " 'luxury': 36,\n",
              " 'wearing': 31,\n",
              " 'rollexes': 2,\n",
              " 'stylish': 23,\n",
              " 'smart': 234,\n",
              " 'pure': 140,\n",
              " 'gold': 347,\n",
              " 'dazzling': 8,\n",
              " 'diamond': 94,\n",
              " 'rim': 23,\n",
              " 'twinkling': 1,\n",
              " 'star': 368,\n",
              " 'sky': 125,\n",
              " ...}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXm6yay1yqp4",
        "outputId": "81f839da-734c-4b74-df38-8a8fa47ef160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def keep_token(proccessed_token, threshold):\n",
        "  if proccessed_token not in token_counter:\n",
        "    return False\n",
        "  else:\n",
        "    return token_counter[proccessed_token] > threshold\n",
        "\n",
        "keep_token('random', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMiIpz440Lco",
        "outputId": "67b5081f-595d-417f-c715-dabc1aca34b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0',\n",
              " '00',\n",
              " '000',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '08',\n",
              " '09',\n",
              " '1',\n",
              " '10',\n",
              " '100',\n",
              " '11',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '18',\n",
              " '19',\n",
              " '1994',\n",
              " '1995',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '2',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2001',\n",
              " '2002',\n",
              " '21',\n",
              " '22',\n",
              " '23',\n",
              " '24',\n",
              " '25',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '3',\n",
              " '30',\n",
              " '31',\n",
              " '35',\n",
              " '3d',\n",
              " '4',\n",
              " '40',\n",
              " '44',\n",
              " '45',\n",
              " '49',\n",
              " '5',\n",
              " '50',\n",
              " '500',\n",
              " '6',\n",
              " '60',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '90',\n",
              " '95',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '_',\n",
              " 'able',\n",
              " 'abstract',\n",
              " 'ac',\n",
              " 'accepted',\n",
              " 'access',\n",
              " 'account',\n",
              " 'acquisition',\n",
              " 'act',\n",
              " 'action',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'add',\n",
              " 'additional',\n",
              " 'address',\n",
              " 'ago',\n",
              " 'agreement',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'america',\n",
              " 'american',\n",
              " 'among',\n",
              " 'amount',\n",
              " 'analysis',\n",
              " 'analyst',\n",
              " 'announcement',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'application',\n",
              " 'applied',\n",
              " 'approach',\n",
              " 'april',\n",
              " 'area',\n",
              " 'argument',\n",
              " 'around',\n",
              " 'article',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'aspect',\n",
              " 'asset',\n",
              " 'association',\n",
              " 'attached',\n",
              " 'august',\n",
              " 'author',\n",
              " 'available',\n",
              " 'away',\n",
              " 'b',\n",
              " 'back',\n",
              " 'bank',\n",
              " 'based',\n",
              " 'basis',\n",
              " 'become',\n",
              " 'believe',\n",
              " 'benjamin',\n",
              " 'best',\n",
              " 'better',\n",
              " 'big',\n",
              " 'bill',\n",
              " 'billion',\n",
              " 'board',\n",
              " 'book',\n",
              " 'box',\n",
              " 'break',\n",
              " 'bulk',\n",
              " 'business',\n",
              " 'buy',\n",
              " 'c',\n",
              " 'ca',\n",
              " 'california',\n",
              " 'call',\n",
              " 'called',\n",
              " 'canada',\n",
              " 'cannot',\n",
              " 'capital',\n",
              " 'card',\n",
              " 'case',\n",
              " 'cash',\n",
              " 'category',\n",
              " 'cc',\n",
              " 'cd',\n",
              " 'center',\n",
              " 'certain',\n",
              " 'chair',\n",
              " 'change',\n",
              " 'charge',\n",
              " 'check',\n",
              " 'chief',\n",
              " 'child',\n",
              " 'chinese',\n",
              " 'city',\n",
              " 'claim',\n",
              " 'class',\n",
              " 'clear',\n",
              " 'click',\n",
              " 'co',\n",
              " 'code',\n",
              " 'cognitive',\n",
              " 'college',\n",
              " 'color',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comment',\n",
              " 'commercial',\n",
              " 'committee',\n",
              " 'common',\n",
              " 'communication',\n",
              " 'community',\n",
              " 'company',\n",
              " 'complete',\n",
              " 'computational',\n",
              " 'computer',\n",
              " 'conference',\n",
              " 'construction',\n",
              " 'contact',\n",
              " 'content',\n",
              " 'context',\n",
              " 'continue',\n",
              " 'contract',\n",
              " 'control',\n",
              " 'copy',\n",
              " 'copyright',\n",
              " 'corp',\n",
              " 'corpus',\n",
              " 'cost',\n",
              " 'could',\n",
              " 'country',\n",
              " 'course',\n",
              " 'cover',\n",
              " 'credit',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'customer',\n",
              " 'data',\n",
              " 'database',\n",
              " 'date',\n",
              " 'david',\n",
              " 'day',\n",
              " 'de',\n",
              " 'deadline',\n",
              " 'deal',\n",
              " 'dear',\n",
              " 'debt',\n",
              " 'december',\n",
              " 'department',\n",
              " 'description',\n",
              " 'detail',\n",
              " 'development',\n",
              " 'dialect',\n",
              " 'difference',\n",
              " 'different',\n",
              " 'direct',\n",
              " 'director',\n",
              " 'directory',\n",
              " 'discourse',\n",
              " 'discussion',\n",
              " 'document',\n",
              " 'doe',\n",
              " 'dollar',\n",
              " 'domain',\n",
              " 'done',\n",
              " 'dow',\n",
              " 'dr',\n",
              " 'due',\n",
              " 'dynegy',\n",
              " 'e',\n",
              " 'early',\n",
              " 'easy',\n",
              " 'ect',\n",
              " 'ed',\n",
              " 'editor',\n",
              " 'edu',\n",
              " 'education',\n",
              " 'effect',\n",
              " 'effort',\n",
              " 'either',\n",
              " 'electronic',\n",
              " 'else',\n",
              " 'email',\n",
              " 'en',\n",
              " 'end',\n",
              " 'energy',\n",
              " 'english',\n",
              " 'enough',\n",
              " 'enron',\n",
              " 'error',\n",
              " 'especially',\n",
              " 'et',\n",
              " 'etc',\n",
              " 'europe',\n",
              " 'european',\n",
              " 'evaluation',\n",
              " 'even',\n",
              " 'event',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'evidence',\n",
              " 'example',\n",
              " 'exchange',\n",
              " 'executive',\n",
              " 'experience',\n",
              " 'f',\n",
              " 'fact',\n",
              " 'family',\n",
              " 'far',\n",
              " 'fax',\n",
              " 'feature',\n",
              " 'february',\n",
              " 'fee',\n",
              " 'feel',\n",
              " 'field',\n",
              " 'file',\n",
              " 'final',\n",
              " 'financial',\n",
              " 'find',\n",
              " 'first',\n",
              " 'five',\n",
              " 'focus',\n",
              " 'follow',\n",
              " 'following',\n",
              " 'font',\n",
              " 'foreign',\n",
              " 'form',\n",
              " 'format',\n",
              " 'forward',\n",
              " 'forwarded',\n",
              " 'found',\n",
              " 'four',\n",
              " 'france',\n",
              " 'free',\n",
              " 'french',\n",
              " 'friday',\n",
              " 'friend',\n",
              " 'full',\n",
              " 'fund',\n",
              " 'future',\n",
              " 'g',\n",
              " 'gas',\n",
              " 'general',\n",
              " 'german',\n",
              " 'germany',\n",
              " 'get',\n",
              " 'getting',\n",
              " 'give',\n",
              " 'given',\n",
              " 'global',\n",
              " 'go',\n",
              " 'going',\n",
              " 'good',\n",
              " 'got',\n",
              " 'government',\n",
              " 'grammar',\n",
              " 'grant',\n",
              " 'great',\n",
              " 'group',\n",
              " 'h',\n",
              " 'ha',\n",
              " 'hand',\n",
              " 'hard',\n",
              " 'head',\n",
              " 'held',\n",
              " 'help',\n",
              " 'high',\n",
              " 'historical',\n",
              " 'history',\n",
              " 'home',\n",
              " 'hope',\n",
              " 'hotel',\n",
              " 'hou',\n",
              " 'hour',\n",
              " 'houston',\n",
              " 'however',\n",
              " 'html',\n",
              " 'http',\n",
              " 'human',\n",
              " 'hundred',\n",
              " 'id',\n",
              " 'idea',\n",
              " 'ie',\n",
              " 'ilug',\n",
              " 'immediately',\n",
              " 'important',\n",
              " 'inc',\n",
              " 'include',\n",
              " 'included',\n",
              " 'includes',\n",
              " 'including',\n",
              " 'income',\n",
              " 'increase',\n",
              " 'index',\n",
              " 'individual',\n",
              " 'industry',\n",
              " 'info',\n",
              " 'information',\n",
              " 'institute',\n",
              " 'instruction',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'international',\n",
              " 'internet',\n",
              " 'investment',\n",
              " 'investor',\n",
              " 'invited',\n",
              " 'issue',\n",
              " 'item',\n",
              " 'j',\n",
              " 'january',\n",
              " 'japan',\n",
              " 'japanese',\n",
              " 'job',\n",
              " 'john',\n",
              " 'jones',\n",
              " 'journal',\n",
              " 'july',\n",
              " 'june',\n",
              " 'k',\n",
              " 'kaminski',\n",
              " 'keep',\n",
              " 'key',\n",
              " 'kind',\n",
              " 'know',\n",
              " 'knowledge',\n",
              " 'l',\n",
              " 'la',\n",
              " 'language',\n",
              " 'large',\n",
              " 'last',\n",
              " 'late',\n",
              " 'later',\n",
              " 'latest',\n",
              " 'law',\n",
              " 'le',\n",
              " 'learn',\n",
              " 'learning',\n",
              " 'least',\n",
              " 'legal',\n",
              " 'less',\n",
              " 'let',\n",
              " 'letter',\n",
              " 'level',\n",
              " 'lexical',\n",
              " 'life',\n",
              " 'like',\n",
              " 'limited',\n",
              " 'line',\n",
              " 'linguist',\n",
              " 'linguistic',\n",
              " 'linguistics',\n",
              " 'link',\n",
              " 'linux',\n",
              " 'list',\n",
              " 'listinfo',\n",
              " 'little',\n",
              " 'live',\n",
              " 'local',\n",
              " 'logic',\n",
              " 'london',\n",
              " 'long',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'loss',\n",
              " 'lot',\n",
              " 'low',\n",
              " 'lunch',\n",
              " 'machine',\n",
              " 'made',\n",
              " 'mail',\n",
              " 'mailing',\n",
              " 'mailman',\n",
              " 'main',\n",
              " 'major',\n",
              " 'make',\n",
              " 'making',\n",
              " 'man',\n",
              " 'management',\n",
              " 'many',\n",
              " 'march',\n",
              " 'mark',\n",
              " 'market',\n",
              " 'marketing',\n",
              " 'material',\n",
              " 'matter',\n",
              " 'may',\n",
              " 'mean',\n",
              " 'meaning',\n",
              " 'medium',\n",
              " 'meeting',\n",
              " 'member',\n",
              " 'message',\n",
              " 'method',\n",
              " 'michael',\n",
              " 'microsoft',\n",
              " 'might',\n",
              " 'million',\n",
              " 'mind',\n",
              " 'minute',\n",
              " 'model',\n",
              " 'monday',\n",
              " 'money',\n",
              " 'month',\n",
              " 'morphology',\n",
              " 'move',\n",
              " 'movement',\n",
              " 'mr',\n",
              " 'much',\n",
              " 'multi',\n",
              " 'must',\n",
              " 'n',\n",
              " 'name',\n",
              " 'national',\n",
              " 'native',\n",
              " 'natural',\n",
              " 'need',\n",
              " 'net',\n",
              " 'network',\n",
              " 'never',\n",
              " 'new',\n",
              " 'news',\n",
              " 'next',\n",
              " 'nl',\n",
              " 'non',\n",
              " 'north',\n",
              " 'note',\n",
              " 'nothing',\n",
              " 'november',\n",
              " 'number',\n",
              " 'object',\n",
              " 'october',\n",
              " 'offer',\n",
              " 'office',\n",
              " 'old',\n",
              " 'one',\n",
              " 'online',\n",
              " 'open',\n",
              " 'operation',\n",
              " 'opportunity',\n",
              " 'option',\n",
              " 'order',\n",
              " 'org',\n",
              " 'organization',\n",
              " 'original',\n",
              " 'others',\n",
              " 'p',\n",
              " 'package',\n",
              " 'page',\n",
              " 'paid',\n",
              " 'paper',\n",
              " 'part',\n",
              " 'participant',\n",
              " 'particular',\n",
              " 'party',\n",
              " 'past',\n",
              " 'paul',\n",
              " 'pay',\n",
              " 'payment',\n",
              " 'pc',\n",
              " 'people',\n",
              " 'per',\n",
              " 'percent',\n",
              " 'performance',\n",
              " 'person',\n",
              " 'personal',\n",
              " 'phone',\n",
              " 'phonology',\n",
              " 'place',\n",
              " 'plan',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'point',\n",
              " 'policy',\n",
              " 'position',\n",
              " 'possible',\n",
              " 'post',\n",
              " 'potential',\n",
              " 'power',\n",
              " 'pp',\n",
              " 'pragmatic',\n",
              " 'pre',\n",
              " 'present',\n",
              " 'presentation',\n",
              " 'president',\n",
              " 'press',\n",
              " 'price',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'proceeding',\n",
              " 'process',\n",
              " 'processing',\n",
              " 'product',\n",
              " 'production',\n",
              " 'professional',\n",
              " 'professor',\n",
              " 'profit',\n",
              " 'program',\n",
              " 'programme',\n",
              " 'project',\n",
              " 'proposal',\n",
              " 'provide',\n",
              " 'provided',\n",
              " 'public',\n",
              " 'publication',\n",
              " 'published',\n",
              " 'purchase',\n",
              " 'put',\n",
              " 'quality',\n",
              " 'query',\n",
              " 'question',\n",
              " 'r',\n",
              " 'rate',\n",
              " 'rather',\n",
              " 'razor',\n",
              " 'read',\n",
              " 'reading',\n",
              " 'ready',\n",
              " 'real',\n",
              " 'really',\n",
              " 'reason',\n",
              " 'receive',\n",
              " 'received',\n",
              " 'recent',\n",
              " 'record',\n",
              " 'reference',\n",
              " 'regard',\n",
              " 'regarding',\n",
              " 'registration',\n",
              " 'related',\n",
              " 'relation',\n",
              " 'release',\n",
              " 'remember',\n",
              " 'remove',\n",
              " 'removed',\n",
              " 'reply',\n",
              " 'report',\n",
              " 'representation',\n",
              " 'request',\n",
              " 'required',\n",
              " 'research',\n",
              " 'researcher',\n",
              " 'resource',\n",
              " 'response',\n",
              " 'result',\n",
              " 'return',\n",
              " 'review',\n",
              " 'right',\n",
              " 'risk',\n",
              " 'robert',\n",
              " 'role',\n",
              " 'room',\n",
              " 'rpm',\n",
              " 'rule',\n",
              " 'run',\n",
              " 'said',\n",
              " 'sale',\n",
              " 'save',\n",
              " 'say',\n",
              " 'schedule',\n",
              " 'school',\n",
              " 'science',\n",
              " 'search',\n",
              " 'second',\n",
              " 'section',\n",
              " 'security',\n",
              " 'see',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'sell',\n",
              " 'semantic',\n",
              " 'semantics',\n",
              " 'send',\n",
              " 'sent',\n",
              " 'sentence',\n",
              " 'september',\n",
              " 'series',\n",
              " 'server',\n",
              " 'service',\n",
              " 'session',\n",
              " 'set',\n",
              " 'several',\n",
              " 'share',\n",
              " 'short',\n",
              " 'show',\n",
              " 'sign',\n",
              " 'simple',\n",
              " 'simply',\n",
              " 'since',\n",
              " 'single',\n",
              " 'site',\n",
              " 'size',\n",
              " 'small',\n",
              " 'social',\n",
              " 'society',\n",
              " 'software',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'soon',\n",
              " 'sound',\n",
              " 'source',\n",
              " 'sourceforge',\n",
              " 'south',\n",
              " 'spam',\n",
              " 'spamassassin',\n",
              " 'spanish',\n",
              " 'speaker',\n",
              " 'special',\n",
              " 'specific',\n",
              " 'speech',\n",
              " 'spoken',\n",
              " 'st',\n",
              " 'standard',\n",
              " 'start',\n",
              " 'state',\n",
              " 'statement',\n",
              " 'status',\n",
              " 'step',\n",
              " 'still',\n",
              " 'stock',\n",
              " 'stop',\n",
              " 'story',\n",
              " 'street',\n",
              " 'structure',\n",
              " 'student',\n",
              " 'study',\n",
              " 'style',\n",
              " 'subject',\n",
              " 'submission',\n",
              " 'submit',\n",
              " 'subscription',\n",
              " 'success',\n",
              " 'summary',\n",
              " 'summer',\n",
              " 'support',\n",
              " 'sure',\n",
              " 'syntactic',\n",
              " 'syntax',\n",
              " 'system',\n",
              " 'take',\n",
              " 'talk',\n",
              " 'teaching',\n",
              " 'team',\n",
              " 'technical',\n",
              " 'technology',\n",
              " 'tel',\n",
              " 'telephone',\n",
              " 'tell',\n",
              " 'term',\n",
              " 'texas',\n",
              " 'text',\n",
              " 'th',\n",
              " 'thank',\n",
              " 'thanks',\n",
              " 'theme',\n",
              " 'theoretical',\n",
              " 'theory',\n",
              " 'thing',\n",
              " 'think',\n",
              " 'third',\n",
              " 'though',\n",
              " 'thought',\n",
              " 'thousand',\n",
              " 'three',\n",
              " 'thursday',\n",
              " 'time',\n",
              " 'title',\n",
              " 'today',\n",
              " 'together',\n",
              " 'tool',\n",
              " 'top',\n",
              " 'topic',\n",
              " 'total',\n",
              " 'trade',\n",
              " 'trading',\n",
              " 'transaction',\n",
              " 'translation',\n",
              " 'try',\n",
              " 'tuesday',\n",
              " 'tutorial',\n",
              " 'two',\n",
              " 'type',\n",
              " 'u',\n",
              " 'uk',\n",
              " 'un',\n",
              " 'unit',\n",
              " 'united',\n",
              " 'university',\n",
              " 'update',\n",
              " 'url',\n",
              " 'usa',\n",
              " 'use',\n",
              " 'used',\n",
              " 'user',\n",
              " 'using',\n",
              " 'v',\n",
              " 'value',\n",
              " 'various',\n",
              " 'verb',\n",
              " 'version',\n",
              " 'via',\n",
              " 'video',\n",
              " 'view',\n",
              " 'vince',\n",
              " 'visit',\n",
              " 'volume',\n",
              " 'vowel',\n",
              " 'w',\n",
              " 'wa',\n",
              " 'want',\n",
              " 'way',\n",
              " 'web',\n",
              " 'website',\n",
              " 'wednesday',\n",
              " 'week',\n",
              " 'welcome',\n",
              " 'well',\n",
              " 'whether',\n",
              " 'whole',\n",
              " 'wide',\n",
              " 'window',\n",
              " 'wish',\n",
              " 'within',\n",
              " 'without',\n",
              " 'woman',\n",
              " 'word',\n",
              " 'work',\n",
              " 'working',\n",
              " 'workshop',\n",
              " 'world',\n",
              " 'would',\n",
              " 'write',\n",
              " 'writing',\n",
              " 'written',\n",
              " 'wrote',\n",
              " 'www',\n",
              " 'x',\n",
              " 'yahoo',\n",
              " 'year',\n",
              " 'yes',\n",
              " 'yet',\n",
              " 'york',\n",
              " '½ï',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ï'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = set()\n",
        "\n",
        "for token in token_counter:\n",
        "  if keep_token(token, 1000):\n",
        "    features.add(token)\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLz9f1X73cwq",
        "outputId": "28b15d0d-5843-437f-b89c-bdf3abb540df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['least',\n",
              " 'france',\n",
              " 'performance',\n",
              " 'already',\n",
              " 'else',\n",
              " 'let',\n",
              " '11',\n",
              " 'fund',\n",
              " 'writing',\n",
              " 'url',\n",
              " 'april',\n",
              " 'paper',\n",
              " 'issue',\n",
              " 'link',\n",
              " 'content',\n",
              " 'financial',\n",
              " 'man',\n",
              " 'action',\n",
              " 'second',\n",
              " 'class',\n",
              " 'past',\n",
              " 'monday',\n",
              " 'review',\n",
              " 'sentence',\n",
              " 'word',\n",
              " 'also',\n",
              " 'industry',\n",
              " 'world',\n",
              " 'looking',\n",
              " 'short',\n",
              " 'grammar',\n",
              " '49',\n",
              " '2000',\n",
              " 'sale',\n",
              " 'theory',\n",
              " 'get',\n",
              " 'amount',\n",
              " 'california',\n",
              " 'lunch',\n",
              " 'size',\n",
              " 'friday',\n",
              " 'listinfo',\n",
              " 'europe',\n",
              " '17',\n",
              " 'style',\n",
              " 'dow',\n",
              " 'sound',\n",
              " 'welcome',\n",
              " 'member',\n",
              " 'less',\n",
              " 'hand',\n",
              " 'transaction',\n",
              " 'major',\n",
              " '6',\n",
              " 'translation',\n",
              " 'n',\n",
              " 'thousand',\n",
              " 'education',\n",
              " 'project',\n",
              " 'minute',\n",
              " 'international',\n",
              " 'set',\n",
              " 'need',\n",
              " 'required',\n",
              " 'doe',\n",
              " 'security',\n",
              " 'success',\n",
              " 'package',\n",
              " 'feature',\n",
              " 'november',\n",
              " 'right',\n",
              " 'font',\n",
              " 'last',\n",
              " 'london',\n",
              " 'record',\n",
              " 'online',\n",
              " 'bill',\n",
              " 'mailman',\n",
              " 'la',\n",
              " 'come',\n",
              " '100',\n",
              " 'human',\n",
              " 'look',\n",
              " 'plus',\n",
              " 'limited',\n",
              " 'ilug',\n",
              " 'end',\n",
              " 'editor',\n",
              " 'yes',\n",
              " '5',\n",
              " '01',\n",
              " '1998',\n",
              " 'domain',\n",
              " 'sure',\n",
              " 'et',\n",
              " 'present',\n",
              " '03',\n",
              " 'series',\n",
              " 'english',\n",
              " 'written',\n",
              " 'matter',\n",
              " 'loss',\n",
              " '23',\n",
              " 'dear',\n",
              " 'original',\n",
              " 'provide',\n",
              " 'evidence',\n",
              " 'people',\n",
              " 'de',\n",
              " 'team',\n",
              " 'european',\n",
              " 'receive',\n",
              " 'free',\n",
              " 'internet',\n",
              " 'detail',\n",
              " '½ï',\n",
              " '200',\n",
              " '0',\n",
              " 'immediately',\n",
              " 'workshop',\n",
              " 'w',\n",
              " 'section',\n",
              " 'regard',\n",
              " 'tutorial',\n",
              " 'p',\n",
              " 'change',\n",
              " 'early',\n",
              " 'institute',\n",
              " 'community',\n",
              " 'medium',\n",
              " 'recent',\n",
              " 'session',\n",
              " 'america',\n",
              " 'contract',\n",
              " 'ect',\n",
              " 'semantic',\n",
              " 'another',\n",
              " 'level',\n",
              " 'spamassassin',\n",
              " 'open',\n",
              " 'ago',\n",
              " 'data',\n",
              " 'buy',\n",
              " 'share',\n",
              " 'child',\n",
              " 'german',\n",
              " 'message',\n",
              " 'question',\n",
              " 'possible',\n",
              " 'book',\n",
              " 'wide',\n",
              " 'syntactic',\n",
              " 'five',\n",
              " 'week',\n",
              " 'product',\n",
              " 'hope',\n",
              " 'without',\n",
              " 'point',\n",
              " 'kind',\n",
              " 'simple',\n",
              " 'particular',\n",
              " 'etc',\n",
              " 'take',\n",
              " 'global',\n",
              " 'ï',\n",
              " 'journal',\n",
              " 'however',\n",
              " 'government',\n",
              " 'important',\n",
              " 'mail',\n",
              " 'x',\n",
              " 'new',\n",
              " 'public',\n",
              " 'move',\n",
              " 'simply',\n",
              " 'vince',\n",
              " 'professional',\n",
              " '90',\n",
              " 'info',\n",
              " 'linguistics',\n",
              " 'item',\n",
              " 'hundred',\n",
              " 'science',\n",
              " 'later',\n",
              " 'use',\n",
              " 'june',\n",
              " 'certain',\n",
              " 'aspect',\n",
              " '4',\n",
              " 'acquisition',\n",
              " '08',\n",
              " 'study',\n",
              " 'mind',\n",
              " 'japanese',\n",
              " 'morphology',\n",
              " 'various',\n",
              " 'e',\n",
              " 'office',\n",
              " 'abstract',\n",
              " 'version',\n",
              " 'telephone',\n",
              " 'software',\n",
              " 'investor',\n",
              " 'key',\n",
              " 'dialect',\n",
              " 'anyone',\n",
              " 'meaning',\n",
              " '25',\n",
              " '50',\n",
              " 'request',\n",
              " 'either',\n",
              " 'natural',\n",
              " 'thank',\n",
              " 'chair',\n",
              " 'pm',\n",
              " 'form',\n",
              " 'power',\n",
              " 'houston',\n",
              " 'make',\n",
              " 'edu',\n",
              " 'one',\n",
              " 'visit',\n",
              " 'server',\n",
              " 'conference',\n",
              " 'copyright',\n",
              " 'within',\n",
              " 'making',\n",
              " 'third',\n",
              " 'final',\n",
              " 'potential',\n",
              " 'board',\n",
              " 'cognitive',\n",
              " 'search',\n",
              " 'capital',\n",
              " 'enough',\n",
              " 'business',\n",
              " 'cover',\n",
              " 'large',\n",
              " 'step',\n",
              " 'state',\n",
              " 'money',\n",
              " 'give',\n",
              " 'claim',\n",
              " '24',\n",
              " 'debt',\n",
              " '22',\n",
              " 'opportunity',\n",
              " 'model',\n",
              " 'mr',\n",
              " 'commercial',\n",
              " 'late',\n",
              " 'paid',\n",
              " 'far',\n",
              " 'though',\n",
              " '31',\n",
              " 'fact',\n",
              " 'among',\n",
              " 'bulk',\n",
              " 'following',\n",
              " 'purchase',\n",
              " '2',\n",
              " 'never',\n",
              " 'special',\n",
              " 'several',\n",
              " 'ask',\n",
              " 'someone',\n",
              " 'wish',\n",
              " 'good',\n",
              " 'away',\n",
              " 'please',\n",
              " 'basis',\n",
              " 'number',\n",
              " '9',\n",
              " 'room',\n",
              " 'college',\n",
              " 'university',\n",
              " 'interested',\n",
              " 'payment',\n",
              " 'deadline',\n",
              " 'j',\n",
              " 'field',\n",
              " 'update',\n",
              " '09',\n",
              " 'discourse',\n",
              " 'site',\n",
              " 'source',\n",
              " 'th',\n",
              " 'network',\n",
              " 'ad',\n",
              " 'like',\n",
              " 'unit',\n",
              " 'small',\n",
              " '14',\n",
              " 'service',\n",
              " 'seen',\n",
              " 'policy',\n",
              " 'high',\n",
              " 'credit',\n",
              " 'h',\n",
              " 'department',\n",
              " 'best',\n",
              " 'cash',\n",
              " 'run',\n",
              " '27',\n",
              " 'movement',\n",
              " '29',\n",
              " 'want',\n",
              " 'resource',\n",
              " 'something',\n",
              " 'report',\n",
              " 'historical',\n",
              " 'dr',\n",
              " 'news',\n",
              " 'machine',\n",
              " 'index',\n",
              " 'applied',\n",
              " 'foreign',\n",
              " 'format',\n",
              " 'vowel',\n",
              " 'process',\n",
              " 'received',\n",
              " 'national',\n",
              " 'wednesday',\n",
              " '500',\n",
              " '19',\n",
              " 'remember',\n",
              " 'break',\n",
              " 'research',\n",
              " 'l',\n",
              " 'technical',\n",
              " 'provided',\n",
              " 'representation',\n",
              " 'idea',\n",
              " 'current',\n",
              " '1',\n",
              " 'thursday',\n",
              " 'asked',\n",
              " 'kaminski',\n",
              " 'evaluation',\n",
              " 'much',\n",
              " 'month',\n",
              " 'teaching',\n",
              " 'said',\n",
              " 'today',\n",
              " 'management',\n",
              " 'comment',\n",
              " 'rate',\n",
              " 'difference',\n",
              " 'price',\n",
              " 'two',\n",
              " 'north',\n",
              " 'b',\n",
              " 'using',\n",
              " 'send',\n",
              " '8',\n",
              " 'researcher',\n",
              " 'united',\n",
              " 'published',\n",
              " 'society',\n",
              " 'multi',\n",
              " 'example',\n",
              " 'rather',\n",
              " 'plan',\n",
              " 'profit',\n",
              " 'group',\n",
              " 'speaker',\n",
              " 'canada',\n",
              " 'net',\n",
              " 'spanish',\n",
              " 'whether',\n",
              " 'soon',\n",
              " 'le',\n",
              " 'support',\n",
              " 'agreement',\n",
              " 'sourceforge',\n",
              " 'linux',\n",
              " 'john',\n",
              " '_',\n",
              " 'spam',\n",
              " 'registration',\n",
              " 'instruction',\n",
              " 'co',\n",
              " 'woman',\n",
              " '13',\n",
              " '40',\n",
              " 'invited',\n",
              " 'technology',\n",
              " 'way',\n",
              " 'statement',\n",
              " 'speech',\n",
              " 'even',\n",
              " '12',\n",
              " 'mean',\n",
              " 'linguistic',\n",
              " 'enron',\n",
              " 'history',\n",
              " 'inc',\n",
              " 'based',\n",
              " 'real',\n",
              " 'r',\n",
              " 'see',\n",
              " 'always',\n",
              " 'box',\n",
              " 'subscription',\n",
              " 'everyone',\n",
              " 'wrote',\n",
              " '21',\n",
              " 'say',\n",
              " 'talk',\n",
              " 'energy',\n",
              " 'especially',\n",
              " 'put',\n",
              " 'announcement',\n",
              " 'stock',\n",
              " 'september',\n",
              " 'believe',\n",
              " 'microsoft',\n",
              " 'document',\n",
              " 'name',\n",
              " 'meeting',\n",
              " 'thanks',\n",
              " 'full',\n",
              " '95',\n",
              " 'home',\n",
              " 'old',\n",
              " 'called',\n",
              " 'japan',\n",
              " 'id',\n",
              " 'director',\n",
              " 'story',\n",
              " 'relation',\n",
              " 'participant',\n",
              " 'term',\n",
              " 'address',\n",
              " 'effect',\n",
              " 'hotel',\n",
              " 'website',\n",
              " 'nothing',\n",
              " 'st',\n",
              " '1995',\n",
              " 'together',\n",
              " 'feel',\n",
              " 'probably',\n",
              " 'done',\n",
              " 'razor',\n",
              " 'computational',\n",
              " 'street',\n",
              " 'company',\n",
              " 'august',\n",
              " 'call',\n",
              " 'got',\n",
              " 'ever',\n",
              " 'next',\n",
              " 'thing',\n",
              " 'html',\n",
              " 'david',\n",
              " 'return',\n",
              " 'window',\n",
              " 'development',\n",
              " 'become',\n",
              " 'submission',\n",
              " 'great',\n",
              " 'tel',\n",
              " 'included',\n",
              " 'start',\n",
              " 'spoken',\n",
              " 'four',\n",
              " 'rule',\n",
              " 'day',\n",
              " 'could',\n",
              " 'theme',\n",
              " 'file',\n",
              " 'program',\n",
              " 'schedule',\n",
              " '45',\n",
              " 'event',\n",
              " 'press',\n",
              " 'work',\n",
              " 'corpus',\n",
              " 'case',\n",
              " 'benjamin',\n",
              " 'area',\n",
              " 'live',\n",
              " 'around',\n",
              " 'pragmatic',\n",
              " 'ac',\n",
              " 'publication',\n",
              " 'production',\n",
              " 'c',\n",
              " 'tool',\n",
              " '20',\n",
              " 'job',\n",
              " 'place',\n",
              " 'paul',\n",
              " 'center',\n",
              " 'although',\n",
              " 'accepted',\n",
              " 'french',\n",
              " 'main',\n",
              " 'system',\n",
              " '1997',\n",
              " 'en',\n",
              " 'com',\n",
              " 'discussion',\n",
              " 'email',\n",
              " 'video',\n",
              " '3d',\n",
              " 'include',\n",
              " 'answer',\n",
              " '60',\n",
              " '97',\n",
              " 'working',\n",
              " 'fee',\n",
              " 'time',\n",
              " 'since',\n",
              " '35',\n",
              " 'available',\n",
              " 'charge',\n",
              " 'person',\n",
              " 'increase',\n",
              " 'pay',\n",
              " 'trading',\n",
              " 'reference',\n",
              " 'cost',\n",
              " 'interest',\n",
              " 'summer',\n",
              " 'analyst',\n",
              " 'semantics',\n",
              " 'linguist',\n",
              " 'back',\n",
              " 'exchange',\n",
              " 'keep',\n",
              " 'student',\n",
              " 'percent',\n",
              " 'seems',\n",
              " 'topic',\n",
              " 'includes',\n",
              " 'try',\n",
              " 'author',\n",
              " 'u',\n",
              " 'learn',\n",
              " 'corp',\n",
              " 'given',\n",
              " 'code',\n",
              " '7',\n",
              " 'description',\n",
              " 'legal',\n",
              " 'including',\n",
              " 'view',\n",
              " 'getting',\n",
              " 'dynegy',\n",
              " 'operation',\n",
              " '44',\n",
              " '2001',\n",
              " 'hour',\n",
              " 'material',\n",
              " 'language',\n",
              " 'color',\n",
              " '10',\n",
              " 'course',\n",
              " 'effort',\n",
              " 'text',\n",
              " 'lexical',\n",
              " 'directory',\n",
              " 'better',\n",
              " 'phonology',\n",
              " 'future',\n",
              " 'many',\n",
              " 'usa',\n",
              " 'rpm',\n",
              " 'release',\n",
              " 'access',\n",
              " 'k',\n",
              " 'letter',\n",
              " 'programme',\n",
              " 'note',\n",
              " 'query',\n",
              " 'social',\n",
              " 'clear',\n",
              " 'ie',\n",
              " 'hou',\n",
              " 'reason',\n",
              " 'december',\n",
              " 'follow',\n",
              " 'different',\n",
              " 'verb',\n",
              " 'individual',\n",
              " 'v',\n",
              " 'volume',\n",
              " 'might',\n",
              " '1994',\n",
              " 'user',\n",
              " 'go',\n",
              " 'act',\n",
              " 'marketing',\n",
              " 'ready',\n",
              " 'easy',\n",
              " 'status',\n",
              " 'g',\n",
              " 'native',\n",
              " 'pc',\n",
              " 'mark',\n",
              " 'proceeding',\n",
              " 'professor',\n",
              " 'experience',\n",
              " 'tell',\n",
              " 'list',\n",
              " 'date',\n",
              " 'must',\n",
              " 'result',\n",
              " 'response',\n",
              " 'wa',\n",
              " 'submit',\n",
              " 'ã',\n",
              " 'category',\n",
              " 'offer',\n",
              " 'context',\n",
              " '16',\n",
              " 'american',\n",
              " 'communication',\n",
              " 'ca',\n",
              " 'error',\n",
              " 'friend',\n",
              " 'check',\n",
              " 'ha',\n",
              " 'city',\n",
              " 'risk',\n",
              " 'org',\n",
              " 'market',\n",
              " 'total',\n",
              " 'tuesday',\n",
              " 'learning',\n",
              " 'show',\n",
              " 'general',\n",
              " 'committee',\n",
              " 'anything',\n",
              " 'family',\n",
              " 'order',\n",
              " 'read',\n",
              " 'theoretical',\n",
              " 'july',\n",
              " 'electronic',\n",
              " 'three',\n",
              " 'sent',\n",
              " 'life',\n",
              " 'summary',\n",
              " 'executive',\n",
              " 'structure',\n",
              " '99',\n",
              " 'application',\n",
              " '28',\n",
              " '1999',\n",
              " 'bank',\n",
              " 'argument',\n",
              " 'going',\n",
              " 'forwarded',\n",
              " '30',\n",
              " 'chief',\n",
              " 'via',\n",
              " 'problem',\n",
              " 'yahoo',\n",
              " 'www',\n",
              " 'analysis',\n",
              " 'account',\n",
              " 'grant',\n",
              " 'click',\n",
              " 'association',\n",
              " 'really',\n",
              " 'related',\n",
              " 'personal',\n",
              " 'position',\n",
              " 'well',\n",
              " 'stop',\n",
              " 'regarding',\n",
              " 'add',\n",
              " 'sell',\n",
              " 'method',\n",
              " 'nl',\n",
              " 'school',\n",
              " 'currently',\n",
              " 'robert',\n",
              " 'first',\n",
              " '98',\n",
              " 'post',\n",
              " 'presentation',\n",
              " '26',\n",
              " 'type',\n",
              " 'un',\n",
              " 'every',\n",
              " 'everything',\n",
              " 'knowledge',\n",
              " '02',\n",
              " 'â',\n",
              " 'approach',\n",
              " '000',\n",
              " 'held',\n",
              " 'income',\n",
              " 'dollar',\n",
              " 'texas',\n",
              " '15',\n",
              " 'party',\n",
              " 'used',\n",
              " 'fax',\n",
              " 'removed',\n",
              " 'construction',\n",
              " 'others',\n",
              " 'cannot',\n",
              " 'march',\n",
              " 'long',\n",
              " '18',\n",
              " 'reply',\n",
              " 'president',\n",
              " 'card',\n",
              " 'local',\n",
              " 'mailing',\n",
              " 'reading',\n",
              " 'uk',\n",
              " 'jones',\n",
              " 'phone',\n",
              " 'database',\n",
              " 'common',\n",
              " 'control',\n",
              " 'logic',\n",
              " 'line',\n",
              " 'object',\n",
              " 'article',\n",
              " 'cd',\n",
              " 'big',\n",
              " 'direct',\n",
              " 'gas',\n",
              " 'contact',\n",
              " 'deal',\n",
              " 'copy',\n",
              " 'computer',\n",
              " 'non',\n",
              " 'page',\n",
              " '04',\n",
              " 'hard',\n",
              " 'help',\n",
              " 'trade',\n",
              " 'head',\n",
              " 'forward',\n",
              " 'http',\n",
              " 'attached',\n",
              " 'found',\n",
              " 'little',\n",
              " 'top',\n",
              " 'may',\n",
              " 'whole',\n",
              " 'billion',\n",
              " 'pp',\n",
              " 'write',\n",
              " 'michael',\n",
              " 'latest',\n",
              " 'f',\n",
              " 'law',\n",
              " 'south',\n",
              " 'information',\n",
              " 'pre',\n",
              " 'complete',\n",
              " '3',\n",
              " '2002',\n",
              " 'know',\n",
              " 'york',\n",
              " 'asset',\n",
              " 'would',\n",
              " 'million',\n",
              " 'organization',\n",
              " 'processing',\n",
              " 'proposal',\n",
              " 'february',\n",
              " 'january',\n",
              " 'subject',\n",
              " 'actually',\n",
              " 'year',\n",
              " 'specific',\n",
              " 'continue',\n",
              " 'thought',\n",
              " 'made',\n",
              " 'single',\n",
              " 'cc',\n",
              " 'value',\n",
              " 'customer',\n",
              " 'quality',\n",
              " 'remove',\n",
              " '00',\n",
              " 'per',\n",
              " 'yet',\n",
              " 'october',\n",
              " 'part',\n",
              " 'title',\n",
              " 'standard',\n",
              " 'ed',\n",
              " 'option',\n",
              " 'save',\n",
              " 'syntax',\n",
              " 'additional',\n",
              " '05',\n",
              " 'low',\n",
              " 'able',\n",
              " 'role',\n",
              " 'find',\n",
              " 'sign',\n",
              " 'still',\n",
              " 'chinese',\n",
              " 'country',\n",
              " 'germany',\n",
              " 'focus',\n",
              " 'due',\n",
              " 'think',\n",
              " 'lot',\n",
              " 'investment',\n",
              " 'web']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = list(features)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8357HG1Q3itk",
        "outputId": "188a7383-4bdb-49e5-9f40-ef14e28b6258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'least': 0,\n",
              " 'france': 1,\n",
              " 'performance': 2,\n",
              " 'already': 3,\n",
              " 'else': 4,\n",
              " 'let': 5,\n",
              " '11': 6,\n",
              " 'fund': 7,\n",
              " 'writing': 8,\n",
              " 'url': 9,\n",
              " 'april': 10,\n",
              " 'paper': 11,\n",
              " 'issue': 12,\n",
              " 'link': 13,\n",
              " 'content': 14,\n",
              " 'financial': 15,\n",
              " 'man': 16,\n",
              " 'action': 17,\n",
              " 'second': 18,\n",
              " 'class': 19,\n",
              " 'past': 20,\n",
              " 'monday': 21,\n",
              " 'review': 22,\n",
              " 'sentence': 23,\n",
              " 'word': 24,\n",
              " 'also': 25,\n",
              " 'industry': 26,\n",
              " 'world': 27,\n",
              " 'looking': 28,\n",
              " 'short': 29,\n",
              " 'grammar': 30,\n",
              " '49': 31,\n",
              " '2000': 32,\n",
              " 'sale': 33,\n",
              " 'theory': 34,\n",
              " 'get': 35,\n",
              " 'amount': 36,\n",
              " 'california': 37,\n",
              " 'lunch': 38,\n",
              " 'size': 39,\n",
              " 'friday': 40,\n",
              " 'listinfo': 41,\n",
              " 'europe': 42,\n",
              " '17': 43,\n",
              " 'style': 44,\n",
              " 'dow': 45,\n",
              " 'sound': 46,\n",
              " 'welcome': 47,\n",
              " 'member': 48,\n",
              " 'less': 49,\n",
              " 'hand': 50,\n",
              " 'transaction': 51,\n",
              " 'major': 52,\n",
              " '6': 53,\n",
              " 'translation': 54,\n",
              " 'n': 55,\n",
              " 'thousand': 56,\n",
              " 'education': 57,\n",
              " 'project': 58,\n",
              " 'minute': 59,\n",
              " 'international': 60,\n",
              " 'set': 61,\n",
              " 'need': 62,\n",
              " 'required': 63,\n",
              " 'doe': 64,\n",
              " 'security': 65,\n",
              " 'success': 66,\n",
              " 'package': 67,\n",
              " 'feature': 68,\n",
              " 'november': 69,\n",
              " 'right': 70,\n",
              " 'font': 71,\n",
              " 'last': 72,\n",
              " 'london': 73,\n",
              " 'record': 74,\n",
              " 'online': 75,\n",
              " 'bill': 76,\n",
              " 'mailman': 77,\n",
              " 'la': 78,\n",
              " 'come': 79,\n",
              " '100': 80,\n",
              " 'human': 81,\n",
              " 'look': 82,\n",
              " 'plus': 83,\n",
              " 'limited': 84,\n",
              " 'ilug': 85,\n",
              " 'end': 86,\n",
              " 'editor': 87,\n",
              " 'yes': 88,\n",
              " '5': 89,\n",
              " '01': 90,\n",
              " '1998': 91,\n",
              " 'domain': 92,\n",
              " 'sure': 93,\n",
              " 'et': 94,\n",
              " 'present': 95,\n",
              " '03': 96,\n",
              " 'series': 97,\n",
              " 'english': 98,\n",
              " 'written': 99,\n",
              " 'matter': 100,\n",
              " 'loss': 101,\n",
              " '23': 102,\n",
              " 'dear': 103,\n",
              " 'original': 104,\n",
              " 'provide': 105,\n",
              " 'evidence': 106,\n",
              " 'people': 107,\n",
              " 'de': 108,\n",
              " 'team': 109,\n",
              " 'european': 110,\n",
              " 'receive': 111,\n",
              " 'free': 112,\n",
              " 'internet': 113,\n",
              " 'detail': 114,\n",
              " '½ï': 115,\n",
              " '200': 116,\n",
              " '0': 117,\n",
              " 'immediately': 118,\n",
              " 'workshop': 119,\n",
              " 'w': 120,\n",
              " 'section': 121,\n",
              " 'regard': 122,\n",
              " 'tutorial': 123,\n",
              " 'p': 124,\n",
              " 'change': 125,\n",
              " 'early': 126,\n",
              " 'institute': 127,\n",
              " 'community': 128,\n",
              " 'medium': 129,\n",
              " 'recent': 130,\n",
              " 'session': 131,\n",
              " 'america': 132,\n",
              " 'contract': 133,\n",
              " 'ect': 134,\n",
              " 'semantic': 135,\n",
              " 'another': 136,\n",
              " 'level': 137,\n",
              " 'spamassassin': 138,\n",
              " 'open': 139,\n",
              " 'ago': 140,\n",
              " 'data': 141,\n",
              " 'buy': 142,\n",
              " 'share': 143,\n",
              " 'child': 144,\n",
              " 'german': 145,\n",
              " 'message': 146,\n",
              " 'question': 147,\n",
              " 'possible': 148,\n",
              " 'book': 149,\n",
              " 'wide': 150,\n",
              " 'syntactic': 151,\n",
              " 'five': 152,\n",
              " 'week': 153,\n",
              " 'product': 154,\n",
              " 'hope': 155,\n",
              " 'without': 156,\n",
              " 'point': 157,\n",
              " 'kind': 158,\n",
              " 'simple': 159,\n",
              " 'particular': 160,\n",
              " 'etc': 161,\n",
              " 'take': 162,\n",
              " 'global': 163,\n",
              " 'ï': 164,\n",
              " 'journal': 165,\n",
              " 'however': 166,\n",
              " 'government': 167,\n",
              " 'important': 168,\n",
              " 'mail': 169,\n",
              " 'x': 170,\n",
              " 'new': 171,\n",
              " 'public': 172,\n",
              " 'move': 173,\n",
              " 'simply': 174,\n",
              " 'vince': 175,\n",
              " 'professional': 176,\n",
              " '90': 177,\n",
              " 'info': 178,\n",
              " 'linguistics': 179,\n",
              " 'item': 180,\n",
              " 'hundred': 181,\n",
              " 'science': 182,\n",
              " 'later': 183,\n",
              " 'use': 184,\n",
              " 'june': 185,\n",
              " 'certain': 186,\n",
              " 'aspect': 187,\n",
              " '4': 188,\n",
              " 'acquisition': 189,\n",
              " '08': 190,\n",
              " 'study': 191,\n",
              " 'mind': 192,\n",
              " 'japanese': 193,\n",
              " 'morphology': 194,\n",
              " 'various': 195,\n",
              " 'e': 196,\n",
              " 'office': 197,\n",
              " 'abstract': 198,\n",
              " 'version': 199,\n",
              " 'telephone': 200,\n",
              " 'software': 201,\n",
              " 'investor': 202,\n",
              " 'key': 203,\n",
              " 'dialect': 204,\n",
              " 'anyone': 205,\n",
              " 'meaning': 206,\n",
              " '25': 207,\n",
              " '50': 208,\n",
              " 'request': 209,\n",
              " 'either': 210,\n",
              " 'natural': 211,\n",
              " 'thank': 212,\n",
              " 'chair': 213,\n",
              " 'pm': 214,\n",
              " 'form': 215,\n",
              " 'power': 216,\n",
              " 'houston': 217,\n",
              " 'make': 218,\n",
              " 'edu': 219,\n",
              " 'one': 220,\n",
              " 'visit': 221,\n",
              " 'server': 222,\n",
              " 'conference': 223,\n",
              " 'copyright': 224,\n",
              " 'within': 225,\n",
              " 'making': 226,\n",
              " 'third': 227,\n",
              " 'final': 228,\n",
              " 'potential': 229,\n",
              " 'board': 230,\n",
              " 'cognitive': 231,\n",
              " 'search': 232,\n",
              " 'capital': 233,\n",
              " 'enough': 234,\n",
              " 'business': 235,\n",
              " 'cover': 236,\n",
              " 'large': 237,\n",
              " 'step': 238,\n",
              " 'state': 239,\n",
              " 'money': 240,\n",
              " 'give': 241,\n",
              " 'claim': 242,\n",
              " '24': 243,\n",
              " 'debt': 244,\n",
              " '22': 245,\n",
              " 'opportunity': 246,\n",
              " 'model': 247,\n",
              " 'mr': 248,\n",
              " 'commercial': 249,\n",
              " 'late': 250,\n",
              " 'paid': 251,\n",
              " 'far': 252,\n",
              " 'though': 253,\n",
              " '31': 254,\n",
              " 'fact': 255,\n",
              " 'among': 256,\n",
              " 'bulk': 257,\n",
              " 'following': 258,\n",
              " 'purchase': 259,\n",
              " '2': 260,\n",
              " 'never': 261,\n",
              " 'special': 262,\n",
              " 'several': 263,\n",
              " 'ask': 264,\n",
              " 'someone': 265,\n",
              " 'wish': 266,\n",
              " 'good': 267,\n",
              " 'away': 268,\n",
              " 'please': 269,\n",
              " 'basis': 270,\n",
              " 'number': 271,\n",
              " '9': 272,\n",
              " 'room': 273,\n",
              " 'college': 274,\n",
              " 'university': 275,\n",
              " 'interested': 276,\n",
              " 'payment': 277,\n",
              " 'deadline': 278,\n",
              " 'j': 279,\n",
              " 'field': 280,\n",
              " 'update': 281,\n",
              " '09': 282,\n",
              " 'discourse': 283,\n",
              " 'site': 284,\n",
              " 'source': 285,\n",
              " 'th': 286,\n",
              " 'network': 287,\n",
              " 'ad': 288,\n",
              " 'like': 289,\n",
              " 'unit': 290,\n",
              " 'small': 291,\n",
              " '14': 292,\n",
              " 'service': 293,\n",
              " 'seen': 294,\n",
              " 'policy': 295,\n",
              " 'high': 296,\n",
              " 'credit': 297,\n",
              " 'h': 298,\n",
              " 'department': 299,\n",
              " 'best': 300,\n",
              " 'cash': 301,\n",
              " 'run': 302,\n",
              " '27': 303,\n",
              " 'movement': 304,\n",
              " '29': 305,\n",
              " 'want': 306,\n",
              " 'resource': 307,\n",
              " 'something': 308,\n",
              " 'report': 309,\n",
              " 'historical': 310,\n",
              " 'dr': 311,\n",
              " 'news': 312,\n",
              " 'machine': 313,\n",
              " 'index': 314,\n",
              " 'applied': 315,\n",
              " 'foreign': 316,\n",
              " 'format': 317,\n",
              " 'vowel': 318,\n",
              " 'process': 319,\n",
              " 'received': 320,\n",
              " 'national': 321,\n",
              " 'wednesday': 322,\n",
              " '500': 323,\n",
              " '19': 324,\n",
              " 'remember': 325,\n",
              " 'break': 326,\n",
              " 'research': 327,\n",
              " 'l': 328,\n",
              " 'technical': 329,\n",
              " 'provided': 330,\n",
              " 'representation': 331,\n",
              " 'idea': 332,\n",
              " 'current': 333,\n",
              " '1': 334,\n",
              " 'thursday': 335,\n",
              " 'asked': 336,\n",
              " 'kaminski': 337,\n",
              " 'evaluation': 338,\n",
              " 'much': 339,\n",
              " 'month': 340,\n",
              " 'teaching': 341,\n",
              " 'said': 342,\n",
              " 'today': 343,\n",
              " 'management': 344,\n",
              " 'comment': 345,\n",
              " 'rate': 346,\n",
              " 'difference': 347,\n",
              " 'price': 348,\n",
              " 'two': 349,\n",
              " 'north': 350,\n",
              " 'b': 351,\n",
              " 'using': 352,\n",
              " 'send': 353,\n",
              " '8': 354,\n",
              " 'researcher': 355,\n",
              " 'united': 356,\n",
              " 'published': 357,\n",
              " 'society': 358,\n",
              " 'multi': 359,\n",
              " 'example': 360,\n",
              " 'rather': 361,\n",
              " 'plan': 362,\n",
              " 'profit': 363,\n",
              " 'group': 364,\n",
              " 'speaker': 365,\n",
              " 'canada': 366,\n",
              " 'net': 367,\n",
              " 'spanish': 368,\n",
              " 'whether': 369,\n",
              " 'soon': 370,\n",
              " 'le': 371,\n",
              " 'support': 372,\n",
              " 'agreement': 373,\n",
              " 'sourceforge': 374,\n",
              " 'linux': 375,\n",
              " 'john': 376,\n",
              " '_': 377,\n",
              " 'spam': 378,\n",
              " 'registration': 379,\n",
              " 'instruction': 380,\n",
              " 'co': 381,\n",
              " 'woman': 382,\n",
              " '13': 383,\n",
              " '40': 384,\n",
              " 'invited': 385,\n",
              " 'technology': 386,\n",
              " 'way': 387,\n",
              " 'statement': 388,\n",
              " 'speech': 389,\n",
              " 'even': 390,\n",
              " '12': 391,\n",
              " 'mean': 392,\n",
              " 'linguistic': 393,\n",
              " 'enron': 394,\n",
              " 'history': 395,\n",
              " 'inc': 396,\n",
              " 'based': 397,\n",
              " 'real': 398,\n",
              " 'r': 399,\n",
              " 'see': 400,\n",
              " 'always': 401,\n",
              " 'box': 402,\n",
              " 'subscription': 403,\n",
              " 'everyone': 404,\n",
              " 'wrote': 405,\n",
              " '21': 406,\n",
              " 'say': 407,\n",
              " 'talk': 408,\n",
              " 'energy': 409,\n",
              " 'especially': 410,\n",
              " 'put': 411,\n",
              " 'announcement': 412,\n",
              " 'stock': 413,\n",
              " 'september': 414,\n",
              " 'believe': 415,\n",
              " 'microsoft': 416,\n",
              " 'document': 417,\n",
              " 'name': 418,\n",
              " 'meeting': 419,\n",
              " 'thanks': 420,\n",
              " 'full': 421,\n",
              " '95': 422,\n",
              " 'home': 423,\n",
              " 'old': 424,\n",
              " 'called': 425,\n",
              " 'japan': 426,\n",
              " 'id': 427,\n",
              " 'director': 428,\n",
              " 'story': 429,\n",
              " 'relation': 430,\n",
              " 'participant': 431,\n",
              " 'term': 432,\n",
              " 'address': 433,\n",
              " 'effect': 434,\n",
              " 'hotel': 435,\n",
              " 'website': 436,\n",
              " 'nothing': 437,\n",
              " 'st': 438,\n",
              " '1995': 439,\n",
              " 'together': 440,\n",
              " 'feel': 441,\n",
              " 'probably': 442,\n",
              " 'done': 443,\n",
              " 'razor': 444,\n",
              " 'computational': 445,\n",
              " 'street': 446,\n",
              " 'company': 447,\n",
              " 'august': 448,\n",
              " 'call': 449,\n",
              " 'got': 450,\n",
              " 'ever': 451,\n",
              " 'next': 452,\n",
              " 'thing': 453,\n",
              " 'html': 454,\n",
              " 'david': 455,\n",
              " 'return': 456,\n",
              " 'window': 457,\n",
              " 'development': 458,\n",
              " 'become': 459,\n",
              " 'submission': 460,\n",
              " 'great': 461,\n",
              " 'tel': 462,\n",
              " 'included': 463,\n",
              " 'start': 464,\n",
              " 'spoken': 465,\n",
              " 'four': 466,\n",
              " 'rule': 467,\n",
              " 'day': 468,\n",
              " 'could': 469,\n",
              " 'theme': 470,\n",
              " 'file': 471,\n",
              " 'program': 472,\n",
              " 'schedule': 473,\n",
              " '45': 474,\n",
              " 'event': 475,\n",
              " 'press': 476,\n",
              " 'work': 477,\n",
              " 'corpus': 478,\n",
              " 'case': 479,\n",
              " 'benjamin': 480,\n",
              " 'area': 481,\n",
              " 'live': 482,\n",
              " 'around': 483,\n",
              " 'pragmatic': 484,\n",
              " 'ac': 485,\n",
              " 'publication': 486,\n",
              " 'production': 487,\n",
              " 'c': 488,\n",
              " 'tool': 489,\n",
              " '20': 490,\n",
              " 'job': 491,\n",
              " 'place': 492,\n",
              " 'paul': 493,\n",
              " 'center': 494,\n",
              " 'although': 495,\n",
              " 'accepted': 496,\n",
              " 'french': 497,\n",
              " 'main': 498,\n",
              " 'system': 499,\n",
              " '1997': 500,\n",
              " 'en': 501,\n",
              " 'com': 502,\n",
              " 'discussion': 503,\n",
              " 'email': 504,\n",
              " 'video': 505,\n",
              " '3d': 506,\n",
              " 'include': 507,\n",
              " 'answer': 508,\n",
              " '60': 509,\n",
              " '97': 510,\n",
              " 'working': 511,\n",
              " 'fee': 512,\n",
              " 'time': 513,\n",
              " 'since': 514,\n",
              " '35': 515,\n",
              " 'available': 516,\n",
              " 'charge': 517,\n",
              " 'person': 518,\n",
              " 'increase': 519,\n",
              " 'pay': 520,\n",
              " 'trading': 521,\n",
              " 'reference': 522,\n",
              " 'cost': 523,\n",
              " 'interest': 524,\n",
              " 'summer': 525,\n",
              " 'analyst': 526,\n",
              " 'semantics': 527,\n",
              " 'linguist': 528,\n",
              " 'back': 529,\n",
              " 'exchange': 530,\n",
              " 'keep': 531,\n",
              " 'student': 532,\n",
              " 'percent': 533,\n",
              " 'seems': 534,\n",
              " 'topic': 535,\n",
              " 'includes': 536,\n",
              " 'try': 537,\n",
              " 'author': 538,\n",
              " 'u': 539,\n",
              " 'learn': 540,\n",
              " 'corp': 541,\n",
              " 'given': 542,\n",
              " 'code': 543,\n",
              " '7': 544,\n",
              " 'description': 545,\n",
              " 'legal': 546,\n",
              " 'including': 547,\n",
              " 'view': 548,\n",
              " 'getting': 549,\n",
              " 'dynegy': 550,\n",
              " 'operation': 551,\n",
              " '44': 552,\n",
              " '2001': 553,\n",
              " 'hour': 554,\n",
              " 'material': 555,\n",
              " 'language': 556,\n",
              " 'color': 557,\n",
              " '10': 558,\n",
              " 'course': 559,\n",
              " 'effort': 560,\n",
              " 'text': 561,\n",
              " 'lexical': 562,\n",
              " 'directory': 563,\n",
              " 'better': 564,\n",
              " 'phonology': 565,\n",
              " 'future': 566,\n",
              " 'many': 567,\n",
              " 'usa': 568,\n",
              " 'rpm': 569,\n",
              " 'release': 570,\n",
              " 'access': 571,\n",
              " 'k': 572,\n",
              " 'letter': 573,\n",
              " 'programme': 574,\n",
              " 'note': 575,\n",
              " 'query': 576,\n",
              " 'social': 577,\n",
              " 'clear': 578,\n",
              " 'ie': 579,\n",
              " 'hou': 580,\n",
              " 'reason': 581,\n",
              " 'december': 582,\n",
              " 'follow': 583,\n",
              " 'different': 584,\n",
              " 'verb': 585,\n",
              " 'individual': 586,\n",
              " 'v': 587,\n",
              " 'volume': 588,\n",
              " 'might': 589,\n",
              " '1994': 590,\n",
              " 'user': 591,\n",
              " 'go': 592,\n",
              " 'act': 593,\n",
              " 'marketing': 594,\n",
              " 'ready': 595,\n",
              " 'easy': 596,\n",
              " 'status': 597,\n",
              " 'g': 598,\n",
              " 'native': 599,\n",
              " 'pc': 600,\n",
              " 'mark': 601,\n",
              " 'proceeding': 602,\n",
              " 'professor': 603,\n",
              " 'experience': 604,\n",
              " 'tell': 605,\n",
              " 'list': 606,\n",
              " 'date': 607,\n",
              " 'must': 608,\n",
              " 'result': 609,\n",
              " 'response': 610,\n",
              " 'wa': 611,\n",
              " 'submit': 612,\n",
              " 'ã': 613,\n",
              " 'category': 614,\n",
              " 'offer': 615,\n",
              " 'context': 616,\n",
              " '16': 617,\n",
              " 'american': 618,\n",
              " 'communication': 619,\n",
              " 'ca': 620,\n",
              " 'error': 621,\n",
              " 'friend': 622,\n",
              " 'check': 623,\n",
              " 'ha': 624,\n",
              " 'city': 625,\n",
              " 'risk': 626,\n",
              " 'org': 627,\n",
              " 'market': 628,\n",
              " 'total': 629,\n",
              " 'tuesday': 630,\n",
              " 'learning': 631,\n",
              " 'show': 632,\n",
              " 'general': 633,\n",
              " 'committee': 634,\n",
              " 'anything': 635,\n",
              " 'family': 636,\n",
              " 'order': 637,\n",
              " 'read': 638,\n",
              " 'theoretical': 639,\n",
              " 'july': 640,\n",
              " 'electronic': 641,\n",
              " 'three': 642,\n",
              " 'sent': 643,\n",
              " 'life': 644,\n",
              " 'summary': 645,\n",
              " 'executive': 646,\n",
              " 'structure': 647,\n",
              " '99': 648,\n",
              " 'application': 649,\n",
              " '28': 650,\n",
              " '1999': 651,\n",
              " 'bank': 652,\n",
              " 'argument': 653,\n",
              " 'going': 654,\n",
              " 'forwarded': 655,\n",
              " '30': 656,\n",
              " 'chief': 657,\n",
              " 'via': 658,\n",
              " 'problem': 659,\n",
              " 'yahoo': 660,\n",
              " 'www': 661,\n",
              " 'analysis': 662,\n",
              " 'account': 663,\n",
              " 'grant': 664,\n",
              " 'click': 665,\n",
              " 'association': 666,\n",
              " 'really': 667,\n",
              " 'related': 668,\n",
              " 'personal': 669,\n",
              " 'position': 670,\n",
              " 'well': 671,\n",
              " 'stop': 672,\n",
              " 'regarding': 673,\n",
              " 'add': 674,\n",
              " 'sell': 675,\n",
              " 'method': 676,\n",
              " 'nl': 677,\n",
              " 'school': 678,\n",
              " 'currently': 679,\n",
              " 'robert': 680,\n",
              " 'first': 681,\n",
              " '98': 682,\n",
              " 'post': 683,\n",
              " 'presentation': 684,\n",
              " '26': 685,\n",
              " 'type': 686,\n",
              " 'un': 687,\n",
              " 'every': 688,\n",
              " 'everything': 689,\n",
              " 'knowledge': 690,\n",
              " '02': 691,\n",
              " 'â': 692,\n",
              " 'approach': 693,\n",
              " '000': 694,\n",
              " 'held': 695,\n",
              " 'income': 696,\n",
              " 'dollar': 697,\n",
              " 'texas': 698,\n",
              " '15': 699,\n",
              " 'party': 700,\n",
              " 'used': 701,\n",
              " 'fax': 702,\n",
              " 'removed': 703,\n",
              " 'construction': 704,\n",
              " 'others': 705,\n",
              " 'cannot': 706,\n",
              " 'march': 707,\n",
              " 'long': 708,\n",
              " '18': 709,\n",
              " 'reply': 710,\n",
              " 'president': 711,\n",
              " 'card': 712,\n",
              " 'local': 713,\n",
              " 'mailing': 714,\n",
              " 'reading': 715,\n",
              " 'uk': 716,\n",
              " 'jones': 717,\n",
              " 'phone': 718,\n",
              " 'database': 719,\n",
              " 'common': 720,\n",
              " 'control': 721,\n",
              " 'logic': 722,\n",
              " 'line': 723,\n",
              " 'object': 724,\n",
              " 'article': 725,\n",
              " 'cd': 726,\n",
              " 'big': 727,\n",
              " 'direct': 728,\n",
              " 'gas': 729,\n",
              " 'contact': 730,\n",
              " 'deal': 731,\n",
              " 'copy': 732,\n",
              " 'computer': 733,\n",
              " 'non': 734,\n",
              " 'page': 735,\n",
              " '04': 736,\n",
              " 'hard': 737,\n",
              " 'help': 738,\n",
              " 'trade': 739,\n",
              " 'head': 740,\n",
              " 'forward': 741,\n",
              " 'http': 742,\n",
              " 'attached': 743,\n",
              " 'found': 744,\n",
              " 'little': 745,\n",
              " 'top': 746,\n",
              " 'may': 747,\n",
              " 'whole': 748,\n",
              " 'billion': 749,\n",
              " 'pp': 750,\n",
              " 'write': 751,\n",
              " 'michael': 752,\n",
              " 'latest': 753,\n",
              " 'f': 754,\n",
              " 'law': 755,\n",
              " 'south': 756,\n",
              " 'information': 757,\n",
              " 'pre': 758,\n",
              " 'complete': 759,\n",
              " '3': 760,\n",
              " '2002': 761,\n",
              " 'know': 762,\n",
              " 'york': 763,\n",
              " 'asset': 764,\n",
              " 'would': 765,\n",
              " 'million': 766,\n",
              " 'organization': 767,\n",
              " 'processing': 768,\n",
              " 'proposal': 769,\n",
              " 'february': 770,\n",
              " 'january': 771,\n",
              " 'subject': 772,\n",
              " 'actually': 773,\n",
              " 'year': 774,\n",
              " 'specific': 775,\n",
              " 'continue': 776,\n",
              " 'thought': 777,\n",
              " 'made': 778,\n",
              " 'single': 779,\n",
              " 'cc': 780,\n",
              " 'value': 781,\n",
              " 'customer': 782,\n",
              " 'quality': 783,\n",
              " 'remove': 784,\n",
              " '00': 785,\n",
              " 'per': 786,\n",
              " 'yet': 787,\n",
              " 'october': 788,\n",
              " 'part': 789,\n",
              " 'title': 790,\n",
              " 'standard': 791,\n",
              " 'ed': 792,\n",
              " 'option': 793,\n",
              " 'save': 794,\n",
              " 'syntax': 795,\n",
              " 'additional': 796,\n",
              " '05': 797,\n",
              " 'low': 798,\n",
              " 'able': 799,\n",
              " 'role': 800,\n",
              " 'find': 801,\n",
              " 'sign': 802,\n",
              " 'still': 803,\n",
              " 'chinese': 804,\n",
              " 'country': 805,\n",
              " 'germany': 806,\n",
              " 'focus': 807,\n",
              " 'due': 808,\n",
              " 'think': 809,\n",
              " 'lot': 810,\n",
              " 'investment': 811,\n",
              " 'web': 812}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
        "token_to_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE6xAzIN7DY5",
        "outputId": "d8ec86ba-ea77-4e9b-b0c0-53781febc50f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['3d', 'b', 'br', 'com', 'bad', 'font', 'font', 'com', 'randoms']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message_to_token_list('3d b <br> .com bad font font com randoms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OuXO-Cjf4vNo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"Bag of Words\" (counts vector)\n",
        "\n",
        "# ->  http  tr  size  3d  font  br  com  td   p   b\n",
        "# ->    0    1    2    3   4    5    6    7   8   9\n",
        "# ->   [0,   0,   0,   1,  2,   1,   2,   0,  0,  1]\n",
        "\n",
        "[0.,  0.,  0.,   1., 2.,  1., 2.,  0., 0., 1.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ3kBFPW7vpm",
        "outputId": "3280787f-4a7f-4ed2-8836-5d3152393809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def message_to_count_vector(message):\n",
        "  count_vector = np.zeros(len(features))\n",
        "\n",
        "  processed_list_of_tokens = message_to_token_list(message)\n",
        "\n",
        "  for token in processed_list_of_tokens:\n",
        "    if token not in features:\n",
        "      continue\n",
        "    index = token_to_index_mapping[token]\n",
        "    count_vector[index] += 1\n",
        "\n",
        "  return count_vector\n",
        "\n",
        "message_to_count_vector('3d b <br> .com bad font font com randoms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvnPzPqN_kWU",
        "outputId": "2af5915b-12dd-4b9f-b40f-0eacbc4ef2ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message_to_count_vector(train_df['MESSAGE'].iloc[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We7My4so_y4P",
        "outputId": "ecdfd9ac-38a6-40be-c22e-baf528a44839"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SERIAL                                                  13497\n",
              "MESSAGE     btu ' s daily power report - eastern edition a...\n",
              "CATEGORY                                                    0\n",
              "Name: 3, dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.iloc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w3V5tgVbB_9K"
      },
      "outputs": [],
      "source": [
        "def df_to_X_y(dff):\n",
        "  y = dff['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "  message_col = dff['MESSAGE']\n",
        "  count_vectors = []\n",
        "\n",
        "  for message in message_col:\n",
        "    count_vector = message_to_count_vector(message)\n",
        "    count_vectors.append(count_vector)\n",
        "\n",
        "  X = np.array(count_vectors).astype(int)\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1AB_Ehd_18I",
        "outputId": "ac1da35b-3721-441f-a386-efd12e084422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14907, 813), (14907,), (3727, 813), (3727,))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, y_train = df_to_X_y(train_df)\n",
        "\n",
        "X_test, y_test = df_to_X_y(test_df)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "243hURJZC7pt",
        "outputId": "843808cf-2f87-43bc-8d18-f41d8f6abc6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.00568182, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler().fit(X_train)\n",
        "\n",
        "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# **TF-IDF Feature Extraction**\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def custom_tokenizer(message):\n",
        "    return message_to_token_list(message)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['MESSAGE'])\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['MESSAGE'])\n",
        "\n",
        "y_train_tfidf = train_df['CATEGORY'].to_numpy().astype(int)\n",
        "y_test_tfidf = test_df['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "\n",
        "# View the dense matrix (first 5 rows, for example)\n",
        "print(X_train_tfidf_dense[:1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Vector Feature Matrix Shapes:\n",
            "Training Set: (14907, 100)\n",
            "Test Set: (3727, 100)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Prepare tokenized sentences from the training data\n",
        "train_sentences = [message_to_token_list(msg) for msg in train_df['MESSAGE'] if isinstance(msg, str)]\n",
        "test_sentences = [message_to_token_list(msg) for msg in test_df['MESSAGE'] if isinstance(msg, str)]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4, seed=1)\n",
        "\n",
        "# Function to convert a message to a Word2Vec vector (averaged over all tokens)\n",
        "def message_to_wv_vector(message, model, vector_size):\n",
        "    tokens = message_to_token_list(message)\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    if len(vectors) == 0:  # If no tokens are found in the model, return a zero vector\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Extract Word2Vec features for training and test sets\n",
        "vector_size = word2vec_model.vector_size\n",
        "wv_X_train = np.array([message_to_wv_vector(msg, word2vec_model, vector_size) for msg in train_df['MESSAGE']])\n",
        "wv_X_test = np.array([message_to_wv_vector(msg, word2vec_model, vector_size) for msg in test_df['MESSAGE']])\n",
        "\n",
        "# Labels remain the same\n",
        "wv_y_train = train_df['CATEGORY'].to_numpy().astype(int)\n",
        "wv_y_test = test_df['CATEGORY'].to_numpy().astype(int)\n",
        "\n",
        "# Verify the shape of the feature matrices\n",
        "print(\"Word Vector Feature Matrix Shapes:\")\n",
        "print(\"Training Set:\", wv_X_train.shape)\n",
        "print(\"Test Set:\", wv_X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid Train Feature Shape: (14907, 140962)\n",
            "Hybrid Test Feature Shape: (3727, 140962)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Combine TF-IDF and Word Vectors into a hybrid feature\n",
        "hybrid_X_train = hstack([X_train_tfidf, wv_X_train])\n",
        "hybrid_X_test = hstack([X_test_tfidf, wv_X_test])\n",
        "\n",
        "# Ensure the data is in dense format if needed for certain classifiers\n",
        "hybrid_X_train = hybrid_X_train.toarray()\n",
        "hybrid_X_test = hybrid_X_test.toarray()\n",
        "\n",
        "print(f\"Hybrid Train Feature Shape: {hybrid_X_train.shape}\")\n",
        "print(f\"Hybrid Test Feature Shape: {hybrid_X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptZ2wUTxD1MF",
        "outputId": "b960271b-8777-4950-f2ec-0e6109446812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.99      0.77      2290\n",
            "           1       0.86      0.05      0.10      1437\n",
            "\n",
            "    accuracy                           0.63      3727\n",
            "   macro avg       0.74      0.52      0.43      3727\n",
            "weighted avg       0.72      0.63      0.51      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lr = LogisticRegression().fit(X_train, y_train)\n",
        "print(classification_report(y_test, lr.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZK0jMUSEOCi",
        "outputId": "578ad022-7481-43d7-a74e-a38a3afc89cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96      2290\n",
            "           1       0.93      0.95      0.94      1437\n",
            "\n",
            "    accuracy                           0.95      3727\n",
            "   macro avg       0.95      0.95      0.95      3727\n",
            "weighted avg       0.95      0.95      0.95      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare logistic regression to random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier().fit(X_train, y_train)\n",
        "print(classification_report(y_test, rf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with Bag of Words Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      2290\n",
            "           1       0.92      0.97      0.94      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.95      0.96      0.95      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_bow = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "xgb_bow.fit(X_train, y_train)\n",
        "\n",
        "xgb_predictions_bow = xgb_bow.predict(X_test)\n",
        "\n",
        "print(\"XGBoost with Bag of Words Classification Report:\\n\")\n",
        "print(classification_report(y_test, xgb_predictions_bow))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TF_IDF EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3vhIALXMEkb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n",
            "Logistic Regression Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      2290\n",
            "           1       0.94      0.98      0.96      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.96      0.97      0.97      3727\n",
            "weighted avg       0.97      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# **Logistic Regression Training and Evaluation**\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Training Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Predictions and evaluation\n",
        "lr_predictions = lr_model.predict(X_test_tfidf)\n",
        "print(\"Logistic Regression Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, lr_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest...\n",
            "Random Forest Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      2290\n",
            "           1       0.95      0.96      0.96      1437\n",
            "\n",
            "    accuracy                           0.97      3727\n",
            "   macro avg       0.96      0.96      0.96      3727\n",
            "weighted avg       0.97      0.97      0.97      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# **Random Forest Training and Evaluation**\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Training Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Predictions and evaluation\n",
        "rf_predictions = rf_model.predict(X_test_tfidf)\n",
        "print(\"Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, rf_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:58:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97      2290\n",
            "           1       0.93      0.98      0.96      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.96      0.97      0.96      3727\n",
            "weighted avg       0.97      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the XGBoost model\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "xgb_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "xgb_predictions = xgb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"XGBoost Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, xgb_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      2290\n",
            "           1       0.92      0.95      0.94      1437\n",
            "\n",
            "    accuracy                           0.95      3727\n",
            "   macro avg       0.95      0.95      0.95      3727\n",
            "weighted avg       0.95      0.95      0.95      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "wv_lr_model = LogisticRegression().fit(wv_X_train, wv_y_train)\n",
        "wv_predictions = wv_lr_model.predict(wv_X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "print(\"Logistic Regression with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, wv_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest on Word Vectors...\n",
            "Random Forest with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96      2290\n",
            "           1       0.94      0.95      0.94      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.95      0.95      0.95      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Random Forest Classifier on Word Vectors\n",
        "print(\"Training Random Forest on Word Vectors...\")\n",
        "rf_wv_model = RandomForestClassifier(random_state=1)\n",
        "rf_wv_model.fit(wv_X_train, wv_y_train)\n",
        "\n",
        "rf_wv_predictions = rf_wv_model.predict(wv_X_test)\n",
        "\n",
        "print(\"Random Forest with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, rf_wv_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost on Word Vectors...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:58:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with Word Vectors Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      2290\n",
            "           1       0.94      0.97      0.95      1437\n",
            "\n",
            "    accuracy                           0.96      3727\n",
            "   macro avg       0.96      0.96      0.96      3727\n",
            "weighted avg       0.96      0.96      0.96      3727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# XGBoost Classifier on Word Vectors\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Training XGBoost on Word Vectors...\")\n",
        "xgb_wv_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=1)\n",
        "xgb_wv_model.fit(wv_X_train, wv_y_train)\n",
        "\n",
        "xgb_wv_predictions = xgb_wv_model.predict(wv_X_test)\n",
        "\n",
        "print(\"XGBoost with Word Vectors Classification Report:\\n\")\n",
        "print(classification_report(wv_y_test, xgb_wv_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train Logistic Regression on Hybrid Features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m lr_hybrid_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlr_hybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhybrid_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tfidf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m      9\u001b[0m lr_hybrid_predictions \u001b[38;5;241m=\u001b[39m lr_hybrid_model\u001b[38;5;241m.\u001b[39mpredict(hybrid_X_test)\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    451\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m sw_sum)\n\u001b[0;32m    452\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    453\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    454\u001b[0m ]\n\u001b[1;32m--> 455\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    470\u001b[0m     solver,\n\u001b[0;32m    471\u001b[0m     opt_res,\n\u001b[0;32m    472\u001b[0m     max_iter,\n\u001b[0;32m    473\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    475\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:281\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[1;32m--> 281\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m sw_sum \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(sample_weight)\n\u001b[0;32m    288\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m sw_sum\n",
            "File \u001b[1;32mc:\\Users\\fuadn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\_loss\\loss.py:202\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[1;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloss\u001b[38;5;241m.\u001b[39mloss(\n\u001b[0;32m    194\u001b[0m         y_true\u001b[38;5;241m=\u001b[39my_true,\n\u001b[0;32m    195\u001b[0m         raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    199\u001b[0m     )\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_out\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_gradient\u001b[39m(\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    204\u001b[0m     y_true,\n\u001b[0;32m    205\u001b[0m     raw_prediction,\n\u001b[0;32m    206\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    207\u001b[0m     loss_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    208\u001b[0m     gradient_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    210\u001b[0m ):\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss and gradient w.r.t. raw_prediction for each input.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m        Element-wise gradients.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Logistic Regression on Hybrid Features\n",
        "lr_hybrid_model = LogisticRegression()\n",
        "lr_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "lr_hybrid_predictions = lr_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"Logistic Regression Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, lr_hybrid_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest on Hybrid Features\n",
        "rf_hybrid_model = RandomForestClassifier()\n",
        "rf_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_hybrid_predictions = rf_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"Random Forest Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, rf_hybrid_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost on Hybrid Features\n",
        "xgb_hybrid_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_hybrid_model.fit(hybrid_X_train, y_train_tfidf)\n",
        "\n",
        "# Predict and evaluate\n",
        "xgb_hybrid_predictions = xgb_hybrid_model.predict(hybrid_X_test)\n",
        "print(\"XGBoost Classification Report (Hybrid Features):\\n\")\n",
        "print(classification_report(y_test_tfidf, xgb_hybrid_predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
